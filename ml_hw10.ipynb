{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "jSesEiLRkV2M"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MVTZmFccmyHP"
   },
   "source": [
    "# Работа с ансамблями"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VE36FyB9m4Gp"
   },
   "source": [
    "Эта работа является идейным продолжением предыдущей. В данной работе  поэкспериментируем с различными методами ансамблирования и проверим, какие из них работают лучше. Пайплайн предобработки данных возьмем из предыдущей работы.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_u-DyeiInWN2"
   },
   "source": [
    "# Загрузка и подготовка данных "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv(\"X_train\")\n",
    "X_test = pd.read_csv(\"X_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number_of_reviews</th>\n",
       "      <th>number_of_answered_questions</th>\n",
       "      <th>average_review_rating</th>\n",
       "      <th>price</th>\n",
       "      <th>number_avaible</th>\n",
       "      <th>manufacturer_Amscan</th>\n",
       "      <th>manufacturer_Baker Ross</th>\n",
       "      <th>manufacturer_Bandai</th>\n",
       "      <th>manufacturer_Bristol Novelties</th>\n",
       "      <th>manufacturer_Bruder</th>\n",
       "      <th>...</th>\n",
       "      <th>amazon_category_and_sub_category_Hobbies &gt; Trading Cards &amp; Accessories &gt; Packs &amp; Sets</th>\n",
       "      <th>amazon_category_and_sub_category_Party Supplies &gt; Banners, Stickers &amp; Confetti &gt; Banners</th>\n",
       "      <th>amazon_category_and_sub_category_Party Supplies &gt; Decorations &gt; Balloons</th>\n",
       "      <th>amazon_category_and_sub_category_Puppets &amp; Puppet Theatres &gt; Hand Puppets</th>\n",
       "      <th>amazon_category_and_sub_category_other</th>\n",
       "      <th>condition_collectible</th>\n",
       "      <th>condition_new</th>\n",
       "      <th>condition_refurbished</th>\n",
       "      <th>condition_used</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.6</td>\n",
       "      <td>1.50</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.60</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.9</td>\n",
       "      <td>1.64</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.8</td>\n",
       "      <td>6.50</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>89.34</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4130</th>\n",
       "      <td>6.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.3</td>\n",
       "      <td>2.58</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4131</th>\n",
       "      <td>12.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.3</td>\n",
       "      <td>11.93</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4132</th>\n",
       "      <td>16.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>24.99</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4133</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.32</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4134</th>\n",
       "      <td>11.0</td>\n",
       "      <td>1.874485</td>\n",
       "      <td>4.8</td>\n",
       "      <td>7.99</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4135 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      number_of_reviews  number_of_answered_questions  average_review_rating  \\\n",
       "0                   8.0                      2.000000                    4.6   \n",
       "1                   1.0                      1.000000                    5.0   \n",
       "2                   9.0                      1.000000                    4.9   \n",
       "3                   6.0                      1.000000                    4.8   \n",
       "4                   2.0                      1.000000                    5.0   \n",
       "...                 ...                           ...                    ...   \n",
       "4130                6.0                      1.000000                    4.3   \n",
       "4131               12.0                      1.000000                    4.3   \n",
       "4132               16.0                      1.000000                    4.0   \n",
       "4133                1.0                      1.000000                    5.0   \n",
       "4134               11.0                      1.874485                    4.8   \n",
       "\n",
       "      price  number_avaible  manufacturer_Amscan  manufacturer_Baker Ross  \\\n",
       "0      1.50             3.0                    0                        0   \n",
       "1      6.60             2.0                    0                        0   \n",
       "2      1.64            10.0                    0                        0   \n",
       "3      6.50             4.0                    0                        0   \n",
       "4     89.34             2.0                    0                        0   \n",
       "...     ...             ...                  ...                      ...   \n",
       "4130   2.58            12.0                    0                        0   \n",
       "4131  11.93             3.0                    0                        0   \n",
       "4132  24.99             7.0                    0                        0   \n",
       "4133   1.32             3.0                    0                        0   \n",
       "4134   7.99             3.0                    0                        0   \n",
       "\n",
       "      manufacturer_Bandai  manufacturer_Bristol Novelties  \\\n",
       "0                       0                               0   \n",
       "1                       0                               0   \n",
       "2                       0                               0   \n",
       "3                       0                               0   \n",
       "4                       0                               0   \n",
       "...                   ...                             ...   \n",
       "4130                    0                               0   \n",
       "4131                    0                               0   \n",
       "4132                    0                               0   \n",
       "4133                    0                               0   \n",
       "4134                    0                               0   \n",
       "\n",
       "      manufacturer_Bruder  ...  \\\n",
       "0                       0  ...   \n",
       "1                       0  ...   \n",
       "2                       0  ...   \n",
       "3                       0  ...   \n",
       "4                       0  ...   \n",
       "...                   ...  ...   \n",
       "4130                    0  ...   \n",
       "4131                    0  ...   \n",
       "4132                    0  ...   \n",
       "4133                    0  ...   \n",
       "4134                    0  ...   \n",
       "\n",
       "      amazon_category_and_sub_category_Hobbies > Trading Cards & Accessories > Packs & Sets  \\\n",
       "0                                                     0                                       \n",
       "1                                                     0                                       \n",
       "2                                                     0                                       \n",
       "3                                                     0                                       \n",
       "4                                                     0                                       \n",
       "...                                                 ...                                       \n",
       "4130                                                  0                                       \n",
       "4131                                                  0                                       \n",
       "4132                                                  0                                       \n",
       "4133                                                  0                                       \n",
       "4134                                                  0                                       \n",
       "\n",
       "      amazon_category_and_sub_category_Party Supplies > Banners, Stickers & Confetti > Banners  \\\n",
       "0                                                     0                                          \n",
       "1                                                     0                                          \n",
       "2                                                     1                                          \n",
       "3                                                     0                                          \n",
       "4                                                     0                                          \n",
       "...                                                 ...                                          \n",
       "4130                                                  0                                          \n",
       "4131                                                  0                                          \n",
       "4132                                                  0                                          \n",
       "4133                                                  1                                          \n",
       "4134                                                  0                                          \n",
       "\n",
       "      amazon_category_and_sub_category_Party Supplies > Decorations > Balloons  \\\n",
       "0                                                     0                          \n",
       "1                                                     0                          \n",
       "2                                                     0                          \n",
       "3                                                     0                          \n",
       "4                                                     0                          \n",
       "...                                                 ...                          \n",
       "4130                                                  0                          \n",
       "4131                                                  0                          \n",
       "4132                                                  0                          \n",
       "4133                                                  0                          \n",
       "4134                                                  0                          \n",
       "\n",
       "      amazon_category_and_sub_category_Puppets & Puppet Theatres > Hand Puppets  \\\n",
       "0                                                     0                           \n",
       "1                                                     0                           \n",
       "2                                                     0                           \n",
       "3                                                     0                           \n",
       "4                                                     0                           \n",
       "...                                                 ...                           \n",
       "4130                                                  0                           \n",
       "4131                                                  0                           \n",
       "4132                                                  0                           \n",
       "4133                                                  0                           \n",
       "4134                                                  0                           \n",
       "\n",
       "      amazon_category_and_sub_category_other  condition_collectible  \\\n",
       "0                                          1                      0   \n",
       "1                                          0                      0   \n",
       "2                                          0                      0   \n",
       "3                                          1                      0   \n",
       "4                                          1                      0   \n",
       "...                                      ...                    ...   \n",
       "4130                                       1                      0   \n",
       "4131                                       0                      0   \n",
       "4132                                       1                      0   \n",
       "4133                                       0                      0   \n",
       "4134                                       0                      0   \n",
       "\n",
       "      condition_new  condition_refurbished  condition_used  split  \n",
       "0                 1                      0               0  train  \n",
       "1                 1                      0               0  train  \n",
       "2                 1                      0               0  train  \n",
       "3                 1                      0               0  train  \n",
       "4                 1                      0               0  train  \n",
       "...             ...                    ...             ...    ...  \n",
       "4130              1                      0               0  train  \n",
       "4131              1                      0               0  train  \n",
       "4132              1                      0               0  train  \n",
       "4133              1                      0               0  train  \n",
       "4134              1                      0               0  train  \n",
       "\n",
       "[4135 rows x 64 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[\"split\"] = \"train\"\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[\"split\"] = \"test\"\n",
    "df = pd.concat([X_train, X_test], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = X_train.drop(columns='price'), X_train.price\n",
    "X_test, y_test = X_test.drop(columns='price'), X_test.price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "0K3HJ_AqLg9i"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cO5VjK7kp_si"
   },
   "source": [
    "# Стекинг "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9HnkGgBpqfgI"
   },
   "source": [
    "## Простой стекинг своими руками (2 балла)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "in4yMYq1qVei"
   },
   "outputs": [],
   "source": [
    "class StackingRegressionSolver:\n",
    "    def __init__(self, base_estimators: list, meta_estimator):\n",
    "        self._base_estimators = base_estimators\n",
    "        self._meta_estimator = meta_estimator\n",
    "\n",
    "    def _fit_base(self, X: pd.DataFrame, y: pd.Series) -> None:\n",
    "        for estimator in self._base_estimators:\n",
    "            estimator.fit(X, y)\n",
    "\n",
    "    def _predict_base(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        cnt = 1\n",
    "        meta_data = pd.DataFrame()\n",
    "        for x in self._base_estimators:\n",
    "            meta_data[f\"{cnt}\"] = x.predict(X)\n",
    "            cnt += 1\n",
    "        return meta_data\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, y: pd.Series):\n",
    "        self._fit_base(X, y)\n",
    "        meta_features = self._predict_base(X)\n",
    "        self._meta_estimator.fit(meta_features, y)\n",
    "\n",
    "\n",
    "    def predict(self, X: pd.DataFrame) -> pd.Series:\n",
    "        meta_features = self._predict_base(X)\n",
    "        return self._meta_estimator.predict(meta_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error as mse, r2_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import LinearSVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.drop(columns='split', inplace=True)\n",
    "X_test.drop(columns='split', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "OnrpvxiHccHL"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sav4u\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sav4u\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models: [[LinearRegression(), LinearSVR()], [DecisionTreeRegressor()]] , result:  mse = 1930.9114782743297 R2_score = -0.25184805902098084\n",
      "Models: [[LinearRegression(), DecisionTreeRegressor()], [LinearSVR()]] , result:  mse = 2100.0078275463416 R2_score = -0.3614765629712935\n",
      "Models: [[DecisionTreeRegressor(), LinearSVR()], [LinearRegression()]] , result:  mse = 2095.9751792049437 R2_score = -0.3588621173813773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sav4u\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "lr = LinearRegression()\n",
    "dt = DecisionTreeRegressor()\n",
    "svr = LinearSVR()\n",
    "\n",
    "estimator = [[[lr, svr], [dt]], [[lr, dt], [svr]], [[dt, svr], [lr]]]\n",
    "\n",
    "for x in estimator:\n",
    "    stacking_regressor = StackingRegressionSolver(base_estimators=x[0], meta_estimator=x[1][0])\n",
    "    stacking_regressor.fit(X_train, y_train)\n",
    "    y_pred = stacking_regressor.predict(X_test)\n",
    "    print(\"Models:\", x, \", result: \", \"mse =\", mse(y_test, y_pred),\"R2_score =\", r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лучший результат показала комбинация моделей [LinearRegression(), LinearSVR()], [DecisionTreeRegressor()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем перебрать гиперпараметры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sav4u\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sav4u\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_depth =  2 epsilon = 0.1\n",
      "mse = 1785.6397243272158 r2 = -0.1576655109055196\n",
      "-------------------------------------------\n",
      "max_depth =  2 epsilon = 0.2\n",
      "mse = 1898.9905517871753 r2 = -0.23115309174013543\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sav4u\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sav4u\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_depth =  2 epsilon = 0.3\n",
      "mse = 1940.6849280528713 r2 = -0.25818437959956086\n",
      "-------------------------------------------\n",
      "max_depth =  2 epsilon = 0.4\n",
      "mse = 1946.6997810824403 r2 = -0.2620839276498381\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sav4u\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sav4u\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_depth =  2 epsilon = 0.5\n",
      "mse = 1867.1501755799134 r2 = -0.21051034679711966\n",
      "-------------------------------------------\n",
      "max_depth =  2 epsilon = 0.6\n",
      "mse = 1827.6973386629138 r2 = -0.1849322931819657\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sav4u\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sav4u\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_depth =  2 epsilon = 0.7\n",
      "mse = 1978.4745809919987 r2 = -0.282684158183508\n",
      "-------------------------------------------\n",
      "max_depth =  2 epsilon = 0.8\n",
      "mse = 1870.2291927207918 r2 = -0.2125065344395205\n",
      "-------------------------------------------\n",
      "max_depth =  2 epsilon = 0.9\n",
      "mse = 1828.904880785349 r2 = -0.18571516659649823\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sav4u\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sav4u\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_depth =  3 epsilon = 0.1\n",
      "mse = 1970.5660532696754 r2 = -0.2775569034179235\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sav4u\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sav4u\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_depth =  3 epsilon = 0.2\n",
      "mse = 1716.2914143004964 r2 = -0.11270563144956469\n",
      "-------------------------------------------\n",
      "max_depth =  3 epsilon = 0.3\n",
      "mse = 1817.304046771353 r2 = -0.1781941167157317\n",
      "-------------------------------------------\n",
      "max_depth =  3 epsilon = 0.4\n",
      "mse = 1912.4955356976316 r2 = -0.23990863961772302\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sav4u\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sav4u\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_depth =  3 epsilon = 0.5\n",
      "mse = 2069.3253214695405 r2 = -0.34158448810907904\n",
      "-------------------------------------------\n",
      "max_depth =  3 epsilon = 0.6\n",
      "mse = 1801.848094510736 r2 = -0.16817371751282972\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sav4u\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sav4u\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sav4u\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_depth =  3 epsilon = 0.7\n",
      "mse = 1881.6289969790546 r2 = -0.21989725275804184\n",
      "-------------------------------------------\n",
      "max_depth =  3 epsilon = 0.8\n",
      "mse = 1946.013742280763 r2 = -0.2616391551411261\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sav4u\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_depth =  3 epsilon = 0.9\n",
      "mse = 1756.8498933721396 r2 = -0.13900049471696607\n",
      "-------------------------------------------\n",
      "max_depth =  4 epsilon = 0.1\n",
      "mse = 1748.371807294315 r2 = -0.13350398401714747\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sav4u\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sav4u\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_depth =  4 epsilon = 0.2\n",
      "mse = 1824.7284926631296 r2 = -0.18300753166690975\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sav4u\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_depth =  4 epsilon = 0.3\n",
      "mse = 1952.009511713885 r2 = -0.2655263309188056\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sav4u\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sav4u\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_depth =  4 epsilon = 0.4\n",
      "mse = 1880.5729331963319 r2 = -0.21921258574379565\n",
      "-------------------------------------------\n",
      "max_depth =  4 epsilon = 0.5\n",
      "mse = 1823.793148398429 r2 = -0.1824011295011676\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sav4u\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sav4u\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_depth =  4 epsilon = 0.6\n",
      "mse = 1880.9011992717808 r2 = -0.21942540712583058\n",
      "-------------------------------------------\n",
      "max_depth =  4 epsilon = 0.7\n",
      "mse = 1890.0183463877227 r2 = -0.2253362337221385\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sav4u\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sav4u\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_depth =  4 epsilon = 0.8\n",
      "mse = 1979.5400045382307 r2 = -0.28337489331735166\n",
      "-------------------------------------------\n",
      "max_depth =  4 epsilon = 0.9\n",
      "mse = 1970.0918181269826 r2 = -0.2772494474057867\n",
      "-------------------------------------------\n",
      "max_depth =  5 epsilon = 0.1\n",
      "mse = 1914.0758241290546 r2 = -0.2409331718284582\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sav4u\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sav4u\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sav4u\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_depth =  5 epsilon = 0.2\n",
      "mse = 1984.1537873094394 r2 = -0.28636609983917705\n",
      "-------------------------------------------\n",
      "max_depth =  5 epsilon = 0.3\n",
      "mse = 1968.9779899069945 r2 = -0.27652733056564993\n",
      "-------------------------------------------\n",
      "max_depth =  5 epsilon = 0.4\n",
      "mse = 1913.2413405426425 r2 = -0.24039215963308402\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sav4u\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sav4u\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_depth =  5 epsilon = 0.5\n",
      "mse = 1912.8477191600064 r2 = -0.24013696711422083\n",
      "-------------------------------------------\n",
      "max_depth =  5 epsilon = 0.6\n",
      "mse = 1883.185586505726 r2 = -0.22090642050062392\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sav4u\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sav4u\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_depth =  5 epsilon = 0.7\n",
      "mse = 1960.4787401065723 r2 = -0.2710170990063545\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sav4u\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sav4u\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_depth =  5 epsilon = 0.8\n",
      "mse = 1791.557129266539 r2 = -0.16150188143367727\n",
      "-------------------------------------------\n",
      "max_depth =  5 epsilon = 0.9\n",
      "mse = 1825.0365992241373 r2 = -0.18320728323745494\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sav4u\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sav4u\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_depth =  6 epsilon = 0.1\n",
      "mse = 1813.8363591017146 r2 = -0.1759459462908466\n",
      "-------------------------------------------\n",
      "max_depth =  6 epsilon = 0.2\n",
      "mse = 1900.9959424666204 r2 = -0.2324532261367085\n",
      "-------------------------------------------\n",
      "max_depth =  6 epsilon = 0.3\n",
      "mse = 2021.6543532114256 r2 = -0.31067845758565693\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sav4u\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sav4u\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_depth =  6 epsilon = 0.4\n",
      "mse = 1752.2086306900992 r2 = -0.13599147242604714\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sav4u\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sav4u\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_depth =  6 epsilon = 0.5\n",
      "mse = 1791.2346455413974 r2 = -0.16129280886358477\n",
      "-------------------------------------------\n",
      "max_depth =  6 epsilon = 0.6\n",
      "mse = 1823.0106250607896 r2 = -0.1818938041616145\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sav4u\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sav4u\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_depth =  6 epsilon = 0.7\n",
      "mse = 1870.233267162904 r2 = -0.21250917598083774\n",
      "-------------------------------------------\n",
      "max_depth =  6 epsilon = 0.8\n",
      "mse = 1845.6009400220482 r2 = -0.19653955165191284\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sav4u\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sav4u\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_depth =  6 epsilon = 0.9\n",
      "mse = 1884.5088618241818 r2 = -0.22176432603259588\n",
      "-------------------------------------------\n",
      "max_depth =  7 epsilon = 0.1\n",
      "mse = 1929.3049123005824 r2 = -0.25080649056040993\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sav4u\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sav4u\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_depth =  7 epsilon = 0.2\n",
      "mse = 2026.9679136397458 r2 = -0.31412334378760143\n",
      "-------------------------------------------\n",
      "max_depth =  7 epsilon = 0.3\n",
      "mse = 1889.4819087246349 r2 = -0.22498845058713846\n",
      "-------------------------------------------\n",
      "max_depth =  7 epsilon = 0.4\n",
      "mse = 1817.4916448054198 r2 = -0.17831574022745933\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sav4u\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sav4u\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_depth =  7 epsilon = 0.5\n",
      "mse = 1928.6611446421398 r2 = -0.2503891232689781\n",
      "-------------------------------------------\n",
      "max_depth =  7 epsilon = 0.6\n",
      "mse = 1869.3791300304026 r2 = -0.21195542200325224\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sav4u\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sav4u\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_depth =  7 epsilon = 0.7\n",
      "mse = 1873.2585384113765 r2 = -0.2144705190993721\n",
      "-------------------------------------------\n",
      "max_depth =  7 epsilon = 0.8\n",
      "mse = 1875.5857658248551 r2 = -0.21597930660892017\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sav4u\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sav4u\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_depth =  7 epsilon = 0.9\n",
      "mse = 1854.6945914748114 r2 = -0.20243514554559505\n",
      "-------------------------------------------\n",
      "max_depth =  8 epsilon = 0.1\n",
      "mse = 1740.5077433097797 r2 = -0.1284055559712094\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sav4u\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sav4u\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sav4u\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_depth =  8 epsilon = 0.2\n",
      "mse = 2019.5037790781755 r2 = -0.309284197887695\n",
      "-------------------------------------------\n",
      "max_depth =  8 epsilon = 0.3\n",
      "mse = 1923.2138334226202 r2 = -0.24685752378667924\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sav4u\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sav4u\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_depth =  8 epsilon = 0.4\n",
      "mse = 1844.0247170221999 r2 = -0.19551765514078534\n",
      "-------------------------------------------\n",
      "max_depth =  8 epsilon = 0.5\n",
      "mse = 1970.0738438289738 r2 = -0.277237794313282\n",
      "-------------------------------------------\n",
      "max_depth =  8 epsilon = 0.6\n",
      "mse = 1926.3637571390734 r2 = -0.24889968156287634\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sav4u\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sav4u\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sav4u\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_depth =  8 epsilon = 0.7\n",
      "mse = 2003.0752519300308 r2 = -0.29863325917073325\n",
      "-------------------------------------------\n",
      "max_depth =  8 epsilon = 0.8\n",
      "mse = 1925.9158611219696 r2 = -0.2486093017262221\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sav4u\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sav4u\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_depth =  8 epsilon = 0.9\n",
      "mse = 1824.8708161104817 r2 = -0.18309980276961202\n",
      "-------------------------------------------\n",
      "max_depth =  9 epsilon = 0.1\n",
      "mse = 2060.839588017903 r2 = -0.3360830194657338\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sav4u\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_depth =  9 epsilon = 0.2\n",
      "mse = 2060.8745281700512 r2 = -0.3361056718566593\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sav4u\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sav4u\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_depth =  9 epsilon = 0.3\n",
      "mse = 2001.8150307336302 r2 = -0.2978162328718055\n",
      "-------------------------------------------\n",
      "max_depth =  9 epsilon = 0.4\n",
      "mse = 1892.426629543598 r2 = -0.22689757127084387\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sav4u\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sav4u\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_depth =  9 epsilon = 0.5\n",
      "mse = 1922.122858666596 r2 = -0.24615022329873004\n",
      "-------------------------------------------\n",
      "max_depth =  9 epsilon = 0.6\n",
      "mse = 1919.5747608443505 r2 = -0.2444982411395975\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sav4u\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sav4u\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_depth =  9 epsilon = 0.7\n",
      "mse = 1839.1023452543614 r2 = -0.19232638427586957\n",
      "-------------------------------------------\n",
      "max_depth =  9 epsilon = 0.8\n",
      "mse = 1807.2114207169413 r2 = -0.17165086785176453\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sav4u\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_depth =  9 epsilon = 0.9\n",
      "mse = 1838.4737314132167 r2 = -0.1919188415035753\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sav4u\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sav4u\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_depth =  10 epsilon = 0.1\n",
      "mse = 1927.2717683757214 r2 = -0.24948836318653256\n",
      "-------------------------------------------\n",
      "max_depth =  10 epsilon = 0.2\n",
      "mse = 1828.7947440445612 r2 = -0.18564376277155148\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sav4u\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sav4u\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_depth =  10 epsilon = 0.3\n",
      "mse = 1961.9688741892796 r2 = -0.2719831823717014\n",
      "-------------------------------------------\n",
      "max_depth =  10 epsilon = 0.4\n",
      "mse = 2126.9662278892606 r2 = -0.3789542265117034\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sav4u\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sav4u\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_depth =  10 epsilon = 0.5\n",
      "mse = 1759.3487233190358 r2 = -0.1406205355391592\n",
      "-------------------------------------------\n",
      "max_depth =  10 epsilon = 0.6\n",
      "mse = 1825.4587194254382 r2 = -0.18348095210348947\n",
      "-------------------------------------------\n",
      "max_depth =  10 epsilon = 0.7\n",
      "mse = 1804.7501329423362 r2 = -0.17005516636156215\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sav4u\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sav4u\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_depth =  10 epsilon = 0.8\n",
      "mse = 2195.5351384111373 r2 = -0.4234088058706227\n",
      "-------------------------------------------\n",
      "max_depth =  10 epsilon = 0.9\n",
      "mse = 1824.0608724652848 r2 = -0.18257470030294587\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sav4u\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "eps = [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n",
    "mse_score = 100000\n",
    "r2 = -1\n",
    "for depth in range(2, 11):\n",
    "    for e in eps:\n",
    "        lr = LinearRegression()\n",
    "        dt = DecisionTreeRegressor(max_depth=depth)\n",
    "        svr = LinearSVR(epsilon=e)\n",
    "        \n",
    "        stacking_regressor = StackingRegressionSolver(base_estimators=estimator[0][0], meta_estimator=estimator[0][1][0])\n",
    "        stacking_regressor.fit(X_train, y_train)\n",
    "        y_pred = stacking_regressor.predict(X_test)\n",
    "        print(\"max_depth = \", depth, \"epsilon =\", e)\n",
    "        print(\"mse =\", mse(y_test, y_pred), \"r2 =\", r2_score(y_test, y_pred))\n",
    "        if (mse(y_test, y_pred) < mse_score) and (r2 < r2_score(y_test, y_pred)):\n",
    "            mse_score = mse(y_test, y_pred)\n",
    "            ep = e\n",
    "            d = depth\n",
    "            r2 = r2_score(y_test, y_pred)\n",
    "        print(\"-------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лучшие гиперпараметры и результат ошибки при них"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epsilon = 0.2 max_depth = 3\n",
      "mse = 1716.2914143004964 r2_score = -0.11270563144956469\n"
     ]
    }
   ],
   "source": [
    "print(\"epsilon =\", ep, \"max_depth =\", d)\n",
    "print(\"mse =\", mse_score, \"r2_score =\", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tISuwHbhqjGN"
   },
   "source": [
    "## Использование встроенной модели стекинга (0.5 балла)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "PgHoaeW0nWGh"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sav4u\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sav4u\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sav4u\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sav4u\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sav4u\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse: 1893.802385803341, r2_score: -0.22778950123388086\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sav4u\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import StackingRegressor\n",
    "\n",
    "estimators = [(\"lin_reg\", LinearRegression()), (\"svr\", LinearSVR())]\n",
    "clf = StackingRegressor(estimators=estimators, final_estimator=dt)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(f'mse: {mse(y_test, y_pred)}, r2_score: {r2_score(y_test, y_pred)}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Y95u9XcqmeP"
   },
   "source": [
    "## Блендинг (0.5 балла)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "t6JPX4o4rBP1"
   },
   "outputs": [],
   "source": [
    "class BlandingRegressionSolver:\n",
    "    def __init__(self, base_estimators: list, meta_estimator):\n",
    "        self._base_estimators = base_estimators\n",
    "        self._meta_estimator = meta_estimator\n",
    "\n",
    "    def _fit_base(self, X: pd.DataFrame, y: pd.Series) -> None:\n",
    "        for estimator in self._base_estimators:\n",
    "            estimator.fit(X, y)\n",
    "\n",
    "    def _predict_base(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        cnt = 1\n",
    "        meta_data = pd.DataFrame()\n",
    "        for x in self._base_estimators:\n",
    "            meta_data[f\"{cnt}\"] = x.predict(X)\n",
    "            cnt += 1\n",
    "        return meta_data\n",
    "\n",
    "    def fit(self, X_train: pd.DataFrame, y_train: pd.Series, X_valid: pd.DataFrame, y_valid: pd.Series):\n",
    "        self._fit_base(X_train, y_train)\n",
    "        meta_features = self._predict_base(X_valid)\n",
    "        self._meta_estimator.fit(meta_features, y_valid)\n",
    "\n",
    "\n",
    "    def predict(self, X: pd.DataFrame) -> pd.Series:\n",
    "        meta_features = self._predict_base(X)\n",
    "        return self._meta_estimator.predict(meta_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sav4u\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sav4u\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models: [[LinearRegression(), LinearSVR()], [DecisionTreeRegressor()]] , result:  mse = 1337.9143140540034 R2_score = 0.13260371796964465\n",
      "Models: [[LinearRegression(), DecisionTreeRegressor()], [LinearSVR()]] , result:  mse = 1577.158262489606 R2_score = -0.02250286037504856\n",
      "Models: [[DecisionTreeRegressor(), LinearSVR()], [LinearRegression()]] , result:  mse = 1459.398410084293 R2_score = 0.053843178437635086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sav4u\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "X_valid, X_test_new, y_valid, y_test_new = train_test_split(X_test, y_test, train_size=0.5, random_state=42)\n",
    "\n",
    "for x in estimator:\n",
    "    stacking_regressor = BlandingRegressionSolver(base_estimators=x[0], meta_estimator=x[1][0])\n",
    "    stacking_regressor.fit(X_train, y_train, X_valid, y_valid)\n",
    "    y_pred = stacking_regressor.predict(X_test)\n",
    "    print(\"Models:\", x, \", result: \", \"mse =\", mse(y_test, y_pred),\"R2_score =\", r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Качество заметно улучшилось"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bu3-DYDlqKhT"
   },
   "source": [
    "# Бэггинг "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cWrKsOxerRUC"
   },
   "source": [
    "## Бэггинг своими руками (2 балла)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "M6TGRabcnSxG"
   },
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "\n",
    "class BaggingRegressionSolver:\n",
    "    def __init__(\n",
    "        self,\n",
    "        base_estimator_ctor,\n",
    "        max_samples,\n",
    "        n_estimators: int = 10,\n",
    "        sample_random_state=42,\n",
    "        **model_kwargs\n",
    "    ):\n",
    "        if max_samples < 0 or max_samples > 1:\n",
    "            raise ValueError\n",
    "        self._estimators = [\n",
    "            base_estimator_ctor(**model_kwargs) for _ in range(n_estimators)\n",
    "        ]\n",
    "        self._max_samples = max_samples\n",
    "        self._random_state = sample_random_state\n",
    "\n",
    "    def _sample_data(self, X: pd.DataFrame, y: pd.Series) -> Tuple[pd.DataFrame, pd.Series]:\n",
    "        x_i = X.sample(frac=self._max_samples, random_state=self._random_state)\n",
    "        y_i = y.loc[x_i.index]\n",
    "        return x_i, y_i\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, y: pd.Series):\n",
    "        for estimator in self._estimators:\n",
    "            x_i, y_i = self._sample_data(X, y)\n",
    "            estimator.fit(x_i, y_i)\n",
    "\n",
    "    def predict(self, X: pd.DataFrame) -> pd.Series:\n",
    "        ans = pd.DataFrame()\n",
    "        for i, x in enumerate(self._estimators):\n",
    "            ans[f'{i}'] = x.predict(X)\n",
    "        return ans.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "R7E2FbSHa_Hf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models: <class 'sklearn.linear_model._base.LinearRegression'> , result:  mse = 1441.6620661155575 R2_score = 0.06534200063700013\n",
      "Models: <class 'sklearn.tree._classes.DecisionTreeRegressor'> , result:  mse = 1848.8823569770495 R2_score = -0.19866695909247967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sav4u\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sav4u\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sav4u\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sav4u\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sav4u\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sav4u\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sav4u\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sav4u\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models: <class 'sklearn.svm._classes.LinearSVR'> , result:  mse = 1567.5880688736365 R2_score = -0.01629831478225241\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sav4u\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sav4u\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "estimators = [LinearRegression, DecisionTreeRegressor, LinearSVR]\n",
    "\n",
    "for x in estimators:\n",
    "    bagging = BaggingRegressionSolver(x, n_estimators=10, max_samples=0.7)\n",
    "    bagging.fit(X_train, y_train)\n",
    "    y_pred = bagging.predict(X_test)\n",
    "    print(\"Models:\", x, \", result: \", \"mse =\", mse(y_test, y_pred),\"R2_score =\", r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лучший результат показала линейная регрессия, возможно это потому что для деревьев и svm необходимо подбирать корректные параметры для лучшей работы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jLVRU23TrTfj"
   },
   "source": [
    "## Использование встроенной модели бэггинга \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "ugjjfpES2kk1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models: LinearRegression() , result:  mse = 1433.4868284798677 R2_score = 0.07064216870861983\n",
      "Models: DecisionTreeRegressor() , result:  mse = 1610.0310546031162 R2_score = -0.04381494094679783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sav4u\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sav4u\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sav4u\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sav4u\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sav4u\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sav4u\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sav4u\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sav4u\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sav4u\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models: LinearSVR() , result:  mse = 1568.7316043896726 R2_score = -0.017039691449323158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sav4u\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingRegressor\n",
    "\n",
    "estimators = [LinearRegression(), DecisionTreeRegressor(), LinearSVR()]\n",
    "\n",
    "for x in estimators:\n",
    "    model = BaggingRegressor(base_estimator=x, n_estimators=10, random_state=12)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(\"Models:\", x, \", result: \", \"mse =\", mse(y_test, y_pred),\"R2_score =\", r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse = 1563.956535590414 R2_score = -0.013943920009098498\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "regr = RandomForestRegressor()\n",
    "regr.fit(X_train, y_train)\n",
    "y_pred = regr.predict(X_test)\n",
    "print(\"mse =\", mse(y_test, y_pred),\"R2_score =\", r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лучший результат показала линейная регрессия, но попробуем подобрать параметры для деревьев"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_depth =  3 n_estimators = 20\n",
      "mse = 1571.8470830368415 r2 = -0.019059517806596205\n",
      "-------------------------------------------\n",
      "max_depth =  5 n_estimators = 20\n",
      "mse = 1622.2696853311613 r2 = -0.05174948703778193\n",
      "-------------------------------------------\n",
      "max_depth =  7 n_estimators = 20\n",
      "mse = 1467.1919484079958 r2 = 0.04879047357094046\n",
      "-------------------------------------------\n",
      "max_depth =  9 n_estimators = 20\n",
      "mse = 1468.7023755953603 r2 = 0.0478112337849278\n",
      "-------------------------------------------\n",
      "max_depth =  3 n_estimators = 30\n",
      "mse = 1559.155972368717 r2 = -0.010831620031131806\n",
      "-------------------------------------------\n",
      "max_depth =  5 n_estimators = 30\n",
      "mse = 1543.4579474674576 r2 = -0.0006542803528430952\n",
      "-------------------------------------------\n",
      "max_depth =  7 n_estimators = 30\n",
      "mse = 1489.0727838121138 r2 = 0.03460469569422364\n",
      "-------------------------------------------\n",
      "max_depth =  9 n_estimators = 30\n",
      "mse = 1468.6502387812573 r2 = 0.047845035111541034\n",
      "-------------------------------------------\n",
      "max_depth =  3 n_estimators = 40\n",
      "mse = 1546.5332438971118 r2 = -0.002648055784649772\n",
      "-------------------------------------------\n",
      "max_depth =  5 n_estimators = 40\n",
      "mse = 1500.6128697215677 r2 = 0.02712303000978933\n",
      "-------------------------------------------\n",
      "max_depth =  7 n_estimators = 40\n",
      "mse = 1558.4701797341804 r2 = -0.010387007117440561\n",
      "-------------------------------------------\n",
      "max_depth =  9 n_estimators = 40\n",
      "mse = 1500.9104802341571 r2 = 0.02693008323479673\n",
      "-------------------------------------------\n",
      "max_depth =  3 n_estimators = 50\n",
      "mse = 1526.181324410546 r2 = 0.010546495697005054\n",
      "-------------------------------------------\n",
      "max_depth =  5 n_estimators = 50\n",
      "mse = 1533.1729645586827 r2 = 0.00601367726006663\n",
      "-------------------------------------------\n",
      "max_depth =  7 n_estimators = 50\n",
      "mse = 1504.3357498232986 r2 = 0.024709413289521698\n",
      "-------------------------------------------\n",
      "max_depth =  9 n_estimators = 50\n",
      "mse = 1489.739388262316 r2 = 0.03417252285952366\n",
      "-------------------------------------------\n",
      "max_depth =  3 n_estimators = 60\n",
      "mse = 1541.9775012305427 r2 = 0.000305522190680807\n",
      "-------------------------------------------\n",
      "max_depth =  5 n_estimators = 60\n",
      "mse = 1547.3739762908451 r2 = -0.0031931192052636614\n",
      "-------------------------------------------\n",
      "max_depth =  7 n_estimators = 60\n",
      "mse = 1519.2492821943981 r2 = 0.015040675617200905\n",
      "-------------------------------------------\n",
      "max_depth =  9 n_estimators = 60\n",
      "mse = 1499.9058859977715 r2 = 0.027581381525305093\n",
      "-------------------------------------------\n",
      "max_depth =  3 n_estimators = 70\n",
      "mse = 1584.1960578311728 r2 = -0.027065602135743383\n",
      "-------------------------------------------\n",
      "max_depth =  5 n_estimators = 70\n",
      "mse = 1520.701526384018 r2 = 0.0140991570181066\n",
      "-------------------------------------------\n",
      "max_depth =  7 n_estimators = 70\n",
      "mse = 1515.355807675311 r2 = 0.01756489206855416\n",
      "-------------------------------------------\n",
      "max_depth =  9 n_estimators = 70\n",
      "mse = 1501.064674572405 r2 = 0.026830115999001758\n",
      "-------------------------------------------\n",
      "max_depth =  3 n_estimators = 80\n",
      "mse = 1523.4666642271493 r2 = 0.012306463525527223\n",
      "-------------------------------------------\n",
      "max_depth =  5 n_estimators = 80\n",
      "mse = 1522.569833867877 r2 = 0.012887896365488039\n",
      "-------------------------------------------\n",
      "max_depth =  7 n_estimators = 80\n",
      "mse = 1492.91617323832 r2 = 0.03211295039808093\n",
      "-------------------------------------------\n",
      "max_depth =  9 n_estimators = 80\n",
      "mse = 1484.7046434654087 r2 = 0.03743664737925945\n",
      "-------------------------------------------\n",
      "max_depth =  3 n_estimators = 90\n",
      "mse = 1574.5045533333928 r2 = -0.02078240830161615\n",
      "-------------------------------------------\n",
      "max_depth =  5 n_estimators = 90\n",
      "mse = 1509.4196486537758 r2 = 0.02141342123872103\n",
      "-------------------------------------------\n",
      "max_depth =  7 n_estimators = 90\n",
      "mse = 1525.184474972631 r2 = 0.011192772881656432\n",
      "-------------------------------------------\n",
      "max_depth =  9 n_estimators = 90\n",
      "mse = 1493.2539447200802 r2 = 0.03189396647334286\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "mse_score = 1800\n",
    "r2=-1\n",
    "for i in range(20, 100, 10):\n",
    "    for j in range(3, 10, 2):\n",
    "        regr = RandomForestRegressor(n_estimators=i, max_depth=j)\n",
    "        regr.fit(X_train, y_train)\n",
    "        y_pred = regr.predict(X_test)\n",
    "        print(\"max_depth = \", j, \"n_estimators =\", i)\n",
    "        print(\"mse =\", mse(y_test, y_pred), \"r2 =\", r2_score(y_test, y_pred))\n",
    "        if (mse(y_test, y_pred) < mse_score) and (r2 < r2_score(y_test, y_pred)):\n",
    "            mse_score = mse(y_test, y_pred)\n",
    "            n = i\n",
    "            d = j\n",
    "            r2 = r2_score(y_test, y_pred)\n",
    "        print(\"-------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лучший показатель\n",
      "n_estimators = 20 max_depth = 7\n",
      "mse = 1467.1919484079958 r2_score = 0.04879047357094046\n"
     ]
    }
   ],
   "source": [
    "print(\"Лучший показатель\")\n",
    "print(\"n_estimators =\",n , \"max_depth =\", d)\n",
    "print(\"mse =\", mse_score, \"r2_score =\", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H-jNmNEEqMU6"
   },
   "source": [
    "# Бустинг"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dd9LV-ot4cke"
   },
   "source": [
    "## Бустинг своими руками (2 балла)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from collections import deque\n",
    "from typing import Tuple\n",
    "\n",
    "from sklearn.dummy import DummyRegressor\n",
    "\n",
    "\n",
    "class Loss(ABC):\n",
    "    \"\"\"\n",
    "    Базовый класс для функции потерь\n",
    "    \"\"\"\n",
    "    @abstractmethod\n",
    "    def forward(self, y_true: pd.Series, y_pred: pd.Series) -> float:\n",
    "        \"\"\"\n",
    "        Метод, вычисляющий значение функции потерь\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def backward(self, y_true: pd.Series, y_pred: pd.Series) -> pd.Series:\n",
    "        \"\"\"\n",
    "        Метод, вычисляющий значение градиента функции потерь по предсказаниям модели\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class MSELoss(Loss):\n",
    "    def forward(self, y_pred: pd.Series, y_true: pd.Series) -> float:  # посчитаем значение ошибки\n",
    "        return ((np.array(y_pred) - np.array(y_true)) ** 2).mean()\n",
    "\n",
    "    def backward(self, y_pred: pd.Series, y_true: pd.Series) -> pd.Series:  # посчитаем производную по выходам модели\n",
    "        return np.array(y_true) - np.array(y_pred)\n",
    "\n",
    "\n",
    "class GradientBoostingRegressionSolver:\n",
    "    def __init__(\n",
    "        self,\n",
    "        base_estimator_ctor,\n",
    "        n_estimators: int = 10,\n",
    "        loss: Loss = MSELoss(),\n",
    "        learning_rate: float = 0.1,\n",
    "        early_stopping: int = 5,\n",
    "        **model_kwargs\n",
    "    ):\n",
    "        if early_stopping < 0:\n",
    "            raise ValueError\n",
    "\n",
    "        self._ctor = base_estimator_ctor\n",
    "        self._kwargs = model_kwargs\n",
    "        self._n_estimators = n_estimators\n",
    "        self._estimators = []\n",
    "        self._early_stopping = early_stopping\n",
    "        self._loss = loss\n",
    "        self._lr = learning_rate\n",
    "        self._random_state = 42\n",
    "\n",
    "    def _sample_data(self, X: pd.DataFrame, y: pd.Series, frac: float) -> Tuple[pd.DataFrame, pd.Series]:\n",
    "        x_sample = X.sample(frac=frac, random_state=self._random_state)\n",
    "        y_sample = y.loc[x_sample.index]\n",
    "        return x_sample, y_sample\n",
    "\n",
    "    def _split_data(self, X: pd.DataFrame, y: pd.Series, val_size: float) -> Tuple[pd.DataFrame, pd.DataFrame, pd.Series, pd.Series]:\n",
    "        x_val, y_val = self._sample_data(X, y, val_size)\n",
    "        x_train, y_train = X[~X.index.isin(x_val.index)], y[~y.index.isin(y_val.index)]\n",
    "        return x_train, x_val, y_train, y_val\n",
    "\n",
    "    def predict(self, X: pd.DataFrame) -> pd.Series:\n",
    "        data = pd.DataFrame()\n",
    "        for i, x in enumerate(self._estimators):\n",
    "            data[f'{i}'] = x.predict(X)\n",
    "        return data.sum(axis=1)\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, y: pd.Series, val_size: float = 0.1):\n",
    "        x_train, x_val, y_train, y_val = self._split_data(X, y, val_size)  # Хотим получить валидационную выборку, не тратя на это время снаружи\n",
    "        base_estimator = DummyRegressor()  # Создадим и обучим базовую модель\n",
    "        base_estimator.fit(x_train, y_train)\n",
    "        self._estimators.append(base_estimator)  # Добавим базовую модель в список моделей\n",
    "\n",
    "        y_pred_train, y_pred_val = base_estimator.predict(x_train), base_estimator.predict(x_val)  # Посчитаем предсказания как на обучающей, так и на валидационной выборках\n",
    "        train_loss= self._loss.forward(y_pred_train, y_train)  # Посчитаем значение функции потерь для обучения\n",
    "        val_loss = self._loss.forward(y_pred_val, y_val) # Посчитаем значение функции потерь для валидации\n",
    "        residuals = self._loss.backward(y_pred_train, y_train)*self._lr  # Посчитаем остатки, используя градиент функции потерь\n",
    "        \n",
    "        print(f'train loss: {train_loss}, val loss: {val_loss}')\n",
    "     \n",
    "        previous_val_loss, cnt = val_loss, 0\n",
    "        for i in range(self._n_estimators - 1):\n",
    "            estimator = self._ctor(**self._kwargs)  # Создадим очередную модель\n",
    "            # 1. Обучим её и добавим в список моделей\n",
    "            estimator.fit(x_train, residuals)\n",
    "            self._estimators.append(estimator)\n",
    "            # 2. Предскажем ВСЕМ ансамблем данные из обучающей выборки, то же самое сделаем для валидационной\n",
    "            y_pred_train = self.predict(x_train)\n",
    "            y_pred_val = self.predict(x_val)\n",
    "            # 3. Посчитаем значения функции потерь (на обучении и валидации)\n",
    "            train_loss = self._loss.forward(y_pred_train, y_train)\n",
    "            val_loss =  self._loss.forward(y_pred_val, y_val)\n",
    "            # 4. Обновим остатки для обучающей выборки\n",
    "            residuals = self._loss.backward(y_pred_train, y_train) * self._lr\n",
    "            \n",
    "            print(f'train loss: {train_loss}, val loss: {val_loss}')\n",
    "            # Если валидационный лосс несколько (self._early_stopping) шагов подряд не уменьшается, то остановим обучение\n",
    "            if previous_val_loss == val_loss:\n",
    "                cnt += 1\n",
    "            else:\n",
    "                cnt = 0\n",
    "            previous_val_loss = val_loss\n",
    "            if cnt == self._early_stopping:\n",
    "                print(f'Валидационный лосс несколько шагов = {self._early_stopping} не уменьшается')\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1_Fgwz-JfRw6"
   },
   "source": [
    "Вопросы на дополнительный балл:\n",
    "- Почему градиент по ответам мы берем со знаком минус?\n",
    "- Почему в обучении мы домножаем на `learning_rate`, а в предсказаниях этого не делаем?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1)Потому что мы ищем минимальное значение функции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 1100.3419290353834, val loss: 586.5082683988813\n",
      "train loss: 1080.950609394889, val loss: 572.7706821503102\n",
      "train loss: 1065.2436404860885, val loss: 562.406518392163\n",
      "train loss: 1052.5209956699603, val loss: 554.6984987409395\n",
      "train loss: 1042.2156533688963, val loss: 549.073260517037\n",
      "train loss: 1033.8683261050344, val loss: 545.0732494799055\n",
      "train loss: 1027.1069910213064, val loss: 542.3340292716354\n",
      "train loss: 1021.6303096034866, val loss: 540.5659707615624\n",
      "train loss: 1017.1941976550527, val loss: 539.5394822411668\n",
      "train loss: 1013.6009469768213, val loss: 539.0731015251331\n",
      "base_estimator_ctor: <class 'sklearn.linear_model._base.LinearRegression'>\n",
      "mse: 1455.0188937240011, r2_score: 0.05668250541702691\n",
      "==========================================\n",
      "train loss: 1100.3419290353834, val loss: 586.5082683988813\n",
      "train loss: 948.1603676623253, val loss: 577.5308661362935\n",
      "train loss: 824.8933029501483, val loss: 577.8070197390588\n",
      "train loss: 725.0469805332847, val loss: 583.0245027845549\n",
      "train loss: 644.1714593756253, val loss: 591.6251876556696\n",
      "train loss: 578.6622872379213, val loss: 604.4610105866034\n",
      "train loss: 525.599857806381, val loss: 619.4413294961128\n",
      "train loss: 482.6192899668334, val loss: 635.5826310114638\n",
      "train loss: 447.8050300167998, val loss: 651.6178303089413\n",
      "train loss: 419.6054794572726, val loss: 668.0792057483105\n",
      "base_estimator_ctor: <class 'sklearn.tree._classes.DecisionTreeRegressor'>\n",
      "mse: 1712.2607375363277, r2_score: -0.11009246407211704\n",
      "==========================================\n",
      "train loss: 1100.3419290353834, val loss: 586.5082683988813\n",
      "train loss: 994.3782031129689, val loss: 569.334317196049\n",
      "train loss: 900.0712446471165, val loss: 558.3199704405374\n",
      "train loss: 820.2307007347952, val loss: 550.99889672161\n",
      "train loss: 753.9453410690863, val loss: 546.8024805479216\n",
      "train loss: 698.1299037696997, val loss: 544.6335237736239\n",
      "train loss: 648.1699040256374, val loss: 544.14882416169\n",
      "train loss: 605.7840172708665, val loss: 546.0433636325184\n",
      "train loss: 568.7105386493581, val loss: 548.0963013563419\n",
      "train loss: 536.4263350494094, val loss: 552.0958659749527\n",
      "base_estimator_ctor: <class 'sklearn.ensemble._forest.RandomForestRegressor'>\n",
      "mse: 1497.7356623206877, r2_score: 0.02898838041073648\n",
      "==========================================\n"
     ]
    }
   ],
   "source": [
    "estimators = [LinearRegression, DecisionTreeRegressor, RandomForestRegressor]\n",
    "\n",
    "for x in estimators:\n",
    "    boost = GradientBoostingRegressionSolver(x)\n",
    "    boost.fit(X_train, y_train)\n",
    "    y_pred = boost.predict(X_test)\n",
    "\n",
    "    print(f'base_estimator_ctor: {x}')\n",
    "    print(f'mse: {mse(y_test, y_pred)}, r2_score: {r2_score(y_test, y_pred)}')\n",
    "    print('==========================================')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лучший результат вновь показала линейная регрессия, random forest немножко хуже, попробуем подобрать под него гиперпараметры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "def plot_fitting_curve(parameter: str, values: list, X_train, X_test, y_train, y_test):\n",
    "    train_curve = []\n",
    "    test_curve = []\n",
    "    for value in values:\n",
    "        model = GradientBoostingRegressionSolver(base_estimator_ctor=RandomForestRegressor, **{parameter: value})\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred_train, y_pred_test = model.predict(X_train), model.predict(X_test)\n",
    "        train_curve.append(mse(y_train, y_pred_train))\n",
    "        test_curve.append(mse(y_test, y_pred_test))\n",
    "    sns.lineplot(x=values, y=train_curve, label='train')\n",
    "    sns.lineplot(x=values, y=test_curve, label='test').set_title(f'{parameter}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 1100.3419290353834, val loss: 586.5082683988813\n",
      "train loss: 1066.2124331726739, val loss: 577.6577984016146\n",
      "train loss: 1034.5106325448546, val loss: 571.3761781066081\n",
      "train loss: 1010.6220957623398, val loss: 565.5741659206208\n",
      "train loss: 990.5454641459972, val loss: 560.5125522164446\n",
      "train loss: 971.7987448859128, val loss: 556.3681016740063\n",
      "train loss: 955.4178185392169, val loss: 553.0214911942189\n",
      "train loss: 940.167781072183, val loss: 550.3100649687271\n",
      "train loss: 925.5484159581231, val loss: 547.3165436598102\n",
      "train loss: 912.8181732257956, val loss: 544.9135518221677\n",
      "train loss: 1100.3419290353834, val loss: 586.5082683988813\n",
      "train loss: 1062.7770442426652, val loss: 575.4489603502867\n",
      "train loss: 1026.9301987176025, val loss: 566.3679535537684\n",
      "train loss: 991.2155467583598, val loss: 559.1267855264899\n",
      "train loss: 961.038618545772, val loss: 552.7349437549764\n",
      "train loss: 933.6782889137601, val loss: 548.020031237594\n",
      "train loss: 909.8866205435903, val loss: 543.9079075243842\n",
      "train loss: 889.3951240300821, val loss: 540.5622406355435\n",
      "train loss: 870.8809566988376, val loss: 537.4413095480379\n",
      "train loss: 854.398256171939, val loss: 535.154900644816\n",
      "train loss: 1100.3419290353834, val loss: 586.5082683988813\n",
      "train loss: 1053.4957198456316, val loss: 573.588406683194\n",
      "train loss: 1013.2611442559937, val loss: 562.5564412413138\n",
      "train loss: 975.4381202696192, val loss: 553.9928330218097\n",
      "train loss: 939.6036870342493, val loss: 547.0729527745685\n",
      "train loss: 907.5995589153474, val loss: 541.449330531321\n",
      "train loss: 879.6519974196166, val loss: 537.260653227917\n",
      "train loss: 854.3260633972546, val loss: 533.3435624123098\n",
      "train loss: 830.8685249174978, val loss: 530.3666224482822\n",
      "train loss: 810.0885985936302, val loss: 527.8051823835559\n",
      "train loss: 1100.3419290353834, val loss: 586.5082683988813\n",
      "train loss: 1043.212110928287, val loss: 572.0252973185987\n",
      "train loss: 994.6777491583711, val loss: 560.5083161886128\n",
      "train loss: 951.0661120766856, val loss: 551.4942305322418\n",
      "train loss: 912.0167792088249, val loss: 543.9908764033503\n",
      "train loss: 877.955477710103, val loss: 539.0430607410052\n",
      "train loss: 846.0439896959608, val loss: 534.6306798990742\n",
      "train loss: 817.1862476269537, val loss: 531.1798454749647\n",
      "train loss: 792.6432386804942, val loss: 528.6087830517706\n",
      "train loss: 767.9313230443718, val loss: 526.5246973567929\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_fitting_curve('max_depth', range(3, 10, 2), X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оптимальная глубина равна 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 1100.3419290353834, val loss: 586.5082683988813\n",
      "train loss: 1089.4318012232804, val loss: 584.585445581818\n",
      "train loss: 1078.1073140982546, val loss: 582.6393110594427\n",
      "train loss: 1066.9979311644097, val loss: 580.8953495943692\n",
      "train loss: 1055.9742174764617, val loss: 579.0494710309487\n",
      "train loss: 1045.1994152391117, val loss: 577.4276932909353\n",
      "train loss: 1034.8317740025605, val loss: 575.6860032655793\n",
      "train loss: 1024.2428340245988, val loss: 574.0793301948862\n",
      "train loss: 1013.91907589868, val loss: 572.5046721035684\n",
      "train loss: 1004.0238679114916, val loss: 571.0534831480907\n",
      "train loss: 1100.3419290353834, val loss: 586.5082683988813\n",
      "train loss: 1044.5949756832078, val loss: 577.3924873282563\n",
      "train loss: 994.3254099922859, val loss: 569.4859271322548\n",
      "train loss: 948.2990507507574, val loss: 562.9387836209402\n",
      "train loss: 905.4377775337892, val loss: 557.5357997333531\n",
      "train loss: 865.2299760456839, val loss: 552.9402571062452\n",
      "train loss: 826.6828175028988, val loss: 549.4654802379501\n",
      "train loss: 792.3169271534142, val loss: 546.9627461873458\n",
      "train loss: 761.6961958142122, val loss: 545.244680969042\n",
      "train loss: 730.1574700072706, val loss: 543.8449353435624\n",
      "train loss: 1100.3419290353834, val loss: 586.5082683988813\n",
      "train loss: 1089.3695478705642, val loss: 584.5884575465652\n",
      "train loss: 1078.358033449535, val loss: 582.6479158760189\n",
      "train loss: 1067.667731336884, val loss: 580.654782102243\n",
      "train loss: 1056.6239059537152, val loss: 578.9039498564943\n",
      "train loss: 1045.746804810436, val loss: 577.1925404147805\n",
      "train loss: 1035.2209158159028, val loss: 575.5532505568242\n",
      "train loss: 1025.2817620450212, val loss: 573.9020631785629\n",
      "train loss: 1015.4533090663634, val loss: 572.3787056928923\n",
      "train loss: 1005.8501306910491, val loss: 570.8618177340788\n",
      "train loss: 1100.3419290353834, val loss: 586.5082683988813\n",
      "train loss: 1088.7531220393964, val loss: 584.5724304545688\n",
      "train loss: 1077.8963204457323, val loss: 582.624109376239\n",
      "train loss: 1066.9759207727773, val loss: 580.7758514990027\n",
      "train loss: 1056.006002938669, val loss: 579.0419021240622\n",
      "train loss: 1045.649414861362, val loss: 577.3997715368779\n",
      "train loss: 1035.0521799792418, val loss: 575.8259606815806\n",
      "train loss: 1024.9195620627945, val loss: 574.2055148288808\n",
      "train loss: 1015.1715837833582, val loss: 572.6108966621231\n",
      "train loss: 1005.2292422250873, val loss: 571.1179870347461\n",
      "train loss: 995.3017547674908, val loss: 569.6537957799776\n",
      "train loss: 985.8923365018688, val loss: 568.2424793587777\n",
      "train loss: 976.5203523782641, val loss: 566.9146650000373\n",
      "train loss: 967.1617541823239, val loss: 565.5727648463226\n",
      "train loss: 958.2199790402312, val loss: 564.2719513303964\n",
      "train loss: 949.1917440316377, val loss: 563.0420276009247\n",
      "train loss: 940.3174194095446, val loss: 561.8064496810841\n",
      "train loss: 931.5123022479703, val loss: 560.7703668356812\n",
      "train loss: 923.1103783131144, val loss: 559.72125544868\n",
      "train loss: 914.594781118321, val loss: 558.6900158332459\n",
      "train loss: 1100.3419290353834, val loss: 586.5082683988813\n",
      "train loss: 1045.2555530193167, val loss: 576.8623818099385\n",
      "train loss: 995.5136287715904, val loss: 569.1349333718446\n",
      "train loss: 947.9955635933097, val loss: 562.897488168082\n",
      "train loss: 902.5440875097404, val loss: 557.700095979001\n",
      "train loss: 861.4580236724152, val loss: 553.9357280519044\n",
      "train loss: 823.8518649133769, val loss: 550.4437845968149\n",
      "train loss: 789.7943671126684, val loss: 548.2170323876759\n",
      "train loss: 757.4206735483888, val loss: 546.7217977095208\n",
      "train loss: 728.3756572279035, val loss: 545.4511414781837\n",
      "train loss: 699.7461769431769, val loss: 545.0259887853897\n",
      "train loss: 674.3220818953229, val loss: 544.3783469196594\n",
      "train loss: 650.9342246204217, val loss: 544.360573945153\n",
      "train loss: 629.1034691177691, val loss: 544.7931356244045\n",
      "train loss: 609.034582637027, val loss: 546.0564178093136\n",
      "train loss: 589.6641230122327, val loss: 547.0745432391367\n",
      "train loss: 571.606910172452, val loss: 548.4352319077539\n",
      "train loss: 555.1855550498244, val loss: 549.9912355049845\n",
      "train loss: 539.8701446913699, val loss: 551.4774961178925\n",
      "train loss: 525.551439072119, val loss: 553.5180085102259\n",
      "train loss: 1100.3419290353834, val loss: 586.5082683988813\n",
      "train loss: 1089.4782304610733, val loss: 584.526152021348\n",
      "train loss: 1078.5604255676303, val loss: 582.6346379839601\n",
      "train loss: 1067.7440326678516, val loss: 580.7624725332913\n",
      "train loss: 1057.0431924621685, val loss: 578.9647342615816\n",
      "train loss: 1046.6535157637227, val loss: 577.2162405179639\n",
      "train loss: 1036.2664789049907, val loss: 575.44949462953\n",
      "train loss: 1025.954584752283, val loss: 573.8578195296647\n",
      "train loss: 1015.639437909046, val loss: 572.3350016303888\n",
      "train loss: 1005.7141859521327, val loss: 570.8544734721906\n",
      "train loss: 995.8577280690718, val loss: 569.5081788525082\n",
      "train loss: 985.9364281369184, val loss: 568.1540248703888\n",
      "train loss: 976.0506100957168, val loss: 566.806194359242\n",
      "train loss: 966.5258927625011, val loss: 565.4819910932677\n",
      "train loss: 957.3818011354266, val loss: 564.3174275234677\n",
      "train loss: 948.1044194397463, val loss: 563.1327191093313\n",
      "train loss: 938.8596015973618, val loss: 561.9721561799954\n",
      "train loss: 930.2199217627602, val loss: 560.8749087082936\n",
      "train loss: 921.3981689550368, val loss: 559.7811799364057\n",
      "train loss: 913.0452390732562, val loss: 558.7642553427581\n",
      "train loss: 1100.3419290353834, val loss: 586.5082683988813\n",
      "train loss: 1089.630287279953, val loss: 584.6430769077049\n",
      "train loss: 1078.778185797512, val loss: 582.7961631079669\n",
      "train loss: 1068.2290515485738, val loss: 580.9715787229891\n",
      "train loss: 1057.605355446816, val loss: 579.2344491454509\n",
      "train loss: 1047.055096293418, val loss: 577.4537773322113\n",
      "train loss: 1036.5527724749381, val loss: 575.7929235566004\n",
      "train loss: 1026.5638461849742, val loss: 574.1679389648559\n",
      "train loss: 1016.3615411788261, val loss: 572.5485225316322\n",
      "train loss: 1006.6325240281685, val loss: 571.1758987288684\n",
      "train loss: 997.1831737398885, val loss: 569.7210564993618\n",
      "train loss: 988.1428031100904, val loss: 568.2914673917176\n",
      "train loss: 978.7134840326873, val loss: 566.9441463257772\n",
      "train loss: 969.8131711628552, val loss: 565.6723428199139\n",
      "train loss: 960.5248621638758, val loss: 564.3788717990934\n",
      "train loss: 951.3286020612436, val loss: 563.1423110481596\n",
      "train loss: 942.7929828444982, val loss: 562.148734876918\n",
      "train loss: 933.8991860992172, val loss: 561.0777325690955\n",
      "train loss: 925.3495076510238, val loss: 560.0259444847433\n",
      "train loss: 916.8029220286982, val loss: 558.9349889512404\n",
      "train loss: 908.2860798303216, val loss: 557.9900641211262\n",
      "train loss: 899.9296221027045, val loss: 557.0036157536466\n",
      "train loss: 891.4940317929378, val loss: 556.1198188722352\n",
      "train loss: 883.7253828185978, val loss: 555.2181894817852\n",
      "train loss: 875.7899277691242, val loss: 554.3226811993147\n",
      "train loss: 868.0467134215154, val loss: 553.404057115963\n",
      "train loss: 860.3597708283206, val loss: 552.6795377163446\n",
      "train loss: 852.6878780588871, val loss: 551.9158632268684\n",
      "train loss: 845.4102606058241, val loss: 551.1315438757209\n",
      "train loss: 837.8706837278455, val loss: 550.4896926293385\n",
      "train loss: 1100.3419290353834, val loss: 586.5082683988813\n",
      "train loss: 1044.7952290595981, val loss: 577.4695415056682\n",
      "train loss: 995.951114795838, val loss: 569.4693141311392\n",
      "train loss: 948.959323968414, val loss: 562.8162004569843\n",
      "train loss: 904.3083202380956, val loss: 557.1814920765315\n",
      "train loss: 863.3011716299587, val loss: 552.6519890426691\n",
      "train loss: 825.5818505466139, val loss: 549.1350062554532\n",
      "train loss: 790.2048592165967, val loss: 546.3843198561576\n",
      "train loss: 758.7246734701946, val loss: 544.6722546783993\n",
      "train loss: 730.6855069142965, val loss: 543.6249017008098\n",
      "train loss: 703.2526544469875, val loss: 542.8304611712492\n",
      "train loss: 677.649353143158, val loss: 542.2415346835253\n",
      "train loss: 653.3811903719825, val loss: 542.2120724475332\n",
      "train loss: 631.4979063773147, val loss: 542.9516705068593\n",
      "train loss: 610.5468626984627, val loss: 543.7016151180126\n",
      "train loss: 590.8426650216976, val loss: 545.0261463071254\n",
      "train loss: 572.5954435221416, val loss: 546.232253071805\n",
      "train loss: 556.3365746583148, val loss: 547.859892153429\n",
      "train loss: 541.3267762338534, val loss: 549.4371634094107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 526.2708942826512, val loss: 551.1435213313525\n",
      "train loss: 512.9399661977817, val loss: 553.0465335064657\n",
      "train loss: 499.96557364131064, val loss: 554.9192672660018\n",
      "train loss: 488.656112560226, val loss: 557.0722111641579\n",
      "train loss: 477.563430861538, val loss: 559.2176498404548\n",
      "train loss: 467.16064242482537, val loss: 561.5471756126782\n",
      "train loss: 457.53895724408875, val loss: 563.8265987407408\n",
      "train loss: 448.38043743891393, val loss: 566.2107659810837\n",
      "train loss: 440.2375506250019, val loss: 568.4692212236448\n",
      "train loss: 432.3676680193192, val loss: 571.1132956912708\n",
      "train loss: 424.67109009219934, val loss: 573.6229818759051\n",
      "train loss: 1100.3419290353834, val loss: 586.5082683988813\n",
      "train loss: 1089.549645527083, val loss: 584.5679614833084\n",
      "train loss: 1078.1667561202923, val loss: 582.676618236\n",
      "train loss: 1067.032114686964, val loss: 580.8507533862191\n",
      "train loss: 1056.7263118286558, val loss: 579.0535669431673\n",
      "train loss: 1046.369320492944, val loss: 577.4329417043317\n",
      "train loss: 1036.2683186813595, val loss: 575.7053268239245\n",
      "train loss: 1026.3686197587244, val loss: 574.1195530505846\n",
      "train loss: 1016.1560774152241, val loss: 572.5412165316968\n",
      "train loss: 1006.2652466325211, val loss: 571.0336356938833\n",
      "train loss: 996.6797547831461, val loss: 569.5961496848561\n",
      "train loss: 987.2951119107771, val loss: 568.1557996176651\n",
      "train loss: 978.1188021068249, val loss: 566.8253578764723\n",
      "train loss: 968.9677231605124, val loss: 565.5711026507656\n",
      "train loss: 959.5517988503523, val loss: 564.4769796558062\n",
      "train loss: 950.5153336410334, val loss: 563.1086843807227\n",
      "train loss: 941.6734774580725, val loss: 561.9375300769028\n",
      "train loss: 932.9000113952374, val loss: 560.7590973281884\n",
      "train loss: 924.373148343806, val loss: 559.7657431013895\n",
      "train loss: 916.0004019098494, val loss: 558.7188843391738\n",
      "train loss: 907.3850361367192, val loss: 557.7601370533492\n",
      "train loss: 899.224009340908, val loss: 556.7244617860133\n",
      "train loss: 891.1266273739851, val loss: 555.8097370273125\n",
      "train loss: 883.0162235593856, val loss: 554.8341570127399\n",
      "train loss: 874.8784284462622, val loss: 553.9317442978231\n",
      "train loss: 867.0540947028966, val loss: 553.096466560455\n",
      "train loss: 859.2830191627858, val loss: 552.4768732699491\n",
      "train loss: 851.5620035613766, val loss: 551.7111701383446\n",
      "train loss: 844.02697842465, val loss: 551.0358602190538\n",
      "train loss: 837.0539051701164, val loss: 550.3881128793149\n",
      "train loss: 1100.3419290353834, val loss: 586.5082683988813\n",
      "train loss: 1089.3911928361792, val loss: 584.5250252766045\n",
      "train loss: 1078.5525368644078, val loss: 582.5421077322733\n",
      "train loss: 1068.2695233190238, val loss: 580.6912602382937\n",
      "train loss: 1057.378810360137, val loss: 578.9266848433622\n",
      "train loss: 1046.865198008609, val loss: 577.1596502866549\n",
      "train loss: 1036.240707021285, val loss: 575.394698220138\n",
      "train loss: 1026.2840073301797, val loss: 573.7780995950529\n",
      "train loss: 1016.308636279299, val loss: 572.1966168073839\n",
      "train loss: 1006.1653901519653, val loss: 570.8256114515873\n",
      "train loss: 996.333368426831, val loss: 569.4318503459074\n",
      "train loss: 986.4067529033518, val loss: 568.0162582019803\n",
      "train loss: 977.0143713887683, val loss: 566.7747758238319\n",
      "train loss: 967.3517967455048, val loss: 565.3124899516331\n",
      "train loss: 958.0186400298375, val loss: 564.039996380156\n",
      "train loss: 948.6410601086196, val loss: 562.8950887204987\n",
      "train loss: 940.0806475357156, val loss: 561.7858896865632\n",
      "train loss: 931.597951570527, val loss: 560.6303143873109\n",
      "train loss: 922.9616507189369, val loss: 559.4564437698194\n",
      "train loss: 914.0845190910084, val loss: 558.4622914875225\n",
      "train loss: 905.8132844914594, val loss: 557.42854256142\n",
      "train loss: 897.2812862092377, val loss: 556.4770149490453\n",
      "train loss: 889.0345957898104, val loss: 555.5763606493787\n",
      "train loss: 880.8607577757181, val loss: 554.7444529418827\n",
      "train loss: 872.9508957184418, val loss: 554.0113546181502\n",
      "train loss: 865.1411851287004, val loss: 553.1733392154367\n",
      "train loss: 857.3613674583949, val loss: 552.3931260989325\n",
      "train loss: 849.7560413944019, val loss: 551.6503366457196\n",
      "train loss: 842.5141476784469, val loss: 550.8960126931058\n",
      "train loss: 835.6790821943891, val loss: 550.1989996820346\n",
      "train loss: 828.7513852453789, val loss: 549.41229275142\n",
      "train loss: 821.7486150376614, val loss: 548.7937099170742\n",
      "train loss: 814.7176675282724, val loss: 548.2089896953348\n",
      "train loss: 807.9745858357794, val loss: 547.5679878776785\n",
      "train loss: 801.23965630793, val loss: 547.0080412297547\n",
      "train loss: 794.4375969946439, val loss: 546.5010488863818\n",
      "train loss: 787.907218298512, val loss: 546.0078253922609\n",
      "train loss: 781.4828497598634, val loss: 545.5780734283561\n",
      "train loss: 775.1997961438892, val loss: 545.1960464687672\n",
      "train loss: 768.9750758599242, val loss: 544.7799567951964\n",
      "train loss: 1100.3419290353834, val loss: 586.5082683988813\n",
      "train loss: 1044.8414590464172, val loss: 577.3404564527979\n",
      "train loss: 993.0300910224634, val loss: 569.5770015717885\n",
      "train loss: 945.2090320650328, val loss: 563.3835666230549\n",
      "train loss: 902.3596185970988, val loss: 557.6284926773691\n",
      "train loss: 861.6897249621256, val loss: 553.2716649870274\n",
      "train loss: 823.8036382111494, val loss: 550.2098419087305\n",
      "train loss: 788.7211866838984, val loss: 547.2194157824185\n",
      "train loss: 756.1962378030864, val loss: 545.3629331627604\n",
      "train loss: 725.5889294408308, val loss: 544.0011293384999\n",
      "train loss: 697.3497789654342, val loss: 543.1211484293401\n",
      "train loss: 672.4824113724931, val loss: 542.8490341496058\n",
      "train loss: 649.3585629839586, val loss: 542.9818474544605\n",
      "train loss: 628.1272985786886, val loss: 543.6820188076791\n",
      "train loss: 608.4351803194194, val loss: 544.1942596387796\n",
      "train loss: 589.5454930468647, val loss: 545.4799128653501\n",
      "train loss: 572.0476301920444, val loss: 546.5599158434148\n",
      "train loss: 555.2544258274994, val loss: 547.8371532078995\n",
      "train loss: 539.6697142719975, val loss: 549.5709252169555\n",
      "train loss: 525.0405485801584, val loss: 551.1790969776703\n",
      "train loss: 512.3588101880675, val loss: 553.11968093996\n",
      "train loss: 499.1286116817296, val loss: 555.051655964038\n",
      "train loss: 486.85211780389807, val loss: 557.3919027051222\n",
      "train loss: 476.0598772035954, val loss: 559.6595250426946\n",
      "train loss: 465.7691877470638, val loss: 562.0114960312644\n",
      "train loss: 456.31744362285133, val loss: 564.2006959389715\n",
      "train loss: 447.43952933410486, val loss: 566.3893869316047\n",
      "train loss: 439.35760861481594, val loss: 568.7476146199925\n",
      "train loss: 431.71458271511943, val loss: 571.3002223898602\n",
      "train loss: 424.4516337642304, val loss: 573.2215396779517\n",
      "train loss: 417.3824687771714, val loss: 575.7736982604393\n",
      "train loss: 410.8056119550151, val loss: 578.1498464518055\n",
      "train loss: 404.60621480722017, val loss: 581.0123498443044\n",
      "train loss: 398.4383019818845, val loss: 583.6317218327437\n",
      "train loss: 392.72760506276995, val loss: 586.2186312939343\n",
      "train loss: 387.70453627374803, val loss: 588.3981589342213\n",
      "train loss: 382.8548393674548, val loss: 590.559383498878\n",
      "train loss: 378.3062703132526, val loss: 592.9662944071937\n",
      "train loss: 373.9947314405994, val loss: 594.9268805864374\n",
      "train loss: 370.0963762769328, val loss: 597.1150549790397\n",
      "train loss: 1100.3419290353834, val loss: 586.5082683988813\n",
      "train loss: 1088.8426509254543, val loss: 584.5691692922786\n",
      "train loss: 1078.121128109018, val loss: 582.668228220762\n",
      "train loss: 1067.5831462592937, val loss: 580.7270016559019\n",
      "train loss: 1056.7586220823089, val loss: 578.8946143893916\n",
      "train loss: 1046.2307772949564, val loss: 577.1514059704582\n",
      "train loss: 1036.0349699245423, val loss: 575.5558867237091\n",
      "train loss: 1025.8723531776757, val loss: 574.0281032690034\n",
      "train loss: 1015.3069282483692, val loss: 572.4097740906337\n",
      "train loss: 1005.1826030495407, val loss: 570.9328259743921\n",
      "train loss: 995.0156954123925, val loss: 569.4166909527502\n",
      "train loss: 985.569661910736, val loss: 568.0823566078565\n",
      "train loss: 976.2610747514228, val loss: 566.7360435788069\n",
      "train loss: 966.7744733299454, val loss: 565.4648086675884\n",
      "train loss: 957.6014020824595, val loss: 564.1996650259798\n",
      "train loss: 948.5193075533958, val loss: 563.0055998235443\n",
      "train loss: 939.601235244347, val loss: 561.7189641880558\n",
      "train loss: 931.0950031220447, val loss: 560.4942674185894\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 922.7811711634204, val loss: 559.3374770034969\n",
      "train loss: 914.3658978519022, val loss: 558.4089868500716\n",
      "train loss: 906.482073218578, val loss: 557.3999827134265\n",
      "train loss: 898.0728849802663, val loss: 556.4051741137578\n",
      "train loss: 889.9542300449511, val loss: 555.4632295459232\n",
      "train loss: 881.9379063140171, val loss: 554.6566035324987\n",
      "train loss: 874.3840681260191, val loss: 553.8193395178209\n",
      "train loss: 866.9033007881336, val loss: 552.88014144191\n",
      "train loss: 859.3381092895453, val loss: 552.089757848887\n",
      "train loss: 851.7845969895995, val loss: 551.3061171032874\n",
      "train loss: 844.4030523324324, val loss: 550.6125855223937\n",
      "train loss: 837.2229436318114, val loss: 550.0021215458959\n",
      "train loss: 830.0678680218922, val loss: 549.3684986372326\n",
      "train loss: 822.6902506723817, val loss: 548.7197036665146\n",
      "train loss: 815.8448543746501, val loss: 548.1415746761662\n",
      "train loss: 808.9628242726773, val loss: 547.5990649531915\n",
      "train loss: 802.1194765677872, val loss: 547.0881030356893\n",
      "train loss: 795.581371110102, val loss: 546.6993595593666\n",
      "train loss: 788.7964290817412, val loss: 546.2673955385828\n",
      "train loss: 782.0701618418115, val loss: 545.9146959436963\n",
      "train loss: 775.765070600808, val loss: 545.4851206648063\n",
      "train loss: 769.4539959150579, val loss: 545.1233994756121\n"
     ]
    }
   ],
   "source": [
    "learning_rate = [0.01, 0.05, 0.01]\n",
    "mse_score = 2000\n",
    "r2=-1\n",
    "for i in range(10, 50, 10): # n_estimators\n",
    "    for j in learning_rate:# learning rate\n",
    "        boost = GradientBoostingRegressionSolver(base_estimator_ctor=RandomForestRegressor, learning_rate=j, n_estimators=i)\n",
    "        boost.fit(X_train, y_train)\n",
    "        y_pred = boost.predict(X_test)\n",
    "        if (mse(y_test, y_pred) < mse_score) and (r2 < r2_score(y_test, y_pred)):\n",
    "            mse_score = mse(y_test, y_pred)\n",
    "            n = i\n",
    "            step = j\n",
    "            r2 = r2_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "с подобранными параметрами результат получился такой же как у линейной регрессии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лучший показатель\n",
      "n_estimators = 10 max_depth = 3 lr = 0.05\n",
      "mse = 1491.2384840960158 r2_score = 0.033200629413957405\n"
     ]
    }
   ],
   "source": [
    "print(\"Лучший показатель\")\n",
    "print(\"n_estimators =\",n , \"max_depth =\", 3, \"lr =\", step)\n",
    "print(\"mse =\", mse_score, \"r2_score =\", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iul0dgvdqOIa"
   },
   "source": [
    "# Catboost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "e9ewnPfYqP5P"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.051237\n",
      "0:\tlearn: 32.2612607\ttotal: 145ms\tremaining: 2m 24s\n",
      "1:\tlearn: 32.1364948\ttotal: 148ms\tremaining: 1m 13s\n",
      "2:\tlearn: 32.0119759\ttotal: 150ms\tremaining: 49.7s\n",
      "3:\tlearn: 31.9409338\ttotal: 151ms\tremaining: 37.6s\n",
      "4:\tlearn: 31.8359045\ttotal: 153ms\tremaining: 30.5s\n",
      "5:\tlearn: 31.7189542\ttotal: 155ms\tremaining: 25.7s\n",
      "6:\tlearn: 31.6376013\ttotal: 157ms\tremaining: 22.3s\n",
      "7:\tlearn: 31.5323341\ttotal: 160ms\tremaining: 19.8s\n",
      "8:\tlearn: 31.4217420\ttotal: 162ms\tremaining: 17.8s\n",
      "9:\tlearn: 31.3281155\ttotal: 164ms\tremaining: 16.2s\n",
      "10:\tlearn: 31.2334613\ttotal: 166ms\tremaining: 14.9s\n",
      "11:\tlearn: 31.1608934\ttotal: 167ms\tremaining: 13.8s\n",
      "12:\tlearn: 31.0719458\ttotal: 169ms\tremaining: 12.9s\n",
      "13:\tlearn: 30.9996603\ttotal: 171ms\tremaining: 12s\n",
      "14:\tlearn: 30.9320914\ttotal: 173ms\tremaining: 11.3s\n",
      "15:\tlearn: 30.8657299\ttotal: 175ms\tremaining: 10.7s\n",
      "16:\tlearn: 30.7869002\ttotal: 177ms\tremaining: 10.2s\n",
      "17:\tlearn: 30.7238859\ttotal: 179ms\tremaining: 9.75s\n",
      "18:\tlearn: 30.6519419\ttotal: 181ms\tremaining: 9.33s\n",
      "19:\tlearn: 30.5869855\ttotal: 182ms\tremaining: 8.94s\n",
      "20:\tlearn: 30.5018557\ttotal: 184ms\tremaining: 8.59s\n",
      "21:\tlearn: 30.4553038\ttotal: 186ms\tremaining: 8.27s\n",
      "22:\tlearn: 30.3977087\ttotal: 188ms\tremaining: 7.98s\n",
      "23:\tlearn: 30.3296056\ttotal: 190ms\tremaining: 7.71s\n",
      "24:\tlearn: 30.2746979\ttotal: 192ms\tremaining: 7.48s\n",
      "25:\tlearn: 30.2192332\ttotal: 195ms\tremaining: 7.29s\n",
      "26:\tlearn: 30.1542945\ttotal: 198ms\tremaining: 7.12s\n",
      "27:\tlearn: 30.1036066\ttotal: 201ms\tremaining: 6.96s\n",
      "28:\tlearn: 30.0231315\ttotal: 204ms\tremaining: 6.83s\n",
      "29:\tlearn: 29.9743214\ttotal: 206ms\tremaining: 6.66s\n",
      "30:\tlearn: 29.9156280\ttotal: 208ms\tremaining: 6.5s\n",
      "31:\tlearn: 29.8694237\ttotal: 210ms\tremaining: 6.36s\n",
      "32:\tlearn: 29.8239034\ttotal: 212ms\tremaining: 6.21s\n",
      "33:\tlearn: 29.7798250\ttotal: 214ms\tremaining: 6.07s\n",
      "34:\tlearn: 29.7241108\ttotal: 216ms\tremaining: 5.94s\n",
      "35:\tlearn: 29.6822295\ttotal: 217ms\tremaining: 5.82s\n",
      "36:\tlearn: 29.6242252\ttotal: 219ms\tremaining: 5.71s\n",
      "37:\tlearn: 29.5681348\ttotal: 222ms\tremaining: 5.62s\n",
      "38:\tlearn: 29.5288989\ttotal: 224ms\tremaining: 5.52s\n",
      "39:\tlearn: 29.4726502\ttotal: 227ms\tremaining: 5.44s\n",
      "40:\tlearn: 29.4264245\ttotal: 229ms\tremaining: 5.36s\n",
      "41:\tlearn: 29.3976111\ttotal: 231ms\tremaining: 5.27s\n",
      "42:\tlearn: 29.3616461\ttotal: 233ms\tremaining: 5.18s\n",
      "43:\tlearn: 29.3240684\ttotal: 235ms\tremaining: 5.11s\n",
      "44:\tlearn: 29.2894341\ttotal: 237ms\tremaining: 5.04s\n",
      "45:\tlearn: 29.2504320\ttotal: 239ms\tremaining: 4.97s\n",
      "46:\tlearn: 29.2175870\ttotal: 242ms\tremaining: 4.91s\n",
      "47:\tlearn: 29.1729891\ttotal: 244ms\tremaining: 4.85s\n",
      "48:\tlearn: 29.1287469\ttotal: 247ms\tremaining: 4.79s\n",
      "49:\tlearn: 29.0988349\ttotal: 248ms\tremaining: 4.71s\n",
      "50:\tlearn: 29.0708985\ttotal: 250ms\tremaining: 4.65s\n",
      "51:\tlearn: 29.0412083\ttotal: 252ms\tremaining: 4.6s\n",
      "52:\tlearn: 29.0101203\ttotal: 255ms\tremaining: 4.55s\n",
      "53:\tlearn: 28.9821162\ttotal: 258ms\tremaining: 4.51s\n",
      "54:\tlearn: 28.9545864\ttotal: 260ms\tremaining: 4.46s\n",
      "55:\tlearn: 28.9282224\ttotal: 262ms\tremaining: 4.41s\n",
      "56:\tlearn: 28.9022188\ttotal: 264ms\tremaining: 4.36s\n",
      "57:\tlearn: 28.8509257\ttotal: 266ms\tremaining: 4.32s\n",
      "58:\tlearn: 28.8243521\ttotal: 268ms\tremaining: 4.27s\n",
      "59:\tlearn: 28.7944118\ttotal: 270ms\tremaining: 4.23s\n",
      "60:\tlearn: 28.7700186\ttotal: 272ms\tremaining: 4.19s\n",
      "61:\tlearn: 28.7318695\ttotal: 274ms\tremaining: 4.15s\n",
      "62:\tlearn: 28.6968049\ttotal: 276ms\tremaining: 4.11s\n",
      "63:\tlearn: 28.6729690\ttotal: 278ms\tremaining: 4.07s\n",
      "64:\tlearn: 28.6515803\ttotal: 280ms\tremaining: 4.03s\n",
      "65:\tlearn: 28.6032571\ttotal: 283ms\tremaining: 4s\n",
      "66:\tlearn: 28.5811083\ttotal: 285ms\tremaining: 3.96s\n",
      "67:\tlearn: 28.5579089\ttotal: 287ms\tremaining: 3.93s\n",
      "68:\tlearn: 28.5161138\ttotal: 289ms\tremaining: 3.9s\n",
      "69:\tlearn: 28.4932096\ttotal: 291ms\tremaining: 3.87s\n",
      "70:\tlearn: 28.4720545\ttotal: 293ms\tremaining: 3.83s\n",
      "71:\tlearn: 28.4320162\ttotal: 296ms\tremaining: 3.81s\n",
      "72:\tlearn: 28.4135810\ttotal: 298ms\tremaining: 3.78s\n",
      "73:\tlearn: 28.3804887\ttotal: 300ms\tremaining: 3.75s\n",
      "74:\tlearn: 28.3576045\ttotal: 302ms\tremaining: 3.72s\n",
      "75:\tlearn: 28.3397107\ttotal: 304ms\tremaining: 3.69s\n",
      "76:\tlearn: 28.3211662\ttotal: 306ms\tremaining: 3.66s\n",
      "77:\tlearn: 28.2946817\ttotal: 307ms\tremaining: 3.63s\n",
      "78:\tlearn: 28.2628399\ttotal: 310ms\tremaining: 3.61s\n",
      "79:\tlearn: 28.2385333\ttotal: 312ms\tremaining: 3.58s\n",
      "80:\tlearn: 28.2227731\ttotal: 314ms\tremaining: 3.56s\n",
      "81:\tlearn: 28.2075338\ttotal: 316ms\tremaining: 3.54s\n",
      "82:\tlearn: 28.1884267\ttotal: 318ms\tremaining: 3.52s\n",
      "83:\tlearn: 28.1739196\ttotal: 320ms\tremaining: 3.49s\n",
      "84:\tlearn: 28.1591046\ttotal: 322ms\tremaining: 3.47s\n",
      "85:\tlearn: 28.1439210\ttotal: 324ms\tremaining: 3.44s\n",
      "86:\tlearn: 28.1306611\ttotal: 327ms\tremaining: 3.43s\n",
      "87:\tlearn: 28.1166671\ttotal: 329ms\tremaining: 3.41s\n",
      "88:\tlearn: 28.1037907\ttotal: 332ms\tremaining: 3.4s\n",
      "89:\tlearn: 28.0909508\ttotal: 334ms\tremaining: 3.37s\n",
      "90:\tlearn: 28.0789086\ttotal: 336ms\tremaining: 3.35s\n",
      "91:\tlearn: 28.0669405\ttotal: 337ms\tremaining: 3.33s\n",
      "92:\tlearn: 28.0550266\ttotal: 339ms\tremaining: 3.31s\n",
      "93:\tlearn: 28.0335185\ttotal: 341ms\tremaining: 3.29s\n",
      "94:\tlearn: 28.0190124\ttotal: 343ms\tremaining: 3.27s\n",
      "95:\tlearn: 28.0046146\ttotal: 345ms\tremaining: 3.25s\n",
      "96:\tlearn: 27.9761060\ttotal: 347ms\tremaining: 3.23s\n",
      "97:\tlearn: 27.9552596\ttotal: 349ms\tremaining: 3.21s\n",
      "98:\tlearn: 27.9394478\ttotal: 351ms\tremaining: 3.19s\n",
      "99:\tlearn: 27.9262270\ttotal: 353ms\tremaining: 3.17s\n",
      "100:\tlearn: 27.9154102\ttotal: 354ms\tremaining: 3.15s\n",
      "101:\tlearn: 27.9003077\ttotal: 356ms\tremaining: 3.14s\n",
      "102:\tlearn: 27.8839828\ttotal: 358ms\tremaining: 3.12s\n",
      "103:\tlearn: 27.8728942\ttotal: 360ms\tremaining: 3.1s\n",
      "104:\tlearn: 27.8477373\ttotal: 362ms\tremaining: 3.09s\n",
      "105:\tlearn: 27.8229483\ttotal: 364ms\tremaining: 3.07s\n",
      "106:\tlearn: 27.8088276\ttotal: 366ms\tremaining: 3.06s\n",
      "107:\tlearn: 27.7931804\ttotal: 368ms\tremaining: 3.04s\n",
      "108:\tlearn: 27.7672137\ttotal: 371ms\tremaining: 3.03s\n",
      "109:\tlearn: 27.7508723\ttotal: 372ms\tremaining: 3.01s\n",
      "110:\tlearn: 27.7409541\ttotal: 374ms\tremaining: 3s\n",
      "111:\tlearn: 27.7322682\ttotal: 376ms\tremaining: 2.98s\n",
      "112:\tlearn: 27.7195314\ttotal: 378ms\tremaining: 2.97s\n",
      "113:\tlearn: 27.7071714\ttotal: 380ms\tremaining: 2.95s\n",
      "114:\tlearn: 27.6978138\ttotal: 382ms\tremaining: 2.94s\n",
      "115:\tlearn: 27.6859071\ttotal: 384ms\tremaining: 2.92s\n",
      "116:\tlearn: 27.6724762\ttotal: 387ms\tremaining: 2.92s\n",
      "117:\tlearn: 27.6635149\ttotal: 390ms\tremaining: 2.92s\n",
      "118:\tlearn: 27.6559462\ttotal: 392ms\tremaining: 2.9s\n",
      "119:\tlearn: 27.6410697\ttotal: 395ms\tremaining: 2.9s\n",
      "120:\tlearn: 27.6333341\ttotal: 398ms\tremaining: 2.89s\n",
      "121:\tlearn: 27.6082220\ttotal: 402ms\tremaining: 2.89s\n",
      "122:\tlearn: 27.5942944\ttotal: 404ms\tremaining: 2.88s\n",
      "123:\tlearn: 27.5715282\ttotal: 407ms\tremaining: 2.87s\n",
      "124:\tlearn: 27.5626412\ttotal: 410ms\tremaining: 2.87s\n",
      "125:\tlearn: 27.5516809\ttotal: 412ms\tremaining: 2.86s\n",
      "126:\tlearn: 27.5405827\ttotal: 414ms\tremaining: 2.85s\n",
      "127:\tlearn: 27.5133753\ttotal: 417ms\tremaining: 2.84s\n",
      "128:\tlearn: 27.5033237\ttotal: 419ms\tremaining: 2.83s\n",
      "129:\tlearn: 27.4891699\ttotal: 422ms\tremaining: 2.82s\n",
      "130:\tlearn: 27.4827251\ttotal: 424ms\tremaining: 2.81s\n",
      "131:\tlearn: 27.4648182\ttotal: 427ms\tremaining: 2.81s\n",
      "132:\tlearn: 27.4374820\ttotal: 430ms\tremaining: 2.8s\n",
      "133:\tlearn: 27.4222569\ttotal: 433ms\tremaining: 2.8s\n",
      "134:\tlearn: 27.4114721\ttotal: 435ms\tremaining: 2.79s\n",
      "135:\tlearn: 27.4029994\ttotal: 438ms\tremaining: 2.78s\n",
      "136:\tlearn: 27.3940406\ttotal: 440ms\tremaining: 2.77s\n",
      "137:\tlearn: 27.3818720\ttotal: 442ms\tremaining: 2.76s\n",
      "138:\tlearn: 27.3756501\ttotal: 445ms\tremaining: 2.75s\n",
      "139:\tlearn: 27.3613429\ttotal: 447ms\tremaining: 2.74s\n",
      "140:\tlearn: 27.3484640\ttotal: 449ms\tremaining: 2.73s\n",
      "141:\tlearn: 27.3405461\ttotal: 451ms\tremaining: 2.73s\n",
      "142:\tlearn: 27.3346777\ttotal: 453ms\tremaining: 2.71s\n",
      "143:\tlearn: 27.3253956\ttotal: 455ms\tremaining: 2.7s\n",
      "144:\tlearn: 27.3178454\ttotal: 457ms\tremaining: 2.69s\n",
      "145:\tlearn: 27.3032922\ttotal: 460ms\tremaining: 2.69s\n",
      "146:\tlearn: 27.2944750\ttotal: 463ms\tremaining: 2.69s\n",
      "147:\tlearn: 27.2830543\ttotal: 466ms\tremaining: 2.68s\n",
      "148:\tlearn: 27.2616644\ttotal: 469ms\tremaining: 2.68s\n",
      "149:\tlearn: 27.2506862\ttotal: 472ms\tremaining: 2.67s\n",
      "150:\tlearn: 27.2430765\ttotal: 475ms\tremaining: 2.67s\n",
      "151:\tlearn: 27.2376454\ttotal: 477ms\tremaining: 2.66s\n",
      "152:\tlearn: 27.2238950\ttotal: 480ms\tremaining: 2.66s\n",
      "153:\tlearn: 27.2161612\ttotal: 483ms\tremaining: 2.65s\n",
      "154:\tlearn: 27.2097375\ttotal: 486ms\tremaining: 2.65s\n",
      "155:\tlearn: 27.1972556\ttotal: 489ms\tremaining: 2.65s\n",
      "156:\tlearn: 27.1923595\ttotal: 492ms\tremaining: 2.64s\n",
      "157:\tlearn: 27.1815467\ttotal: 495ms\tremaining: 2.64s\n",
      "158:\tlearn: 27.1703524\ttotal: 497ms\tremaining: 2.63s\n",
      "159:\tlearn: 27.1573781\ttotal: 500ms\tremaining: 2.63s\n",
      "160:\tlearn: 27.1405825\ttotal: 503ms\tremaining: 2.62s\n",
      "161:\tlearn: 27.1341633\ttotal: 506ms\tremaining: 2.62s\n",
      "162:\tlearn: 27.1282163\ttotal: 508ms\tremaining: 2.61s\n",
      "163:\tlearn: 27.1141119\ttotal: 510ms\tremaining: 2.6s\n",
      "164:\tlearn: 27.1090544\ttotal: 512ms\tremaining: 2.59s\n",
      "165:\tlearn: 27.0868804\ttotal: 514ms\tremaining: 2.58s\n",
      "166:\tlearn: 27.0792078\ttotal: 517ms\tremaining: 2.58s\n",
      "167:\tlearn: 27.0582201\ttotal: 519ms\tremaining: 2.57s\n",
      "168:\tlearn: 27.0456009\ttotal: 522ms\tremaining: 2.56s\n",
      "169:\tlearn: 27.0414743\ttotal: 524ms\tremaining: 2.56s\n",
      "170:\tlearn: 27.0320557\ttotal: 526ms\tremaining: 2.55s\n",
      "171:\tlearn: 27.0228384\ttotal: 528ms\tremaining: 2.54s\n",
      "172:\tlearn: 27.0118537\ttotal: 531ms\tremaining: 2.54s\n",
      "173:\tlearn: 27.0033677\ttotal: 533ms\tremaining: 2.53s\n",
      "174:\tlearn: 26.9944020\ttotal: 535ms\tremaining: 2.52s\n",
      "175:\tlearn: 26.9756346\ttotal: 537ms\tremaining: 2.52s\n",
      "176:\tlearn: 26.9631042\ttotal: 539ms\tremaining: 2.51s\n",
      "177:\tlearn: 26.9531944\ttotal: 541ms\tremaining: 2.5s\n",
      "178:\tlearn: 26.9436950\ttotal: 543ms\tremaining: 2.49s\n",
      "179:\tlearn: 26.9279656\ttotal: 545ms\tremaining: 2.48s\n",
      "180:\tlearn: 26.9241543\ttotal: 547ms\tremaining: 2.47s\n",
      "181:\tlearn: 26.9137939\ttotal: 549ms\tremaining: 2.47s\n",
      "182:\tlearn: 26.9009738\ttotal: 552ms\tremaining: 2.46s\n",
      "183:\tlearn: 26.8962737\ttotal: 555ms\tremaining: 2.46s\n",
      "184:\tlearn: 26.8841840\ttotal: 557ms\tremaining: 2.45s\n",
      "185:\tlearn: 26.8633067\ttotal: 559ms\tremaining: 2.44s\n",
      "186:\tlearn: 26.8545930\ttotal: 561ms\tremaining: 2.44s\n",
      "187:\tlearn: 26.8380879\ttotal: 563ms\tremaining: 2.43s\n",
      "188:\tlearn: 26.8286576\ttotal: 565ms\tremaining: 2.42s\n",
      "189:\tlearn: 26.8207867\ttotal: 567ms\tremaining: 2.42s\n",
      "190:\tlearn: 26.8171897\ttotal: 569ms\tremaining: 2.41s\n",
      "191:\tlearn: 26.8076663\ttotal: 571ms\tremaining: 2.4s\n",
      "192:\tlearn: 26.7975872\ttotal: 573ms\tremaining: 2.4s\n",
      "193:\tlearn: 26.7862352\ttotal: 575ms\tremaining: 2.39s\n",
      "194:\tlearn: 26.7733865\ttotal: 577ms\tremaining: 2.38s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "195:\tlearn: 26.7570788\ttotal: 580ms\tremaining: 2.38s\n",
      "196:\tlearn: 26.7434583\ttotal: 583ms\tremaining: 2.38s\n",
      "197:\tlearn: 26.7338764\ttotal: 585ms\tremaining: 2.37s\n",
      "198:\tlearn: 26.7195018\ttotal: 588ms\tremaining: 2.37s\n",
      "199:\tlearn: 26.7139530\ttotal: 590ms\tremaining: 2.36s\n",
      "200:\tlearn: 26.7012548\ttotal: 592ms\tremaining: 2.35s\n",
      "201:\tlearn: 26.6927954\ttotal: 595ms\tremaining: 2.35s\n",
      "202:\tlearn: 26.6807386\ttotal: 598ms\tremaining: 2.35s\n",
      "203:\tlearn: 26.6704995\ttotal: 601ms\tremaining: 2.35s\n",
      "204:\tlearn: 26.6601626\ttotal: 605ms\tremaining: 2.34s\n",
      "205:\tlearn: 26.6478925\ttotal: 608ms\tremaining: 2.34s\n",
      "206:\tlearn: 26.6418292\ttotal: 610ms\tremaining: 2.34s\n",
      "207:\tlearn: 26.6337363\ttotal: 613ms\tremaining: 2.33s\n",
      "208:\tlearn: 26.6270570\ttotal: 615ms\tremaining: 2.33s\n",
      "209:\tlearn: 26.6209938\ttotal: 618ms\tremaining: 2.32s\n",
      "210:\tlearn: 26.6035001\ttotal: 620ms\tremaining: 2.32s\n",
      "211:\tlearn: 26.5940193\ttotal: 622ms\tremaining: 2.31s\n",
      "212:\tlearn: 26.5686596\ttotal: 624ms\tremaining: 2.31s\n",
      "213:\tlearn: 26.5539392\ttotal: 626ms\tremaining: 2.3s\n",
      "214:\tlearn: 26.5482886\ttotal: 629ms\tremaining: 2.29s\n",
      "215:\tlearn: 26.5408959\ttotal: 631ms\tremaining: 2.29s\n",
      "216:\tlearn: 26.5310432\ttotal: 633ms\tremaining: 2.28s\n",
      "217:\tlearn: 26.5077807\ttotal: 635ms\tremaining: 2.28s\n",
      "218:\tlearn: 26.5018650\ttotal: 637ms\tremaining: 2.27s\n",
      "219:\tlearn: 26.4900248\ttotal: 639ms\tremaining: 2.27s\n",
      "220:\tlearn: 26.4814603\ttotal: 641ms\tremaining: 2.26s\n",
      "221:\tlearn: 26.4745316\ttotal: 643ms\tremaining: 2.25s\n",
      "222:\tlearn: 26.4683622\ttotal: 645ms\tremaining: 2.25s\n",
      "223:\tlearn: 26.4622429\ttotal: 648ms\tremaining: 2.24s\n",
      "224:\tlearn: 26.4581983\ttotal: 650ms\tremaining: 2.24s\n",
      "225:\tlearn: 26.4475908\ttotal: 652ms\tremaining: 2.23s\n",
      "226:\tlearn: 26.4378753\ttotal: 654ms\tremaining: 2.23s\n",
      "227:\tlearn: 26.4256062\ttotal: 656ms\tremaining: 2.22s\n",
      "228:\tlearn: 26.4198496\ttotal: 658ms\tremaining: 2.22s\n",
      "229:\tlearn: 26.4144945\ttotal: 660ms\tremaining: 2.21s\n",
      "230:\tlearn: 26.4090951\ttotal: 662ms\tremaining: 2.21s\n",
      "231:\tlearn: 26.4037840\ttotal: 665ms\tremaining: 2.2s\n",
      "232:\tlearn: 26.3913779\ttotal: 667ms\tremaining: 2.19s\n",
      "233:\tlearn: 26.3854834\ttotal: 669ms\tremaining: 2.19s\n",
      "234:\tlearn: 26.3756251\ttotal: 671ms\tremaining: 2.18s\n",
      "235:\tlearn: 26.3717993\ttotal: 673ms\tremaining: 2.18s\n",
      "236:\tlearn: 26.3673424\ttotal: 675ms\tremaining: 2.17s\n",
      "237:\tlearn: 26.3615937\ttotal: 677ms\tremaining: 2.17s\n",
      "238:\tlearn: 26.3549047\ttotal: 679ms\tremaining: 2.16s\n",
      "239:\tlearn: 26.3429736\ttotal: 681ms\tremaining: 2.16s\n",
      "240:\tlearn: 26.3403652\ttotal: 683ms\tremaining: 2.15s\n",
      "241:\tlearn: 26.3302093\ttotal: 685ms\tremaining: 2.15s\n",
      "242:\tlearn: 26.3223098\ttotal: 687ms\tremaining: 2.14s\n",
      "243:\tlearn: 26.3179007\ttotal: 689ms\tremaining: 2.13s\n",
      "244:\tlearn: 26.3148639\ttotal: 691ms\tremaining: 2.13s\n",
      "245:\tlearn: 26.3059701\ttotal: 694ms\tremaining: 2.13s\n",
      "246:\tlearn: 26.2974446\ttotal: 696ms\tremaining: 2.12s\n",
      "247:\tlearn: 26.2904060\ttotal: 698ms\tremaining: 2.12s\n",
      "248:\tlearn: 26.2846077\ttotal: 701ms\tremaining: 2.11s\n",
      "249:\tlearn: 26.2813868\ttotal: 703ms\tremaining: 2.11s\n",
      "250:\tlearn: 26.2747445\ttotal: 704ms\tremaining: 2.1s\n",
      "251:\tlearn: 26.2679800\ttotal: 706ms\tremaining: 2.1s\n",
      "252:\tlearn: 26.2577012\ttotal: 708ms\tremaining: 2.09s\n",
      "253:\tlearn: 26.2479536\ttotal: 710ms\tremaining: 2.09s\n",
      "254:\tlearn: 26.2340838\ttotal: 712ms\tremaining: 2.08s\n",
      "255:\tlearn: 26.2287693\ttotal: 714ms\tremaining: 2.08s\n",
      "256:\tlearn: 26.2214875\ttotal: 716ms\tremaining: 2.07s\n",
      "257:\tlearn: 26.2109325\ttotal: 718ms\tremaining: 2.06s\n",
      "258:\tlearn: 26.2078906\ttotal: 720ms\tremaining: 2.06s\n",
      "259:\tlearn: 26.1964502\ttotal: 722ms\tremaining: 2.06s\n",
      "260:\tlearn: 26.1929583\ttotal: 724ms\tremaining: 2.05s\n",
      "261:\tlearn: 26.1859854\ttotal: 726ms\tremaining: 2.04s\n",
      "262:\tlearn: 26.1825310\ttotal: 728ms\tremaining: 2.04s\n",
      "263:\tlearn: 26.1715219\ttotal: 730ms\tremaining: 2.04s\n",
      "264:\tlearn: 26.1689772\ttotal: 732ms\tremaining: 2.03s\n",
      "265:\tlearn: 26.1629282\ttotal: 734ms\tremaining: 2.02s\n",
      "266:\tlearn: 26.1563336\ttotal: 736ms\tremaining: 2.02s\n",
      "267:\tlearn: 26.1514624\ttotal: 738ms\tremaining: 2.02s\n",
      "268:\tlearn: 26.1466452\ttotal: 741ms\tremaining: 2.01s\n",
      "269:\tlearn: 26.1423007\ttotal: 743ms\tremaining: 2.01s\n",
      "270:\tlearn: 26.1297700\ttotal: 746ms\tremaining: 2.01s\n",
      "271:\tlearn: 26.1230116\ttotal: 749ms\tremaining: 2s\n",
      "272:\tlearn: 26.1203606\ttotal: 752ms\tremaining: 2s\n",
      "273:\tlearn: 26.1179467\ttotal: 755ms\tremaining: 2s\n",
      "274:\tlearn: 26.1155828\ttotal: 758ms\tremaining: 2s\n",
      "275:\tlearn: 26.1099240\ttotal: 760ms\tremaining: 1.99s\n",
      "276:\tlearn: 26.1051990\ttotal: 763ms\tremaining: 1.99s\n",
      "277:\tlearn: 26.1025708\ttotal: 765ms\tremaining: 1.99s\n",
      "278:\tlearn: 26.0943360\ttotal: 768ms\tremaining: 1.98s\n",
      "279:\tlearn: 26.0889054\ttotal: 769ms\tremaining: 1.98s\n",
      "280:\tlearn: 26.0710969\ttotal: 772ms\tremaining: 1.97s\n",
      "281:\tlearn: 26.0663465\ttotal: 775ms\tremaining: 1.97s\n",
      "282:\tlearn: 26.0624890\ttotal: 779ms\tremaining: 1.97s\n",
      "283:\tlearn: 26.0581145\ttotal: 781ms\tremaining: 1.97s\n",
      "284:\tlearn: 26.0516878\ttotal: 785ms\tremaining: 1.97s\n",
      "285:\tlearn: 26.0438783\ttotal: 788ms\tremaining: 1.97s\n",
      "286:\tlearn: 26.0384225\ttotal: 791ms\tremaining: 1.97s\n",
      "287:\tlearn: 26.0315509\ttotal: 795ms\tremaining: 1.97s\n",
      "288:\tlearn: 26.0248284\ttotal: 798ms\tremaining: 1.96s\n",
      "289:\tlearn: 26.0215090\ttotal: 800ms\tremaining: 1.96s\n",
      "290:\tlearn: 26.0112332\ttotal: 802ms\tremaining: 1.95s\n",
      "291:\tlearn: 26.0065752\ttotal: 805ms\tremaining: 1.95s\n",
      "292:\tlearn: 25.9992610\ttotal: 807ms\tremaining: 1.95s\n",
      "293:\tlearn: 25.9929719\ttotal: 809ms\tremaining: 1.94s\n",
      "294:\tlearn: 25.9900995\ttotal: 812ms\tremaining: 1.94s\n",
      "295:\tlearn: 25.9871196\ttotal: 815ms\tremaining: 1.94s\n",
      "296:\tlearn: 25.9828821\ttotal: 817ms\tremaining: 1.93s\n",
      "297:\tlearn: 25.9771798\ttotal: 820ms\tremaining: 1.93s\n",
      "298:\tlearn: 25.9722722\ttotal: 823ms\tremaining: 1.93s\n",
      "299:\tlearn: 25.9649310\ttotal: 825ms\tremaining: 1.93s\n",
      "300:\tlearn: 25.9625523\ttotal: 828ms\tremaining: 1.92s\n",
      "301:\tlearn: 25.9563808\ttotal: 831ms\tremaining: 1.92s\n",
      "302:\tlearn: 25.9501030\ttotal: 833ms\tremaining: 1.92s\n",
      "303:\tlearn: 25.9459573\ttotal: 836ms\tremaining: 1.91s\n",
      "304:\tlearn: 25.9422380\ttotal: 838ms\tremaining: 1.91s\n",
      "305:\tlearn: 25.9381786\ttotal: 841ms\tremaining: 1.91s\n",
      "306:\tlearn: 25.9320231\ttotal: 843ms\tremaining: 1.9s\n",
      "307:\tlearn: 25.9248161\ttotal: 847ms\tremaining: 1.9s\n",
      "308:\tlearn: 25.9210018\ttotal: 849ms\tremaining: 1.9s\n",
      "309:\tlearn: 25.9192480\ttotal: 852ms\tremaining: 1.9s\n",
      "310:\tlearn: 25.9151480\ttotal: 854ms\tremaining: 1.89s\n",
      "311:\tlearn: 25.9062371\ttotal: 856ms\tremaining: 1.89s\n",
      "312:\tlearn: 25.9025997\ttotal: 858ms\tremaining: 1.88s\n",
      "313:\tlearn: 25.9008800\ttotal: 860ms\tremaining: 1.88s\n",
      "314:\tlearn: 25.8992192\ttotal: 863ms\tremaining: 1.88s\n",
      "315:\tlearn: 25.8914136\ttotal: 865ms\tremaining: 1.87s\n",
      "316:\tlearn: 25.8855534\ttotal: 867ms\tremaining: 1.87s\n",
      "317:\tlearn: 25.8818486\ttotal: 870ms\tremaining: 1.86s\n",
      "318:\tlearn: 25.8749885\ttotal: 872ms\tremaining: 1.86s\n",
      "319:\tlearn: 25.8678888\ttotal: 874ms\tremaining: 1.86s\n",
      "320:\tlearn: 25.8642114\ttotal: 876ms\tremaining: 1.85s\n",
      "321:\tlearn: 25.8601130\ttotal: 878ms\tremaining: 1.85s\n",
      "322:\tlearn: 25.8528981\ttotal: 881ms\tremaining: 1.84s\n",
      "323:\tlearn: 25.8492398\ttotal: 884ms\tremaining: 1.84s\n",
      "324:\tlearn: 25.8435822\ttotal: 887ms\tremaining: 1.84s\n",
      "325:\tlearn: 25.8399803\ttotal: 890ms\tremaining: 1.84s\n",
      "326:\tlearn: 25.8356215\ttotal: 892ms\tremaining: 1.83s\n",
      "327:\tlearn: 25.8298188\ttotal: 894ms\tremaining: 1.83s\n",
      "328:\tlearn: 25.8173773\ttotal: 896ms\tremaining: 1.83s\n",
      "329:\tlearn: 25.8124691\ttotal: 899ms\tremaining: 1.82s\n",
      "330:\tlearn: 25.8103043\ttotal: 902ms\tremaining: 1.82s\n",
      "331:\tlearn: 25.8039366\ttotal: 905ms\tremaining: 1.82s\n",
      "332:\tlearn: 25.7988483\ttotal: 907ms\tremaining: 1.82s\n",
      "333:\tlearn: 25.7924957\ttotal: 909ms\tremaining: 1.81s\n",
      "334:\tlearn: 25.7860335\ttotal: 911ms\tremaining: 1.81s\n",
      "335:\tlearn: 25.7760048\ttotal: 915ms\tremaining: 1.81s\n",
      "336:\tlearn: 25.7724439\ttotal: 918ms\tremaining: 1.8s\n",
      "337:\tlearn: 25.7661174\ttotal: 921ms\tremaining: 1.8s\n",
      "338:\tlearn: 25.7609006\ttotal: 923ms\tremaining: 1.8s\n",
      "339:\tlearn: 25.7532730\ttotal: 925ms\tremaining: 1.8s\n",
      "340:\tlearn: 25.7504290\ttotal: 928ms\tremaining: 1.79s\n",
      "341:\tlearn: 25.7442161\ttotal: 931ms\tremaining: 1.79s\n",
      "342:\tlearn: 25.7377268\ttotal: 934ms\tremaining: 1.79s\n",
      "343:\tlearn: 25.7350587\ttotal: 937ms\tremaining: 1.79s\n",
      "344:\tlearn: 25.7287520\ttotal: 939ms\tremaining: 1.78s\n",
      "345:\tlearn: 25.7268860\ttotal: 941ms\tremaining: 1.78s\n",
      "346:\tlearn: 25.7211577\ttotal: 944ms\tremaining: 1.78s\n",
      "347:\tlearn: 25.7154388\ttotal: 947ms\tremaining: 1.77s\n",
      "348:\tlearn: 25.7111838\ttotal: 950ms\tremaining: 1.77s\n",
      "349:\tlearn: 25.7049751\ttotal: 952ms\tremaining: 1.77s\n",
      "350:\tlearn: 25.7015010\ttotal: 954ms\tremaining: 1.76s\n",
      "351:\tlearn: 25.6969866\ttotal: 956ms\tremaining: 1.76s\n",
      "352:\tlearn: 25.6919576\ttotal: 958ms\tremaining: 1.76s\n",
      "353:\tlearn: 25.6866586\ttotal: 962ms\tremaining: 1.75s\n",
      "354:\tlearn: 25.6849619\ttotal: 965ms\tremaining: 1.75s\n",
      "355:\tlearn: 25.6707588\ttotal: 968ms\tremaining: 1.75s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "356:\tlearn: 25.6623634\ttotal: 972ms\tremaining: 1.75s\n",
      "357:\tlearn: 25.6551384\ttotal: 975ms\tremaining: 1.75s\n",
      "358:\tlearn: 25.6503786\ttotal: 978ms\tremaining: 1.75s\n",
      "359:\tlearn: 25.6465523\ttotal: 981ms\tremaining: 1.74s\n",
      "360:\tlearn: 25.6414502\ttotal: 984ms\tremaining: 1.74s\n",
      "361:\tlearn: 25.6386525\ttotal: 987ms\tremaining: 1.74s\n",
      "362:\tlearn: 25.6345925\ttotal: 990ms\tremaining: 1.74s\n",
      "363:\tlearn: 25.6312454\ttotal: 993ms\tremaining: 1.73s\n",
      "364:\tlearn: 25.6283322\ttotal: 996ms\tremaining: 1.73s\n",
      "365:\tlearn: 25.6205740\ttotal: 999ms\tremaining: 1.73s\n",
      "366:\tlearn: 25.6155205\ttotal: 1s\tremaining: 1.73s\n",
      "367:\tlearn: 25.6071124\ttotal: 1s\tremaining: 1.72s\n",
      "368:\tlearn: 25.6010647\ttotal: 1s\tremaining: 1.72s\n",
      "369:\tlearn: 25.5991779\ttotal: 1.01s\tremaining: 1.72s\n",
      "370:\tlearn: 25.5956959\ttotal: 1.01s\tremaining: 1.72s\n",
      "371:\tlearn: 25.5890891\ttotal: 1.01s\tremaining: 1.71s\n",
      "372:\tlearn: 25.5828696\ttotal: 1.01s\tremaining: 1.71s\n",
      "373:\tlearn: 25.5773768\ttotal: 1.02s\tremaining: 1.7s\n",
      "374:\tlearn: 25.5704954\ttotal: 1.02s\tremaining: 1.7s\n",
      "375:\tlearn: 25.5657258\ttotal: 1.02s\tremaining: 1.7s\n",
      "376:\tlearn: 25.5583822\ttotal: 1.02s\tremaining: 1.69s\n",
      "377:\tlearn: 25.5500887\ttotal: 1.03s\tremaining: 1.69s\n",
      "378:\tlearn: 25.5425850\ttotal: 1.03s\tremaining: 1.69s\n",
      "379:\tlearn: 25.5410647\ttotal: 1.03s\tremaining: 1.68s\n",
      "380:\tlearn: 25.5362660\ttotal: 1.03s\tremaining: 1.68s\n",
      "381:\tlearn: 25.5345466\ttotal: 1.03s\tremaining: 1.68s\n",
      "382:\tlearn: 25.5300040\ttotal: 1.04s\tremaining: 1.67s\n",
      "383:\tlearn: 25.5250559\ttotal: 1.04s\tremaining: 1.67s\n",
      "384:\tlearn: 25.5221883\ttotal: 1.04s\tremaining: 1.67s\n",
      "385:\tlearn: 25.5207141\ttotal: 1.05s\tremaining: 1.67s\n",
      "386:\tlearn: 25.5182929\ttotal: 1.05s\tremaining: 1.66s\n",
      "387:\tlearn: 25.5144228\ttotal: 1.05s\tremaining: 1.66s\n",
      "388:\tlearn: 25.5125599\ttotal: 1.05s\tremaining: 1.66s\n",
      "389:\tlearn: 25.5104190\ttotal: 1.06s\tremaining: 1.65s\n",
      "390:\tlearn: 25.5065570\ttotal: 1.06s\tremaining: 1.65s\n",
      "391:\tlearn: 25.5053068\ttotal: 1.06s\tremaining: 1.65s\n",
      "392:\tlearn: 25.5036451\ttotal: 1.06s\tremaining: 1.64s\n",
      "393:\tlearn: 25.4975630\ttotal: 1.07s\tremaining: 1.64s\n",
      "394:\tlearn: 25.4942358\ttotal: 1.07s\tremaining: 1.64s\n",
      "395:\tlearn: 25.4905537\ttotal: 1.07s\tremaining: 1.63s\n",
      "396:\tlearn: 25.4889553\ttotal: 1.07s\tremaining: 1.63s\n",
      "397:\tlearn: 25.4855040\ttotal: 1.08s\tremaining: 1.63s\n",
      "398:\tlearn: 25.4770297\ttotal: 1.08s\tremaining: 1.63s\n",
      "399:\tlearn: 25.4718641\ttotal: 1.08s\tremaining: 1.62s\n",
      "400:\tlearn: 25.4661227\ttotal: 1.08s\tremaining: 1.62s\n",
      "401:\tlearn: 25.4617387\ttotal: 1.09s\tremaining: 1.62s\n",
      "402:\tlearn: 25.4549740\ttotal: 1.09s\tremaining: 1.61s\n",
      "403:\tlearn: 25.4460506\ttotal: 1.09s\tremaining: 1.61s\n",
      "404:\tlearn: 25.4444506\ttotal: 1.09s\tremaining: 1.61s\n",
      "405:\tlearn: 25.4388378\ttotal: 1.1s\tremaining: 1.6s\n",
      "406:\tlearn: 25.4329549\ttotal: 1.1s\tremaining: 1.6s\n",
      "407:\tlearn: 25.4284299\ttotal: 1.1s\tremaining: 1.6s\n",
      "408:\tlearn: 25.4269113\ttotal: 1.11s\tremaining: 1.6s\n",
      "409:\tlearn: 25.4214983\ttotal: 1.11s\tremaining: 1.59s\n",
      "410:\tlearn: 25.4199015\ttotal: 1.11s\tremaining: 1.59s\n",
      "411:\tlearn: 25.4158007\ttotal: 1.11s\tremaining: 1.59s\n",
      "412:\tlearn: 25.4132371\ttotal: 1.11s\tremaining: 1.58s\n",
      "413:\tlearn: 25.4107613\ttotal: 1.12s\tremaining: 1.58s\n",
      "414:\tlearn: 25.4044326\ttotal: 1.12s\tremaining: 1.58s\n",
      "415:\tlearn: 25.3980003\ttotal: 1.13s\tremaining: 1.58s\n",
      "416:\tlearn: 25.3923629\ttotal: 1.13s\tremaining: 1.57s\n",
      "417:\tlearn: 25.3886867\ttotal: 1.13s\tremaining: 1.57s\n",
      "418:\tlearn: 25.3867996\ttotal: 1.13s\tremaining: 1.57s\n",
      "419:\tlearn: 25.3796252\ttotal: 1.14s\tremaining: 1.57s\n",
      "420:\tlearn: 25.3749751\ttotal: 1.14s\tremaining: 1.56s\n",
      "421:\tlearn: 25.3724047\ttotal: 1.14s\tremaining: 1.56s\n",
      "422:\tlearn: 25.3700381\ttotal: 1.14s\tremaining: 1.56s\n",
      "423:\tlearn: 25.3684573\ttotal: 1.14s\tremaining: 1.55s\n",
      "424:\tlearn: 25.3629004\ttotal: 1.15s\tremaining: 1.55s\n",
      "425:\tlearn: 25.3612278\ttotal: 1.15s\tremaining: 1.55s\n",
      "426:\tlearn: 25.3598708\ttotal: 1.15s\tremaining: 1.54s\n",
      "427:\tlearn: 25.3566820\ttotal: 1.15s\tremaining: 1.54s\n",
      "428:\tlearn: 25.3543262\ttotal: 1.16s\tremaining: 1.54s\n",
      "429:\tlearn: 25.3529201\ttotal: 1.16s\tremaining: 1.53s\n",
      "430:\tlearn: 25.3487934\ttotal: 1.16s\tremaining: 1.53s\n",
      "431:\tlearn: 25.3393996\ttotal: 1.16s\tremaining: 1.53s\n",
      "432:\tlearn: 25.3363881\ttotal: 1.17s\tremaining: 1.53s\n",
      "433:\tlearn: 25.3337317\ttotal: 1.17s\tremaining: 1.52s\n",
      "434:\tlearn: 25.3301335\ttotal: 1.17s\tremaining: 1.52s\n",
      "435:\tlearn: 25.3218987\ttotal: 1.17s\tremaining: 1.52s\n",
      "436:\tlearn: 25.3194332\ttotal: 1.18s\tremaining: 1.51s\n",
      "437:\tlearn: 25.3128511\ttotal: 1.18s\tremaining: 1.51s\n",
      "438:\tlearn: 25.3089369\ttotal: 1.18s\tremaining: 1.51s\n",
      "439:\tlearn: 25.3015226\ttotal: 1.18s\tremaining: 1.51s\n",
      "440:\tlearn: 25.2956250\ttotal: 1.19s\tremaining: 1.5s\n",
      "441:\tlearn: 25.2913754\ttotal: 1.19s\tremaining: 1.5s\n",
      "442:\tlearn: 25.2891818\ttotal: 1.19s\tremaining: 1.5s\n",
      "443:\tlearn: 25.2878042\ttotal: 1.19s\tremaining: 1.49s\n",
      "444:\tlearn: 25.2837602\ttotal: 1.19s\tremaining: 1.49s\n",
      "445:\tlearn: 25.2806424\ttotal: 1.2s\tremaining: 1.49s\n",
      "446:\tlearn: 25.2780360\ttotal: 1.2s\tremaining: 1.48s\n",
      "447:\tlearn: 25.2736480\ttotal: 1.2s\tremaining: 1.48s\n",
      "448:\tlearn: 25.2704461\ttotal: 1.2s\tremaining: 1.48s\n",
      "449:\tlearn: 25.2683847\ttotal: 1.21s\tremaining: 1.47s\n",
      "450:\tlearn: 25.2655752\ttotal: 1.21s\tremaining: 1.47s\n",
      "451:\tlearn: 25.2633439\ttotal: 1.21s\tremaining: 1.47s\n",
      "452:\tlearn: 25.2604625\ttotal: 1.21s\tremaining: 1.47s\n",
      "453:\tlearn: 25.2574803\ttotal: 1.22s\tremaining: 1.46s\n",
      "454:\tlearn: 25.2535186\ttotal: 1.22s\tremaining: 1.46s\n",
      "455:\tlearn: 25.2516610\ttotal: 1.22s\tremaining: 1.45s\n",
      "456:\tlearn: 25.2496607\ttotal: 1.22s\tremaining: 1.45s\n",
      "457:\tlearn: 25.2467811\ttotal: 1.22s\tremaining: 1.45s\n",
      "458:\tlearn: 25.2430193\ttotal: 1.23s\tremaining: 1.45s\n",
      "459:\tlearn: 25.2362604\ttotal: 1.23s\tremaining: 1.44s\n",
      "460:\tlearn: 25.2350515\ttotal: 1.23s\tremaining: 1.44s\n",
      "461:\tlearn: 25.2297490\ttotal: 1.23s\tremaining: 1.43s\n",
      "462:\tlearn: 25.2286159\ttotal: 1.23s\tremaining: 1.43s\n",
      "463:\tlearn: 25.2245442\ttotal: 1.24s\tremaining: 1.43s\n",
      "464:\tlearn: 25.2200304\ttotal: 1.24s\tremaining: 1.43s\n",
      "465:\tlearn: 25.2172219\ttotal: 1.24s\tremaining: 1.42s\n",
      "466:\tlearn: 25.2143210\ttotal: 1.24s\tremaining: 1.42s\n",
      "467:\tlearn: 25.2076472\ttotal: 1.25s\tremaining: 1.42s\n",
      "468:\tlearn: 25.2025516\ttotal: 1.25s\tremaining: 1.41s\n",
      "469:\tlearn: 25.1983190\ttotal: 1.25s\tremaining: 1.41s\n",
      "470:\tlearn: 25.1931931\ttotal: 1.25s\tremaining: 1.41s\n",
      "471:\tlearn: 25.1883599\ttotal: 1.25s\tremaining: 1.4s\n",
      "472:\tlearn: 25.1867582\ttotal: 1.26s\tremaining: 1.4s\n",
      "473:\tlearn: 25.1849862\ttotal: 1.26s\tremaining: 1.4s\n",
      "474:\tlearn: 25.1786384\ttotal: 1.26s\tremaining: 1.39s\n",
      "475:\tlearn: 25.1731511\ttotal: 1.26s\tremaining: 1.39s\n",
      "476:\tlearn: 25.1701711\ttotal: 1.26s\tremaining: 1.39s\n",
      "477:\tlearn: 25.1664722\ttotal: 1.27s\tremaining: 1.38s\n",
      "478:\tlearn: 25.1581101\ttotal: 1.27s\tremaining: 1.38s\n",
      "479:\tlearn: 25.1566304\ttotal: 1.27s\tremaining: 1.38s\n",
      "480:\tlearn: 25.1515645\ttotal: 1.27s\tremaining: 1.37s\n",
      "481:\tlearn: 25.1474416\ttotal: 1.28s\tremaining: 1.37s\n",
      "482:\tlearn: 25.1445946\ttotal: 1.28s\tremaining: 1.37s\n",
      "483:\tlearn: 25.1392877\ttotal: 1.28s\tremaining: 1.36s\n",
      "484:\tlearn: 25.1348882\ttotal: 1.28s\tremaining: 1.36s\n",
      "485:\tlearn: 25.1295279\ttotal: 1.28s\tremaining: 1.36s\n",
      "486:\tlearn: 25.1248407\ttotal: 1.29s\tremaining: 1.35s\n",
      "487:\tlearn: 25.1224350\ttotal: 1.29s\tremaining: 1.35s\n",
      "488:\tlearn: 25.1204161\ttotal: 1.29s\tremaining: 1.35s\n",
      "489:\tlearn: 25.1188969\ttotal: 1.29s\tremaining: 1.34s\n",
      "490:\tlearn: 25.1175142\ttotal: 1.29s\tremaining: 1.34s\n",
      "491:\tlearn: 25.1155290\ttotal: 1.3s\tremaining: 1.34s\n",
      "492:\tlearn: 25.1131549\ttotal: 1.3s\tremaining: 1.33s\n",
      "493:\tlearn: 25.1102113\ttotal: 1.3s\tremaining: 1.33s\n",
      "494:\tlearn: 25.1078743\ttotal: 1.3s\tremaining: 1.33s\n",
      "495:\tlearn: 25.1028818\ttotal: 1.3s\tremaining: 1.33s\n",
      "496:\tlearn: 25.0990803\ttotal: 1.31s\tremaining: 1.32s\n",
      "497:\tlearn: 25.0966089\ttotal: 1.31s\tremaining: 1.32s\n",
      "498:\tlearn: 25.0944459\ttotal: 1.31s\tremaining: 1.32s\n",
      "499:\tlearn: 25.0893914\ttotal: 1.31s\tremaining: 1.31s\n",
      "500:\tlearn: 25.0818765\ttotal: 1.32s\tremaining: 1.31s\n",
      "501:\tlearn: 25.0801779\ttotal: 1.32s\tremaining: 1.31s\n",
      "502:\tlearn: 25.0770503\ttotal: 1.32s\tremaining: 1.3s\n",
      "503:\tlearn: 25.0724616\ttotal: 1.32s\tremaining: 1.3s\n",
      "504:\tlearn: 25.0710227\ttotal: 1.32s\tremaining: 1.3s\n",
      "505:\tlearn: 25.0652588\ttotal: 1.33s\tremaining: 1.29s\n",
      "506:\tlearn: 25.0596538\ttotal: 1.33s\tremaining: 1.29s\n",
      "507:\tlearn: 25.0552331\ttotal: 1.33s\tremaining: 1.29s\n",
      "508:\tlearn: 25.0501253\ttotal: 1.33s\tremaining: 1.29s\n",
      "509:\tlearn: 25.0478220\ttotal: 1.33s\tremaining: 1.28s\n",
      "510:\tlearn: 25.0471143\ttotal: 1.34s\tremaining: 1.28s\n",
      "511:\tlearn: 25.0420401\ttotal: 1.34s\tremaining: 1.28s\n",
      "512:\tlearn: 25.0393782\ttotal: 1.34s\tremaining: 1.27s\n",
      "513:\tlearn: 25.0366278\ttotal: 1.34s\tremaining: 1.27s\n",
      "514:\tlearn: 25.0321949\ttotal: 1.35s\tremaining: 1.27s\n",
      "515:\tlearn: 25.0281682\ttotal: 1.35s\tremaining: 1.26s\n",
      "516:\tlearn: 25.0258313\ttotal: 1.35s\tremaining: 1.26s\n",
      "517:\tlearn: 25.0237987\ttotal: 1.35s\tremaining: 1.26s\n",
      "518:\tlearn: 25.0227569\ttotal: 1.35s\tremaining: 1.25s\n",
      "519:\tlearn: 25.0119943\ttotal: 1.36s\tremaining: 1.25s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "520:\tlearn: 25.0074087\ttotal: 1.36s\tremaining: 1.25s\n",
      "521:\tlearn: 25.0009359\ttotal: 1.36s\tremaining: 1.25s\n",
      "522:\tlearn: 24.9963089\ttotal: 1.36s\tremaining: 1.25s\n",
      "523:\tlearn: 24.9924111\ttotal: 1.37s\tremaining: 1.24s\n",
      "524:\tlearn: 24.9904825\ttotal: 1.37s\tremaining: 1.24s\n",
      "525:\tlearn: 24.9889583\ttotal: 1.37s\tremaining: 1.24s\n",
      "526:\tlearn: 24.9834312\ttotal: 1.38s\tremaining: 1.24s\n",
      "527:\tlearn: 24.9813955\ttotal: 1.38s\tremaining: 1.23s\n",
      "528:\tlearn: 24.9802545\ttotal: 1.38s\tremaining: 1.23s\n",
      "529:\tlearn: 24.9727344\ttotal: 1.38s\tremaining: 1.23s\n",
      "530:\tlearn: 24.9690976\ttotal: 1.39s\tremaining: 1.22s\n",
      "531:\tlearn: 24.9672495\ttotal: 1.39s\tremaining: 1.22s\n",
      "532:\tlearn: 24.9649766\ttotal: 1.39s\tremaining: 1.22s\n",
      "533:\tlearn: 24.9615215\ttotal: 1.39s\tremaining: 1.22s\n",
      "534:\tlearn: 24.9595697\ttotal: 1.4s\tremaining: 1.21s\n",
      "535:\tlearn: 24.9538427\ttotal: 1.4s\tremaining: 1.21s\n",
      "536:\tlearn: 24.9488151\ttotal: 1.4s\tremaining: 1.21s\n",
      "537:\tlearn: 24.9479332\ttotal: 1.4s\tremaining: 1.21s\n",
      "538:\tlearn: 24.9453821\ttotal: 1.41s\tremaining: 1.2s\n",
      "539:\tlearn: 24.9433968\ttotal: 1.41s\tremaining: 1.2s\n",
      "540:\tlearn: 24.9393536\ttotal: 1.41s\tremaining: 1.2s\n",
      "541:\tlearn: 24.9376180\ttotal: 1.41s\tremaining: 1.19s\n",
      "542:\tlearn: 24.9356502\ttotal: 1.42s\tremaining: 1.19s\n",
      "543:\tlearn: 24.9311628\ttotal: 1.42s\tremaining: 1.19s\n",
      "544:\tlearn: 24.9265238\ttotal: 1.42s\tremaining: 1.19s\n",
      "545:\tlearn: 24.9253978\ttotal: 1.42s\tremaining: 1.18s\n",
      "546:\tlearn: 24.9243003\ttotal: 1.43s\tremaining: 1.18s\n",
      "547:\tlearn: 24.9189175\ttotal: 1.43s\tremaining: 1.18s\n",
      "548:\tlearn: 24.9146309\ttotal: 1.43s\tremaining: 1.18s\n",
      "549:\tlearn: 24.9115667\ttotal: 1.44s\tremaining: 1.18s\n",
      "550:\tlearn: 24.9082072\ttotal: 1.44s\tremaining: 1.17s\n",
      "551:\tlearn: 24.9035502\ttotal: 1.44s\tremaining: 1.17s\n",
      "552:\tlearn: 24.8998410\ttotal: 1.45s\tremaining: 1.17s\n",
      "553:\tlearn: 24.8981612\ttotal: 1.45s\tremaining: 1.17s\n",
      "554:\tlearn: 24.8951867\ttotal: 1.45s\tremaining: 1.16s\n",
      "555:\tlearn: 24.8905231\ttotal: 1.45s\tremaining: 1.16s\n",
      "556:\tlearn: 24.8881613\ttotal: 1.46s\tremaining: 1.16s\n",
      "557:\tlearn: 24.8863001\ttotal: 1.46s\tremaining: 1.16s\n",
      "558:\tlearn: 24.8836801\ttotal: 1.46s\tremaining: 1.16s\n",
      "559:\tlearn: 24.8808807\ttotal: 1.47s\tremaining: 1.15s\n",
      "560:\tlearn: 24.8797877\ttotal: 1.47s\tremaining: 1.15s\n",
      "561:\tlearn: 24.8781320\ttotal: 1.47s\tremaining: 1.15s\n",
      "562:\tlearn: 24.8731524\ttotal: 1.48s\tremaining: 1.15s\n",
      "563:\tlearn: 24.8702477\ttotal: 1.48s\tremaining: 1.14s\n",
      "564:\tlearn: 24.8645063\ttotal: 1.48s\tremaining: 1.14s\n",
      "565:\tlearn: 24.8627265\ttotal: 1.49s\tremaining: 1.14s\n",
      "566:\tlearn: 24.8594050\ttotal: 1.49s\tremaining: 1.14s\n",
      "567:\tlearn: 24.8553670\ttotal: 1.49s\tremaining: 1.14s\n",
      "568:\tlearn: 24.8529579\ttotal: 1.5s\tremaining: 1.13s\n",
      "569:\tlearn: 24.8501213\ttotal: 1.5s\tremaining: 1.13s\n",
      "570:\tlearn: 24.8469143\ttotal: 1.5s\tremaining: 1.13s\n",
      "571:\tlearn: 24.8455947\ttotal: 1.5s\tremaining: 1.13s\n",
      "572:\tlearn: 24.8436040\ttotal: 1.51s\tremaining: 1.12s\n",
      "573:\tlearn: 24.8396968\ttotal: 1.51s\tremaining: 1.12s\n",
      "574:\tlearn: 24.8363200\ttotal: 1.51s\tremaining: 1.12s\n",
      "575:\tlearn: 24.8343626\ttotal: 1.52s\tremaining: 1.12s\n",
      "576:\tlearn: 24.8318367\ttotal: 1.52s\tremaining: 1.11s\n",
      "577:\tlearn: 24.8233238\ttotal: 1.52s\tremaining: 1.11s\n",
      "578:\tlearn: 24.8216729\ttotal: 1.52s\tremaining: 1.11s\n",
      "579:\tlearn: 24.8157147\ttotal: 1.53s\tremaining: 1.11s\n",
      "580:\tlearn: 24.8125688\ttotal: 1.53s\tremaining: 1.1s\n",
      "581:\tlearn: 24.8065195\ttotal: 1.53s\tremaining: 1.1s\n",
      "582:\tlearn: 24.8034441\ttotal: 1.53s\tremaining: 1.1s\n",
      "583:\tlearn: 24.7982707\ttotal: 1.54s\tremaining: 1.09s\n",
      "584:\tlearn: 24.7966203\ttotal: 1.54s\tremaining: 1.09s\n",
      "585:\tlearn: 24.7931391\ttotal: 1.54s\tremaining: 1.09s\n",
      "586:\tlearn: 24.7909914\ttotal: 1.54s\tremaining: 1.09s\n",
      "587:\tlearn: 24.7833182\ttotal: 1.55s\tremaining: 1.08s\n",
      "588:\tlearn: 24.7816295\ttotal: 1.55s\tremaining: 1.08s\n",
      "589:\tlearn: 24.7793978\ttotal: 1.55s\tremaining: 1.08s\n",
      "590:\tlearn: 24.7766671\ttotal: 1.56s\tremaining: 1.08s\n",
      "591:\tlearn: 24.7751932\ttotal: 1.56s\tremaining: 1.07s\n",
      "592:\tlearn: 24.7729973\ttotal: 1.56s\tremaining: 1.07s\n",
      "593:\tlearn: 24.7696468\ttotal: 1.56s\tremaining: 1.07s\n",
      "594:\tlearn: 24.7684374\ttotal: 1.57s\tremaining: 1.07s\n",
      "595:\tlearn: 24.7672813\ttotal: 1.57s\tremaining: 1.06s\n",
      "596:\tlearn: 24.7625851\ttotal: 1.57s\tremaining: 1.06s\n",
      "597:\tlearn: 24.7567786\ttotal: 1.58s\tremaining: 1.06s\n",
      "598:\tlearn: 24.7509079\ttotal: 1.58s\tremaining: 1.06s\n",
      "599:\tlearn: 24.7503719\ttotal: 1.58s\tremaining: 1.05s\n",
      "600:\tlearn: 24.7452401\ttotal: 1.58s\tremaining: 1.05s\n",
      "601:\tlearn: 24.7421133\ttotal: 1.59s\tremaining: 1.05s\n",
      "602:\tlearn: 24.7378992\ttotal: 1.59s\tremaining: 1.05s\n",
      "603:\tlearn: 24.7320173\ttotal: 1.59s\tremaining: 1.04s\n",
      "604:\tlearn: 24.7304124\ttotal: 1.59s\tremaining: 1.04s\n",
      "605:\tlearn: 24.7279465\ttotal: 1.6s\tremaining: 1.04s\n",
      "606:\tlearn: 24.7257569\ttotal: 1.6s\tremaining: 1.04s\n",
      "607:\tlearn: 24.7207796\ttotal: 1.6s\tremaining: 1.03s\n",
      "608:\tlearn: 24.7184829\ttotal: 1.6s\tremaining: 1.03s\n",
      "609:\tlearn: 24.7077915\ttotal: 1.61s\tremaining: 1.03s\n",
      "610:\tlearn: 24.7038200\ttotal: 1.61s\tremaining: 1.02s\n",
      "611:\tlearn: 24.7014755\ttotal: 1.61s\tremaining: 1.02s\n",
      "612:\tlearn: 24.7001521\ttotal: 1.61s\tremaining: 1.02s\n",
      "613:\tlearn: 24.6958430\ttotal: 1.61s\tremaining: 1.01s\n",
      "614:\tlearn: 24.6924966\ttotal: 1.62s\tremaining: 1.01s\n",
      "615:\tlearn: 24.6911866\ttotal: 1.62s\tremaining: 1.01s\n",
      "616:\tlearn: 24.6881732\ttotal: 1.62s\tremaining: 1.01s\n",
      "617:\tlearn: 24.6871056\ttotal: 1.62s\tremaining: 1s\n",
      "618:\tlearn: 24.6835985\ttotal: 1.63s\tremaining: 1s\n",
      "619:\tlearn: 24.6801762\ttotal: 1.63s\tremaining: 998ms\n",
      "620:\tlearn: 24.6763488\ttotal: 1.63s\tremaining: 995ms\n",
      "621:\tlearn: 24.6748429\ttotal: 1.63s\tremaining: 993ms\n",
      "622:\tlearn: 24.6722583\ttotal: 1.64s\tremaining: 990ms\n",
      "623:\tlearn: 24.6672418\ttotal: 1.64s\tremaining: 987ms\n",
      "624:\tlearn: 24.6651611\ttotal: 1.64s\tremaining: 984ms\n",
      "625:\tlearn: 24.6623050\ttotal: 1.64s\tremaining: 981ms\n",
      "626:\tlearn: 24.6603316\ttotal: 1.64s\tremaining: 979ms\n",
      "627:\tlearn: 24.6562075\ttotal: 1.65s\tremaining: 976ms\n",
      "628:\tlearn: 24.6523112\ttotal: 1.65s\tremaining: 973ms\n",
      "629:\tlearn: 24.6495792\ttotal: 1.65s\tremaining: 970ms\n",
      "630:\tlearn: 24.6468571\ttotal: 1.65s\tremaining: 967ms\n",
      "631:\tlearn: 24.6404656\ttotal: 1.66s\tremaining: 965ms\n",
      "632:\tlearn: 24.6377087\ttotal: 1.66s\tremaining: 962ms\n",
      "633:\tlearn: 24.6369668\ttotal: 1.66s\tremaining: 959ms\n",
      "634:\tlearn: 24.6348252\ttotal: 1.66s\tremaining: 956ms\n",
      "635:\tlearn: 24.6303118\ttotal: 1.67s\tremaining: 953ms\n",
      "636:\tlearn: 24.6257271\ttotal: 1.67s\tremaining: 951ms\n",
      "637:\tlearn: 24.6235161\ttotal: 1.67s\tremaining: 948ms\n",
      "638:\tlearn: 24.6200542\ttotal: 1.67s\tremaining: 945ms\n",
      "639:\tlearn: 24.6163606\ttotal: 1.67s\tremaining: 942ms\n",
      "640:\tlearn: 24.6107361\ttotal: 1.68s\tremaining: 939ms\n",
      "641:\tlearn: 24.6059227\ttotal: 1.68s\tremaining: 937ms\n",
      "642:\tlearn: 24.6043697\ttotal: 1.68s\tremaining: 934ms\n",
      "643:\tlearn: 24.6001791\ttotal: 1.68s\tremaining: 931ms\n",
      "644:\tlearn: 24.5979591\ttotal: 1.69s\tremaining: 928ms\n",
      "645:\tlearn: 24.5960158\ttotal: 1.69s\tremaining: 925ms\n",
      "646:\tlearn: 24.5914795\ttotal: 1.69s\tremaining: 922ms\n",
      "647:\tlearn: 24.5877382\ttotal: 1.69s\tremaining: 919ms\n",
      "648:\tlearn: 24.5828173\ttotal: 1.69s\tremaining: 916ms\n",
      "649:\tlearn: 24.5802162\ttotal: 1.7s\tremaining: 913ms\n",
      "650:\tlearn: 24.5774926\ttotal: 1.7s\tremaining: 910ms\n",
      "651:\tlearn: 24.5741878\ttotal: 1.7s\tremaining: 908ms\n",
      "652:\tlearn: 24.5722645\ttotal: 1.7s\tremaining: 905ms\n",
      "653:\tlearn: 24.5697918\ttotal: 1.71s\tremaining: 902ms\n",
      "654:\tlearn: 24.5656089\ttotal: 1.71s\tremaining: 899ms\n",
      "655:\tlearn: 24.5618838\ttotal: 1.71s\tremaining: 897ms\n",
      "656:\tlearn: 24.5611733\ttotal: 1.71s\tremaining: 894ms\n",
      "657:\tlearn: 24.5597958\ttotal: 1.71s\tremaining: 891ms\n",
      "658:\tlearn: 24.5576139\ttotal: 1.72s\tremaining: 888ms\n",
      "659:\tlearn: 24.5544946\ttotal: 1.72s\tremaining: 885ms\n",
      "660:\tlearn: 24.5526994\ttotal: 1.72s\tremaining: 882ms\n",
      "661:\tlearn: 24.5497554\ttotal: 1.72s\tremaining: 879ms\n",
      "662:\tlearn: 24.5482240\ttotal: 1.72s\tremaining: 876ms\n",
      "663:\tlearn: 24.5449870\ttotal: 1.73s\tremaining: 873ms\n",
      "664:\tlearn: 24.5401598\ttotal: 1.73s\tremaining: 870ms\n",
      "665:\tlearn: 24.5375809\ttotal: 1.73s\tremaining: 867ms\n",
      "666:\tlearn: 24.5347689\ttotal: 1.73s\tremaining: 864ms\n",
      "667:\tlearn: 24.5324376\ttotal: 1.73s\tremaining: 862ms\n",
      "668:\tlearn: 24.5301716\ttotal: 1.74s\tremaining: 859ms\n",
      "669:\tlearn: 24.5279660\ttotal: 1.74s\tremaining: 856ms\n",
      "670:\tlearn: 24.5258290\ttotal: 1.74s\tremaining: 853ms\n",
      "671:\tlearn: 24.5208847\ttotal: 1.74s\tremaining: 850ms\n",
      "672:\tlearn: 24.5202417\ttotal: 1.75s\tremaining: 848ms\n",
      "673:\tlearn: 24.5196412\ttotal: 1.75s\tremaining: 846ms\n",
      "674:\tlearn: 24.5187912\ttotal: 1.75s\tremaining: 843ms\n",
      "675:\tlearn: 24.5169570\ttotal: 1.75s\tremaining: 840ms\n",
      "676:\tlearn: 24.5149460\ttotal: 1.75s\tremaining: 837ms\n",
      "677:\tlearn: 24.5141976\ttotal: 1.76s\tremaining: 835ms\n",
      "678:\tlearn: 24.5132904\ttotal: 1.76s\tremaining: 832ms\n",
      "679:\tlearn: 24.5128369\ttotal: 1.76s\tremaining: 830ms\n",
      "680:\tlearn: 24.5084642\ttotal: 1.76s\tremaining: 827ms\n",
      "681:\tlearn: 24.5054397\ttotal: 1.77s\tremaining: 824ms\n",
      "682:\tlearn: 24.5049637\ttotal: 1.77s\tremaining: 821ms\n",
      "683:\tlearn: 24.5029540\ttotal: 1.77s\tremaining: 819ms\n",
      "684:\tlearn: 24.5002069\ttotal: 1.77s\tremaining: 816ms\n",
      "685:\tlearn: 24.4904316\ttotal: 1.78s\tremaining: 814ms\n",
      "686:\tlearn: 24.4872336\ttotal: 1.78s\tremaining: 811ms\n",
      "687:\tlearn: 24.4822896\ttotal: 1.78s\tremaining: 808ms\n",
      "688:\tlearn: 24.4789986\ttotal: 1.78s\tremaining: 805ms\n",
      "689:\tlearn: 24.4784301\ttotal: 1.79s\tremaining: 802ms\n",
      "690:\tlearn: 24.4767509\ttotal: 1.79s\tremaining: 800ms\n",
      "691:\tlearn: 24.4747016\ttotal: 1.79s\tremaining: 797ms\n",
      "692:\tlearn: 24.4708425\ttotal: 1.79s\tremaining: 794ms\n",
      "693:\tlearn: 24.4672655\ttotal: 1.79s\tremaining: 791ms\n",
      "694:\tlearn: 24.4648245\ttotal: 1.8s\tremaining: 789ms\n",
      "695:\tlearn: 24.4638971\ttotal: 1.8s\tremaining: 786ms\n",
      "696:\tlearn: 24.4610717\ttotal: 1.8s\tremaining: 783ms\n",
      "697:\tlearn: 24.4551694\ttotal: 1.8s\tremaining: 780ms\n",
      "698:\tlearn: 24.4504898\ttotal: 1.8s\tremaining: 777ms\n",
      "699:\tlearn: 24.4459369\ttotal: 1.81s\tremaining: 775ms\n",
      "700:\tlearn: 24.4408567\ttotal: 1.81s\tremaining: 772ms\n",
      "701:\tlearn: 24.4346311\ttotal: 1.81s\tremaining: 769ms\n",
      "702:\tlearn: 24.4324673\ttotal: 1.81s\tremaining: 767ms\n",
      "703:\tlearn: 24.4292482\ttotal: 1.82s\tremaining: 764ms\n",
      "704:\tlearn: 24.4224771\ttotal: 1.82s\tremaining: 761ms\n",
      "705:\tlearn: 24.4198921\ttotal: 1.82s\tremaining: 758ms\n",
      "706:\tlearn: 24.4166733\ttotal: 1.82s\tremaining: 756ms\n",
      "707:\tlearn: 24.4111449\ttotal: 1.82s\tremaining: 753ms\n",
      "708:\tlearn: 24.4063013\ttotal: 1.83s\tremaining: 750ms\n",
      "709:\tlearn: 24.4024798\ttotal: 1.83s\tremaining: 747ms\n",
      "710:\tlearn: 24.3996572\ttotal: 1.83s\tremaining: 745ms\n",
      "711:\tlearn: 24.3960592\ttotal: 1.83s\tremaining: 742ms\n",
      "712:\tlearn: 24.3940916\ttotal: 1.84s\tremaining: 739ms\n",
      "713:\tlearn: 24.3893788\ttotal: 1.84s\tremaining: 736ms\n",
      "714:\tlearn: 24.3880906\ttotal: 1.84s\tremaining: 733ms\n",
      "715:\tlearn: 24.3823755\ttotal: 1.84s\tremaining: 731ms\n",
      "716:\tlearn: 24.3819403\ttotal: 1.84s\tremaining: 728ms\n",
      "717:\tlearn: 24.3813889\ttotal: 1.85s\tremaining: 725ms\n",
      "718:\tlearn: 24.3777482\ttotal: 1.85s\tremaining: 722ms\n",
      "719:\tlearn: 24.3746263\ttotal: 1.85s\tremaining: 720ms\n",
      "720:\tlearn: 24.3731436\ttotal: 1.85s\tremaining: 717ms\n",
      "721:\tlearn: 24.3723411\ttotal: 1.85s\tremaining: 714ms\n",
      "722:\tlearn: 24.3705588\ttotal: 1.86s\tremaining: 711ms\n",
      "723:\tlearn: 24.3676095\ttotal: 1.86s\tremaining: 708ms\n",
      "724:\tlearn: 24.3670800\ttotal: 1.86s\tremaining: 706ms\n",
      "725:\tlearn: 24.3643809\ttotal: 1.86s\tremaining: 703ms\n",
      "726:\tlearn: 24.3619511\ttotal: 1.86s\tremaining: 700ms\n",
      "727:\tlearn: 24.3577843\ttotal: 1.87s\tremaining: 698ms\n",
      "728:\tlearn: 24.3539106\ttotal: 1.87s\tremaining: 695ms\n",
      "729:\tlearn: 24.3500506\ttotal: 1.87s\tremaining: 692ms\n",
      "730:\tlearn: 24.3487279\ttotal: 1.87s\tremaining: 689ms\n",
      "731:\tlearn: 24.3475450\ttotal: 1.88s\tremaining: 687ms\n",
      "732:\tlearn: 24.3452775\ttotal: 1.88s\tremaining: 684ms\n",
      "733:\tlearn: 24.3413795\ttotal: 1.88s\tremaining: 681ms\n",
      "734:\tlearn: 24.3371312\ttotal: 1.88s\tremaining: 678ms\n",
      "735:\tlearn: 24.3357248\ttotal: 1.88s\tremaining: 676ms\n",
      "736:\tlearn: 24.3314528\ttotal: 1.89s\tremaining: 673ms\n",
      "737:\tlearn: 24.3285082\ttotal: 1.89s\tremaining: 670ms\n",
      "738:\tlearn: 24.3270667\ttotal: 1.89s\tremaining: 667ms\n",
      "739:\tlearn: 24.3258514\ttotal: 1.89s\tremaining: 665ms\n",
      "740:\tlearn: 24.3232144\ttotal: 1.89s\tremaining: 662ms\n",
      "741:\tlearn: 24.3207470\ttotal: 1.9s\tremaining: 659ms\n",
      "742:\tlearn: 24.3143345\ttotal: 1.9s\tremaining: 656ms\n",
      "743:\tlearn: 24.3105597\ttotal: 1.9s\tremaining: 654ms\n",
      "744:\tlearn: 24.3081072\ttotal: 1.9s\tremaining: 651ms\n",
      "745:\tlearn: 24.3075804\ttotal: 1.9s\tremaining: 648ms\n",
      "746:\tlearn: 24.3049740\ttotal: 1.91s\tremaining: 645ms\n",
      "747:\tlearn: 24.3014110\ttotal: 1.91s\tremaining: 643ms\n",
      "748:\tlearn: 24.2957296\ttotal: 1.91s\tremaining: 640ms\n",
      "749:\tlearn: 24.2933227\ttotal: 1.91s\tremaining: 637ms\n",
      "750:\tlearn: 24.2887690\ttotal: 1.91s\tremaining: 635ms\n",
      "751:\tlearn: 24.2835704\ttotal: 1.92s\tremaining: 632ms\n",
      "752:\tlearn: 24.2806852\ttotal: 1.92s\tremaining: 629ms\n",
      "753:\tlearn: 24.2772671\ttotal: 1.92s\tremaining: 627ms\n",
      "754:\tlearn: 24.2755353\ttotal: 1.92s\tremaining: 624ms\n",
      "755:\tlearn: 24.2733865\ttotal: 1.92s\tremaining: 621ms\n",
      "756:\tlearn: 24.2724188\ttotal: 1.93s\tremaining: 619ms\n",
      "757:\tlearn: 24.2702078\ttotal: 1.93s\tremaining: 616ms\n",
      "758:\tlearn: 24.2684666\ttotal: 1.93s\tremaining: 613ms\n",
      "759:\tlearn: 24.2654401\ttotal: 1.93s\tremaining: 610ms\n",
      "760:\tlearn: 24.2616169\ttotal: 1.93s\tremaining: 608ms\n",
      "761:\tlearn: 24.2582487\ttotal: 1.94s\tremaining: 605ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "762:\tlearn: 24.2570179\ttotal: 1.94s\tremaining: 602ms\n",
      "763:\tlearn: 24.2545796\ttotal: 1.94s\tremaining: 600ms\n",
      "764:\tlearn: 24.2518761\ttotal: 1.94s\tremaining: 597ms\n",
      "765:\tlearn: 24.2494225\ttotal: 1.95s\tremaining: 595ms\n",
      "766:\tlearn: 24.2457405\ttotal: 1.95s\tremaining: 592ms\n",
      "767:\tlearn: 24.2418877\ttotal: 1.95s\tremaining: 589ms\n",
      "768:\tlearn: 24.2411785\ttotal: 1.95s\tremaining: 587ms\n",
      "769:\tlearn: 24.2383213\ttotal: 1.96s\tremaining: 584ms\n",
      "770:\tlearn: 24.2335319\ttotal: 1.96s\tremaining: 582ms\n",
      "771:\tlearn: 24.2301369\ttotal: 1.96s\tremaining: 580ms\n",
      "772:\tlearn: 24.2272642\ttotal: 1.97s\tremaining: 577ms\n",
      "773:\tlearn: 24.2262358\ttotal: 1.97s\tremaining: 574ms\n",
      "774:\tlearn: 24.2258846\ttotal: 1.97s\tremaining: 572ms\n",
      "775:\tlearn: 24.2199885\ttotal: 1.97s\tremaining: 569ms\n",
      "776:\tlearn: 24.2195830\ttotal: 1.97s\tremaining: 566ms\n",
      "777:\tlearn: 24.2146022\ttotal: 1.98s\tremaining: 564ms\n",
      "778:\tlearn: 24.2130734\ttotal: 1.98s\tremaining: 561ms\n",
      "779:\tlearn: 24.2115406\ttotal: 1.98s\tremaining: 559ms\n",
      "780:\tlearn: 24.2098446\ttotal: 1.98s\tremaining: 556ms\n",
      "781:\tlearn: 24.2086995\ttotal: 1.98s\tremaining: 553ms\n",
      "782:\tlearn: 24.2067989\ttotal: 1.99s\tremaining: 551ms\n",
      "783:\tlearn: 24.2035007\ttotal: 1.99s\tremaining: 548ms\n",
      "784:\tlearn: 24.2012760\ttotal: 1.99s\tremaining: 545ms\n",
      "785:\tlearn: 24.1989499\ttotal: 1.99s\tremaining: 543ms\n",
      "786:\tlearn: 24.1973189\ttotal: 2s\tremaining: 540ms\n",
      "787:\tlearn: 24.1921270\ttotal: 2s\tremaining: 537ms\n",
      "788:\tlearn: 24.1904581\ttotal: 2s\tremaining: 535ms\n",
      "789:\tlearn: 24.1852741\ttotal: 2s\tremaining: 532ms\n",
      "790:\tlearn: 24.1795469\ttotal: 2s\tremaining: 529ms\n",
      "791:\tlearn: 24.1761082\ttotal: 2s\tremaining: 527ms\n",
      "792:\tlearn: 24.1748547\ttotal: 2.01s\tremaining: 524ms\n",
      "793:\tlearn: 24.1743501\ttotal: 2.01s\tremaining: 521ms\n",
      "794:\tlearn: 24.1719237\ttotal: 2.01s\tremaining: 519ms\n",
      "795:\tlearn: 24.1664372\ttotal: 2.01s\tremaining: 516ms\n",
      "796:\tlearn: 24.1625119\ttotal: 2.02s\tremaining: 514ms\n",
      "797:\tlearn: 24.1576746\ttotal: 2.02s\tremaining: 511ms\n",
      "798:\tlearn: 24.1530795\ttotal: 2.02s\tremaining: 508ms\n",
      "799:\tlearn: 24.1482594\ttotal: 2.02s\tremaining: 506ms\n",
      "800:\tlearn: 24.1464681\ttotal: 2.02s\tremaining: 503ms\n",
      "801:\tlearn: 24.1447604\ttotal: 2.03s\tremaining: 500ms\n",
      "802:\tlearn: 24.1404602\ttotal: 2.03s\tremaining: 498ms\n",
      "803:\tlearn: 24.1384402\ttotal: 2.03s\tremaining: 495ms\n",
      "804:\tlearn: 24.1373091\ttotal: 2.03s\tremaining: 493ms\n",
      "805:\tlearn: 24.1366930\ttotal: 2.04s\tremaining: 490ms\n",
      "806:\tlearn: 24.1362949\ttotal: 2.04s\tremaining: 487ms\n",
      "807:\tlearn: 24.1343612\ttotal: 2.04s\tremaining: 485ms\n",
      "808:\tlearn: 24.1321045\ttotal: 2.04s\tremaining: 482ms\n",
      "809:\tlearn: 24.1218884\ttotal: 2.04s\tremaining: 479ms\n",
      "810:\tlearn: 24.1202754\ttotal: 2.04s\tremaining: 477ms\n",
      "811:\tlearn: 24.1182499\ttotal: 2.05s\tremaining: 474ms\n",
      "812:\tlearn: 24.1147274\ttotal: 2.05s\tremaining: 471ms\n",
      "813:\tlearn: 24.1133362\ttotal: 2.05s\tremaining: 469ms\n",
      "814:\tlearn: 24.1102438\ttotal: 2.05s\tremaining: 466ms\n",
      "815:\tlearn: 24.1065337\ttotal: 2.06s\tremaining: 464ms\n",
      "816:\tlearn: 24.1051541\ttotal: 2.06s\tremaining: 461ms\n",
      "817:\tlearn: 24.1027116\ttotal: 2.06s\tremaining: 458ms\n",
      "818:\tlearn: 24.1007620\ttotal: 2.06s\tremaining: 456ms\n",
      "819:\tlearn: 24.0979466\ttotal: 2.06s\tremaining: 453ms\n",
      "820:\tlearn: 24.0929294\ttotal: 2.07s\tremaining: 451ms\n",
      "821:\tlearn: 24.0905112\ttotal: 2.07s\tremaining: 448ms\n",
      "822:\tlearn: 24.0872710\ttotal: 2.07s\tremaining: 445ms\n",
      "823:\tlearn: 24.0852309\ttotal: 2.07s\tremaining: 443ms\n",
      "824:\tlearn: 24.0830416\ttotal: 2.07s\tremaining: 440ms\n",
      "825:\tlearn: 24.0817986\ttotal: 2.08s\tremaining: 437ms\n",
      "826:\tlearn: 24.0787306\ttotal: 2.08s\tremaining: 435ms\n",
      "827:\tlearn: 24.0754120\ttotal: 2.08s\tremaining: 432ms\n",
      "828:\tlearn: 24.0725756\ttotal: 2.08s\tremaining: 430ms\n",
      "829:\tlearn: 24.0695992\ttotal: 2.08s\tremaining: 427ms\n",
      "830:\tlearn: 24.0680119\ttotal: 2.09s\tremaining: 424ms\n",
      "831:\tlearn: 24.0653375\ttotal: 2.09s\tremaining: 422ms\n",
      "832:\tlearn: 24.0622479\ttotal: 2.09s\tremaining: 419ms\n",
      "833:\tlearn: 24.0602125\ttotal: 2.09s\tremaining: 417ms\n",
      "834:\tlearn: 24.0552349\ttotal: 2.1s\tremaining: 414ms\n",
      "835:\tlearn: 24.0513975\ttotal: 2.1s\tremaining: 411ms\n",
      "836:\tlearn: 24.0472290\ttotal: 2.1s\tremaining: 409ms\n",
      "837:\tlearn: 24.0440972\ttotal: 2.1s\tremaining: 406ms\n",
      "838:\tlearn: 24.0419160\ttotal: 2.1s\tremaining: 404ms\n",
      "839:\tlearn: 24.0390214\ttotal: 2.1s\tremaining: 401ms\n",
      "840:\tlearn: 24.0328908\ttotal: 2.11s\tremaining: 398ms\n",
      "841:\tlearn: 24.0299351\ttotal: 2.11s\tremaining: 396ms\n",
      "842:\tlearn: 24.0276908\ttotal: 2.11s\tremaining: 393ms\n",
      "843:\tlearn: 24.0271697\ttotal: 2.11s\tremaining: 391ms\n",
      "844:\tlearn: 24.0246379\ttotal: 2.12s\tremaining: 388ms\n",
      "845:\tlearn: 24.0231037\ttotal: 2.12s\tremaining: 386ms\n",
      "846:\tlearn: 24.0185999\ttotal: 2.12s\tremaining: 383ms\n",
      "847:\tlearn: 24.0175685\ttotal: 2.12s\tremaining: 380ms\n",
      "848:\tlearn: 24.0106564\ttotal: 2.12s\tremaining: 378ms\n",
      "849:\tlearn: 24.0085461\ttotal: 2.13s\tremaining: 375ms\n",
      "850:\tlearn: 24.0064123\ttotal: 2.13s\tremaining: 373ms\n",
      "851:\tlearn: 24.0055100\ttotal: 2.13s\tremaining: 370ms\n",
      "852:\tlearn: 23.9979383\ttotal: 2.13s\tremaining: 368ms\n",
      "853:\tlearn: 23.9972106\ttotal: 2.13s\tremaining: 365ms\n",
      "854:\tlearn: 23.9948059\ttotal: 2.14s\tremaining: 363ms\n",
      "855:\tlearn: 23.9924282\ttotal: 2.14s\tremaining: 360ms\n",
      "856:\tlearn: 23.9909325\ttotal: 2.14s\tremaining: 357ms\n",
      "857:\tlearn: 23.9896686\ttotal: 2.14s\tremaining: 355ms\n",
      "858:\tlearn: 23.9882312\ttotal: 2.15s\tremaining: 352ms\n",
      "859:\tlearn: 23.9869060\ttotal: 2.15s\tremaining: 350ms\n",
      "860:\tlearn: 23.9852931\ttotal: 2.15s\tremaining: 348ms\n",
      "861:\tlearn: 23.9840228\ttotal: 2.15s\tremaining: 345ms\n",
      "862:\tlearn: 23.9794926\ttotal: 2.16s\tremaining: 342ms\n",
      "863:\tlearn: 23.9747986\ttotal: 2.16s\tremaining: 340ms\n",
      "864:\tlearn: 23.9724171\ttotal: 2.16s\tremaining: 337ms\n",
      "865:\tlearn: 23.9694365\ttotal: 2.16s\tremaining: 335ms\n",
      "866:\tlearn: 23.9651557\ttotal: 2.17s\tremaining: 332ms\n",
      "867:\tlearn: 23.9587140\ttotal: 2.17s\tremaining: 330ms\n",
      "868:\tlearn: 23.9569573\ttotal: 2.17s\tremaining: 327ms\n",
      "869:\tlearn: 23.9560706\ttotal: 2.17s\tremaining: 324ms\n",
      "870:\tlearn: 23.9530124\ttotal: 2.17s\tremaining: 322ms\n",
      "871:\tlearn: 23.9502942\ttotal: 2.17s\tremaining: 319ms\n",
      "872:\tlearn: 23.9489182\ttotal: 2.18s\tremaining: 317ms\n",
      "873:\tlearn: 23.9467671\ttotal: 2.18s\tremaining: 314ms\n",
      "874:\tlearn: 23.9445172\ttotal: 2.18s\tremaining: 312ms\n",
      "875:\tlearn: 23.9426826\ttotal: 2.18s\tremaining: 309ms\n",
      "876:\tlearn: 23.9410786\ttotal: 2.19s\tremaining: 307ms\n",
      "877:\tlearn: 23.9404096\ttotal: 2.19s\tremaining: 304ms\n",
      "878:\tlearn: 23.9379751\ttotal: 2.19s\tremaining: 302ms\n",
      "879:\tlearn: 23.9351945\ttotal: 2.19s\tremaining: 299ms\n",
      "880:\tlearn: 23.9342091\ttotal: 2.19s\tremaining: 297ms\n",
      "881:\tlearn: 23.9313493\ttotal: 2.2s\tremaining: 294ms\n",
      "882:\tlearn: 23.9297906\ttotal: 2.2s\tremaining: 292ms\n",
      "883:\tlearn: 23.9284135\ttotal: 2.2s\tremaining: 289ms\n",
      "884:\tlearn: 23.9268827\ttotal: 2.2s\tremaining: 286ms\n",
      "885:\tlearn: 23.9245598\ttotal: 2.21s\tremaining: 284ms\n",
      "886:\tlearn: 23.9227103\ttotal: 2.21s\tremaining: 281ms\n",
      "887:\tlearn: 23.9205342\ttotal: 2.21s\tremaining: 279ms\n",
      "888:\tlearn: 23.9185449\ttotal: 2.21s\tremaining: 276ms\n",
      "889:\tlearn: 23.9178762\ttotal: 2.21s\tremaining: 274ms\n",
      "890:\tlearn: 23.9160906\ttotal: 2.22s\tremaining: 271ms\n",
      "891:\tlearn: 23.9131654\ttotal: 2.22s\tremaining: 269ms\n",
      "892:\tlearn: 23.9105395\ttotal: 2.22s\tremaining: 266ms\n",
      "893:\tlearn: 23.9074255\ttotal: 2.22s\tremaining: 264ms\n",
      "894:\tlearn: 23.9063387\ttotal: 2.23s\tremaining: 261ms\n",
      "895:\tlearn: 23.9029873\ttotal: 2.23s\tremaining: 259ms\n",
      "896:\tlearn: 23.8991869\ttotal: 2.23s\tremaining: 256ms\n",
      "897:\tlearn: 23.8977115\ttotal: 2.23s\tremaining: 253ms\n",
      "898:\tlearn: 23.8948452\ttotal: 2.23s\tremaining: 251ms\n",
      "899:\tlearn: 23.8899145\ttotal: 2.23s\tremaining: 248ms\n",
      "900:\tlearn: 23.8840516\ttotal: 2.24s\tremaining: 246ms\n",
      "901:\tlearn: 23.8836690\ttotal: 2.24s\tremaining: 243ms\n",
      "902:\tlearn: 23.8804971\ttotal: 2.24s\tremaining: 241ms\n",
      "903:\tlearn: 23.8777839\ttotal: 2.24s\tremaining: 238ms\n",
      "904:\tlearn: 23.8764376\ttotal: 2.25s\tremaining: 236ms\n",
      "905:\tlearn: 23.8758093\ttotal: 2.25s\tremaining: 233ms\n",
      "906:\tlearn: 23.8738142\ttotal: 2.25s\tremaining: 231ms\n",
      "907:\tlearn: 23.8697140\ttotal: 2.25s\tremaining: 228ms\n",
      "908:\tlearn: 23.8681643\ttotal: 2.25s\tremaining: 226ms\n",
      "909:\tlearn: 23.8672531\ttotal: 2.26s\tremaining: 223ms\n",
      "910:\tlearn: 23.8643462\ttotal: 2.26s\tremaining: 221ms\n",
      "911:\tlearn: 23.8623275\ttotal: 2.26s\tremaining: 218ms\n",
      "912:\tlearn: 23.8614949\ttotal: 2.26s\tremaining: 216ms\n",
      "913:\tlearn: 23.8605624\ttotal: 2.26s\tremaining: 213ms\n",
      "914:\tlearn: 23.8588195\ttotal: 2.27s\tremaining: 211ms\n",
      "915:\tlearn: 23.8554420\ttotal: 2.27s\tremaining: 208ms\n",
      "916:\tlearn: 23.8534815\ttotal: 2.27s\tremaining: 206ms\n",
      "917:\tlearn: 23.8521600\ttotal: 2.27s\tremaining: 203ms\n",
      "918:\tlearn: 23.8490951\ttotal: 2.27s\tremaining: 201ms\n",
      "919:\tlearn: 23.8466305\ttotal: 2.28s\tremaining: 198ms\n",
      "920:\tlearn: 23.8445728\ttotal: 2.28s\tremaining: 196ms\n",
      "921:\tlearn: 23.8437975\ttotal: 2.28s\tremaining: 193ms\n",
      "922:\tlearn: 23.8419155\ttotal: 2.28s\tremaining: 191ms\n",
      "923:\tlearn: 23.8389952\ttotal: 2.29s\tremaining: 188ms\n",
      "924:\tlearn: 23.8378182\ttotal: 2.29s\tremaining: 186ms\n",
      "925:\tlearn: 23.8368841\ttotal: 2.29s\tremaining: 183ms\n",
      "926:\tlearn: 23.8346793\ttotal: 2.29s\tremaining: 180ms\n",
      "927:\tlearn: 23.8334861\ttotal: 2.29s\tremaining: 178ms\n",
      "928:\tlearn: 23.8288022\ttotal: 2.3s\tremaining: 175ms\n",
      "929:\tlearn: 23.8276548\ttotal: 2.3s\tremaining: 173ms\n",
      "930:\tlearn: 23.8272852\ttotal: 2.3s\tremaining: 170ms\n",
      "931:\tlearn: 23.8255863\ttotal: 2.3s\tremaining: 168ms\n",
      "932:\tlearn: 23.8190722\ttotal: 2.3s\tremaining: 165ms\n",
      "933:\tlearn: 23.8173848\ttotal: 2.31s\tremaining: 163ms\n",
      "934:\tlearn: 23.8136961\ttotal: 2.31s\tremaining: 160ms\n",
      "935:\tlearn: 23.8112054\ttotal: 2.31s\tremaining: 158ms\n",
      "936:\tlearn: 23.8089069\ttotal: 2.31s\tremaining: 155ms\n",
      "937:\tlearn: 23.8067003\ttotal: 2.31s\tremaining: 153ms\n",
      "938:\tlearn: 23.8026816\ttotal: 2.32s\tremaining: 150ms\n",
      "939:\tlearn: 23.7994129\ttotal: 2.32s\tremaining: 148ms\n",
      "940:\tlearn: 23.7976670\ttotal: 2.32s\tremaining: 145ms\n",
      "941:\tlearn: 23.7971356\ttotal: 2.32s\tremaining: 143ms\n",
      "942:\tlearn: 23.7958740\ttotal: 2.32s\tremaining: 140ms\n",
      "943:\tlearn: 23.7943710\ttotal: 2.33s\tremaining: 138ms\n",
      "944:\tlearn: 23.7931746\ttotal: 2.33s\tremaining: 136ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "945:\tlearn: 23.7908728\ttotal: 2.33s\tremaining: 133ms\n",
      "946:\tlearn: 23.7888536\ttotal: 2.33s\tremaining: 131ms\n",
      "947:\tlearn: 23.7863852\ttotal: 2.34s\tremaining: 128ms\n",
      "948:\tlearn: 23.7854500\ttotal: 2.34s\tremaining: 126ms\n",
      "949:\tlearn: 23.7830519\ttotal: 2.34s\tremaining: 123ms\n",
      "950:\tlearn: 23.7814928\ttotal: 2.34s\tremaining: 121ms\n",
      "951:\tlearn: 23.7782675\ttotal: 2.35s\tremaining: 118ms\n",
      "952:\tlearn: 23.7779551\ttotal: 2.35s\tremaining: 116ms\n",
      "953:\tlearn: 23.7761737\ttotal: 2.35s\tremaining: 113ms\n",
      "954:\tlearn: 23.7752563\ttotal: 2.35s\tremaining: 111ms\n",
      "955:\tlearn: 23.7727756\ttotal: 2.36s\tremaining: 108ms\n",
      "956:\tlearn: 23.7703370\ttotal: 2.36s\tremaining: 106ms\n",
      "957:\tlearn: 23.7696810\ttotal: 2.36s\tremaining: 103ms\n",
      "958:\tlearn: 23.7680484\ttotal: 2.36s\tremaining: 101ms\n",
      "959:\tlearn: 23.7667220\ttotal: 2.37s\tremaining: 98.5ms\n",
      "960:\tlearn: 23.7650871\ttotal: 2.37s\tremaining: 96.1ms\n",
      "961:\tlearn: 23.7640121\ttotal: 2.37s\tremaining: 93.6ms\n",
      "962:\tlearn: 23.7617192\ttotal: 2.37s\tremaining: 91.1ms\n",
      "963:\tlearn: 23.7598286\ttotal: 2.37s\tremaining: 88.6ms\n",
      "964:\tlearn: 23.7589604\ttotal: 2.38s\tremaining: 86.2ms\n",
      "965:\tlearn: 23.7572891\ttotal: 2.38s\tremaining: 83.7ms\n",
      "966:\tlearn: 23.7534151\ttotal: 2.38s\tremaining: 81.2ms\n",
      "967:\tlearn: 23.7514548\ttotal: 2.38s\tremaining: 78.8ms\n",
      "968:\tlearn: 23.7505522\ttotal: 2.38s\tremaining: 76.3ms\n",
      "969:\tlearn: 23.7498049\ttotal: 2.39s\tremaining: 73.8ms\n",
      "970:\tlearn: 23.7486849\ttotal: 2.39s\tremaining: 71.3ms\n",
      "971:\tlearn: 23.7455651\ttotal: 2.39s\tremaining: 68.9ms\n",
      "972:\tlearn: 23.7443507\ttotal: 2.39s\tremaining: 66.4ms\n",
      "973:\tlearn: 23.7413914\ttotal: 2.4s\tremaining: 63.9ms\n",
      "974:\tlearn: 23.7402311\ttotal: 2.4s\tremaining: 61.5ms\n",
      "975:\tlearn: 23.7394852\ttotal: 2.4s\tremaining: 59ms\n",
      "976:\tlearn: 23.7373070\ttotal: 2.4s\tremaining: 56.5ms\n",
      "977:\tlearn: 23.7353923\ttotal: 2.4s\tremaining: 54.1ms\n",
      "978:\tlearn: 23.7321210\ttotal: 2.4s\tremaining: 51.6ms\n",
      "979:\tlearn: 23.7309705\ttotal: 2.41s\tremaining: 49.1ms\n",
      "980:\tlearn: 23.7286660\ttotal: 2.41s\tremaining: 46.7ms\n",
      "981:\tlearn: 23.7263547\ttotal: 2.41s\tremaining: 44.2ms\n",
      "982:\tlearn: 23.7233563\ttotal: 2.41s\tremaining: 41.8ms\n",
      "983:\tlearn: 23.7201250\ttotal: 2.42s\tremaining: 39.3ms\n",
      "984:\tlearn: 23.7184615\ttotal: 2.42s\tremaining: 36.8ms\n",
      "985:\tlearn: 23.7153040\ttotal: 2.42s\tremaining: 34.4ms\n",
      "986:\tlearn: 23.7129748\ttotal: 2.42s\tremaining: 31.9ms\n",
      "987:\tlearn: 23.7127889\ttotal: 2.42s\tremaining: 29.4ms\n",
      "988:\tlearn: 23.7099311\ttotal: 2.43s\tremaining: 27ms\n",
      "989:\tlearn: 23.7089199\ttotal: 2.43s\tremaining: 24.5ms\n",
      "990:\tlearn: 23.7052157\ttotal: 2.43s\tremaining: 22.1ms\n",
      "991:\tlearn: 23.7041157\ttotal: 2.43s\tremaining: 19.6ms\n",
      "992:\tlearn: 23.7018739\ttotal: 2.43s\tremaining: 17.2ms\n",
      "993:\tlearn: 23.6967747\ttotal: 2.44s\tremaining: 14.7ms\n",
      "994:\tlearn: 23.6948837\ttotal: 2.44s\tremaining: 12.3ms\n",
      "995:\tlearn: 23.6937789\ttotal: 2.44s\tremaining: 9.8ms\n",
      "996:\tlearn: 23.6906790\ttotal: 2.44s\tremaining: 7.35ms\n",
      "997:\tlearn: 23.6885442\ttotal: 2.44s\tremaining: 4.9ms\n",
      "998:\tlearn: 23.6874481\ttotal: 2.45s\tremaining: 2.45ms\n",
      "999:\tlearn: 23.6863946\ttotal: 2.45s\tremaining: 0us\n",
      "MSE = 1442.8007235240348\n",
      "R_2 = 0.06460378654343335\n"
     ]
    }
   ],
   "source": [
    "from catboost import CatBoostRegressor\n",
    "\n",
    "cb = CatBoostRegressor(random_state=42)\n",
    "cb.fit(X_train, y_train)\n",
    "y_pred = cb.predict(X_test)\n",
    "print(f'MSE = {mse(y_test, y_pred)}')\n",
    "print(f'R_2 = {r2_score(y_test, y_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь на данных без предобработки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_new = pd.DataFrame({\n",
    "    \"number_of_answered_questions\" : X_train[\"number_of_answered_questions\"],\n",
    "    \"number_of_reviews\" : X_train[\"number_of_reviews\"],\n",
    "    \"average_review_rating\" : X_train[\"average_review_rating\"]\n",
    "}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_new = pd.DataFrame({\n",
    "    \"number_of_answered_questions\" : X_test[\"number_of_answered_questions\"],\n",
    "    \"number_of_reviews\" : X_test[\"number_of_reviews\"],\n",
    "    \"average_review_rating\" : X_test[\"average_review_rating\"]\n",
    "}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number_of_answered_questions</th>\n",
       "      <th>number_of_reviews</th>\n",
       "      <th>average_review_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>4.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.0</td>\n",
       "      <td>183.0</td>\n",
       "      <td>4.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2182</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2183</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2184</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2185</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2186</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2187 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      number_of_answered_questions  number_of_reviews  average_review_rating\n",
       "0                              2.0               17.0                    4.4\n",
       "1                              1.0                1.0                    5.0\n",
       "2                              1.0                1.0                    5.0\n",
       "3                              1.0                2.0                    5.0\n",
       "4                              3.0              183.0                    4.7\n",
       "...                            ...                ...                    ...\n",
       "2182                           2.0                1.0                    5.0\n",
       "2183                           1.0                1.0                    5.0\n",
       "2184                           1.0                1.0                    5.0\n",
       "2185                           2.0                2.0                    5.0\n",
       "2186                           1.0                4.0                    4.8\n",
       "\n",
       "[2187 rows x 3 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Качество ухудшилось"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.051237\n",
      "0:\tlearn: 32.3129891\ttotal: 2.25ms\tremaining: 2.25s\n",
      "1:\tlearn: 32.2197078\ttotal: 5.06ms\tremaining: 2.52s\n",
      "2:\tlearn: 32.1505345\ttotal: 15.1ms\tremaining: 5.01s\n",
      "3:\tlearn: 32.0543707\ttotal: 18.7ms\tremaining: 4.67s\n",
      "4:\tlearn: 31.9662956\ttotal: 21.7ms\tremaining: 4.31s\n",
      "5:\tlearn: 31.8805516\ttotal: 25.7ms\tremaining: 4.25s\n",
      "6:\tlearn: 31.7928738\ttotal: 28.4ms\tremaining: 4.03s\n",
      "7:\tlearn: 31.7117765\ttotal: 31.4ms\tremaining: 3.89s\n",
      "8:\tlearn: 31.6274779\ttotal: 34ms\tremaining: 3.75s\n",
      "9:\tlearn: 31.5484886\ttotal: 36.7ms\tremaining: 3.63s\n",
      "10:\tlearn: 31.4729311\ttotal: 39.9ms\tremaining: 3.58s\n",
      "11:\tlearn: 31.3959256\ttotal: 42.5ms\tremaining: 3.5s\n",
      "12:\tlearn: 31.3244578\ttotal: 44.9ms\tremaining: 3.41s\n",
      "13:\tlearn: 31.2527713\ttotal: 47.6ms\tremaining: 3.35s\n",
      "14:\tlearn: 31.1824510\ttotal: 50.2ms\tremaining: 3.29s\n",
      "15:\tlearn: 31.1124397\ttotal: 53.5ms\tremaining: 3.29s\n",
      "16:\tlearn: 31.0469791\ttotal: 56.2ms\tremaining: 3.25s\n",
      "17:\tlearn: 30.9805212\ttotal: 58.9ms\tremaining: 3.21s\n",
      "18:\tlearn: 30.9176473\ttotal: 61.2ms\tremaining: 3.16s\n",
      "19:\tlearn: 30.8581961\ttotal: 62.9ms\tremaining: 3.08s\n",
      "20:\tlearn: 30.8002005\ttotal: 64.8ms\tremaining: 3.02s\n",
      "21:\tlearn: 30.7554508\ttotal: 66.5ms\tremaining: 2.96s\n",
      "22:\tlearn: 30.6969564\ttotal: 68.7ms\tremaining: 2.92s\n",
      "23:\tlearn: 30.6405004\ttotal: 70.5ms\tremaining: 2.87s\n",
      "24:\tlearn: 30.5864923\ttotal: 72.4ms\tremaining: 2.82s\n",
      "25:\tlearn: 30.5349584\ttotal: 74.3ms\tremaining: 2.78s\n",
      "26:\tlearn: 30.4846950\ttotal: 76.9ms\tremaining: 2.77s\n",
      "27:\tlearn: 30.4355911\ttotal: 79.5ms\tremaining: 2.76s\n",
      "28:\tlearn: 30.3845383\ttotal: 81.3ms\tremaining: 2.72s\n",
      "29:\tlearn: 30.3376785\ttotal: 83.1ms\tremaining: 2.69s\n",
      "30:\tlearn: 30.2900190\ttotal: 85.1ms\tremaining: 2.66s\n",
      "31:\tlearn: 30.2445712\ttotal: 86.6ms\tremaining: 2.62s\n",
      "32:\tlearn: 30.2009135\ttotal: 88.5ms\tremaining: 2.59s\n",
      "33:\tlearn: 30.1562603\ttotal: 90ms\tremaining: 2.56s\n",
      "34:\tlearn: 30.1143897\ttotal: 91.9ms\tremaining: 2.53s\n",
      "35:\tlearn: 30.0727355\ttotal: 93.8ms\tremaining: 2.51s\n",
      "36:\tlearn: 30.0326643\ttotal: 95.7ms\tremaining: 2.49s\n",
      "37:\tlearn: 29.9885072\ttotal: 97.7ms\tremaining: 2.47s\n",
      "38:\tlearn: 29.9579052\ttotal: 99.8ms\tremaining: 2.46s\n",
      "39:\tlearn: 29.9199595\ttotal: 102ms\tremaining: 2.44s\n",
      "40:\tlearn: 29.8841965\ttotal: 103ms\tremaining: 2.41s\n",
      "41:\tlearn: 29.8474706\ttotal: 105ms\tremaining: 2.39s\n",
      "42:\tlearn: 29.8121059\ttotal: 107ms\tremaining: 2.38s\n",
      "43:\tlearn: 29.7785891\ttotal: 108ms\tremaining: 2.35s\n",
      "44:\tlearn: 29.7432826\ttotal: 110ms\tremaining: 2.34s\n",
      "45:\tlearn: 29.7106839\ttotal: 112ms\tremaining: 2.33s\n",
      "46:\tlearn: 29.6779363\ttotal: 114ms\tremaining: 2.31s\n",
      "47:\tlearn: 29.6475363\ttotal: 115ms\tremaining: 2.29s\n",
      "48:\tlearn: 29.6168970\ttotal: 117ms\tremaining: 2.28s\n",
      "49:\tlearn: 29.5872809\ttotal: 119ms\tremaining: 2.26s\n",
      "50:\tlearn: 29.5583495\ttotal: 121ms\tremaining: 2.25s\n",
      "51:\tlearn: 29.5294122\ttotal: 123ms\tremaining: 2.24s\n",
      "52:\tlearn: 29.5023386\ttotal: 125ms\tremaining: 2.23s\n",
      "53:\tlearn: 29.4759435\ttotal: 127ms\tremaining: 2.23s\n",
      "54:\tlearn: 29.4500115\ttotal: 129ms\tremaining: 2.22s\n",
      "55:\tlearn: 29.4242643\ttotal: 131ms\tremaining: 2.21s\n",
      "56:\tlearn: 29.3995344\ttotal: 133ms\tremaining: 2.2s\n",
      "57:\tlearn: 29.3757150\ttotal: 135ms\tremaining: 2.18s\n",
      "58:\tlearn: 29.3522800\ttotal: 136ms\tremaining: 2.17s\n",
      "59:\tlearn: 29.3298135\ttotal: 138ms\tremaining: 2.17s\n",
      "60:\tlearn: 29.3062375\ttotal: 140ms\tremaining: 2.15s\n",
      "61:\tlearn: 29.2846084\ttotal: 141ms\tremaining: 2.14s\n",
      "62:\tlearn: 29.2655750\ttotal: 143ms\tremaining: 2.13s\n",
      "63:\tlearn: 29.2446017\ttotal: 145ms\tremaining: 2.13s\n",
      "64:\tlearn: 29.2240533\ttotal: 147ms\tremaining: 2.12s\n",
      "65:\tlearn: 29.2034281\ttotal: 149ms\tremaining: 2.11s\n",
      "66:\tlearn: 29.1836283\ttotal: 151ms\tremaining: 2.1s\n",
      "67:\tlearn: 29.1637400\ttotal: 153ms\tremaining: 2.1s\n",
      "68:\tlearn: 29.1418040\ttotal: 155ms\tremaining: 2.09s\n",
      "69:\tlearn: 29.1241535\ttotal: 157ms\tremaining: 2.09s\n",
      "70:\tlearn: 29.1067511\ttotal: 160ms\tremaining: 2.09s\n",
      "71:\tlearn: 29.0898695\ttotal: 162ms\tremaining: 2.09s\n",
      "72:\tlearn: 29.0716123\ttotal: 164ms\tremaining: 2.08s\n",
      "73:\tlearn: 29.0556927\ttotal: 165ms\tremaining: 2.07s\n",
      "74:\tlearn: 29.0399897\ttotal: 167ms\tremaining: 2.06s\n",
      "75:\tlearn: 29.0249270\ttotal: 169ms\tremaining: 2.06s\n",
      "76:\tlearn: 29.0099152\ttotal: 171ms\tremaining: 2.05s\n",
      "77:\tlearn: 28.9948841\ttotal: 173ms\tremaining: 2.04s\n",
      "78:\tlearn: 28.9805190\ttotal: 175ms\tremaining: 2.04s\n",
      "79:\tlearn: 28.9665393\ttotal: 177ms\tremaining: 2.03s\n",
      "80:\tlearn: 28.9521338\ttotal: 179ms\tremaining: 2.02s\n",
      "81:\tlearn: 28.9380875\ttotal: 180ms\tremaining: 2.02s\n",
      "82:\tlearn: 28.9252261\ttotal: 182ms\tremaining: 2.01s\n",
      "83:\tlearn: 28.9127003\ttotal: 185ms\tremaining: 2.02s\n",
      "84:\tlearn: 28.9004944\ttotal: 188ms\tremaining: 2.02s\n",
      "85:\tlearn: 28.8885096\ttotal: 190ms\tremaining: 2.02s\n",
      "86:\tlearn: 28.8778872\ttotal: 193ms\tremaining: 2.02s\n",
      "87:\tlearn: 28.8654322\ttotal: 196ms\tremaining: 2.03s\n",
      "88:\tlearn: 28.8543306\ttotal: 198ms\tremaining: 2.03s\n",
      "89:\tlearn: 28.8434832\ttotal: 200ms\tremaining: 2.02s\n",
      "90:\tlearn: 28.8327771\ttotal: 202ms\tremaining: 2.01s\n",
      "91:\tlearn: 28.8216687\ttotal: 203ms\tremaining: 2.01s\n",
      "92:\tlearn: 28.8115744\ttotal: 205ms\tremaining: 2s\n",
      "93:\tlearn: 28.8021106\ttotal: 207ms\tremaining: 2s\n",
      "94:\tlearn: 28.7951556\ttotal: 210ms\tremaining: 2s\n",
      "95:\tlearn: 28.7857718\ttotal: 212ms\tremaining: 2s\n",
      "96:\tlearn: 28.7762885\ttotal: 214ms\tremaining: 1.99s\n",
      "97:\tlearn: 28.7669249\ttotal: 216ms\tremaining: 1.99s\n",
      "98:\tlearn: 28.7580997\ttotal: 218ms\tremaining: 1.98s\n",
      "99:\tlearn: 28.7538452\ttotal: 219ms\tremaining: 1.97s\n",
      "100:\tlearn: 28.7455802\ttotal: 221ms\tremaining: 1.97s\n",
      "101:\tlearn: 28.7354228\ttotal: 223ms\tremaining: 1.97s\n",
      "102:\tlearn: 28.7260968\ttotal: 225ms\tremaining: 1.96s\n",
      "103:\tlearn: 28.7124036\ttotal: 228ms\tremaining: 1.96s\n",
      "104:\tlearn: 28.7026899\ttotal: 230ms\tremaining: 1.96s\n",
      "105:\tlearn: 28.6943664\ttotal: 231ms\tremaining: 1.95s\n",
      "106:\tlearn: 28.6890497\ttotal: 233ms\tremaining: 1.95s\n",
      "107:\tlearn: 28.6796517\ttotal: 235ms\tremaining: 1.94s\n",
      "108:\tlearn: 28.6703306\ttotal: 237ms\tremaining: 1.94s\n",
      "109:\tlearn: 28.6665188\ttotal: 239ms\tremaining: 1.94s\n",
      "110:\tlearn: 28.6593726\ttotal: 241ms\tremaining: 1.93s\n",
      "111:\tlearn: 28.6518328\ttotal: 243ms\tremaining: 1.93s\n",
      "112:\tlearn: 28.6433623\ttotal: 245ms\tremaining: 1.92s\n",
      "113:\tlearn: 28.6368301\ttotal: 247ms\tremaining: 1.92s\n",
      "114:\tlearn: 28.6293581\ttotal: 249ms\tremaining: 1.92s\n",
      "115:\tlearn: 28.6226436\ttotal: 251ms\tremaining: 1.91s\n",
      "116:\tlearn: 28.6163622\ttotal: 253ms\tremaining: 1.91s\n",
      "117:\tlearn: 28.6098602\ttotal: 255ms\tremaining: 1.9s\n",
      "118:\tlearn: 28.6036716\ttotal: 257ms\tremaining: 1.9s\n",
      "119:\tlearn: 28.5972759\ttotal: 258ms\tremaining: 1.9s\n",
      "120:\tlearn: 28.5898315\ttotal: 260ms\tremaining: 1.89s\n",
      "121:\tlearn: 28.5831939\ttotal: 262ms\tremaining: 1.89s\n",
      "122:\tlearn: 28.5766248\ttotal: 264ms\tremaining: 1.88s\n",
      "123:\tlearn: 28.5709291\ttotal: 266ms\tremaining: 1.88s\n",
      "124:\tlearn: 28.5658254\ttotal: 267ms\tremaining: 1.87s\n",
      "125:\tlearn: 28.5603318\ttotal: 269ms\tremaining: 1.87s\n",
      "126:\tlearn: 28.5552939\ttotal: 271ms\tremaining: 1.86s\n",
      "127:\tlearn: 28.5524991\ttotal: 273ms\tremaining: 1.86s\n",
      "128:\tlearn: 28.5490076\ttotal: 274ms\tremaining: 1.85s\n",
      "129:\tlearn: 28.5437356\ttotal: 276ms\tremaining: 1.85s\n",
      "130:\tlearn: 28.5386785\ttotal: 278ms\tremaining: 1.84s\n",
      "131:\tlearn: 28.5339096\ttotal: 280ms\tremaining: 1.84s\n",
      "132:\tlearn: 28.5288101\ttotal: 282ms\tremaining: 1.84s\n",
      "133:\tlearn: 28.5193975\ttotal: 284ms\tremaining: 1.83s\n",
      "134:\tlearn: 28.5126559\ttotal: 285ms\tremaining: 1.83s\n",
      "135:\tlearn: 28.5074443\ttotal: 288ms\tremaining: 1.83s\n",
      "136:\tlearn: 28.5031415\ttotal: 290ms\tremaining: 1.82s\n",
      "137:\tlearn: 28.4956326\ttotal: 292ms\tremaining: 1.82s\n",
      "138:\tlearn: 28.4914310\ttotal: 294ms\tremaining: 1.82s\n",
      "139:\tlearn: 28.4852494\ttotal: 296ms\tremaining: 1.81s\n",
      "140:\tlearn: 28.4817525\ttotal: 297ms\tremaining: 1.81s\n",
      "141:\tlearn: 28.4782786\ttotal: 299ms\tremaining: 1.81s\n",
      "142:\tlearn: 28.4730562\ttotal: 301ms\tremaining: 1.8s\n",
      "143:\tlearn: 28.4669346\ttotal: 303ms\tremaining: 1.8s\n",
      "144:\tlearn: 28.4629195\ttotal: 305ms\tremaining: 1.8s\n",
      "145:\tlearn: 28.4560370\ttotal: 307ms\tremaining: 1.79s\n",
      "146:\tlearn: 28.4517147\ttotal: 309ms\tremaining: 1.79s\n",
      "147:\tlearn: 28.4481757\ttotal: 310ms\tremaining: 1.79s\n",
      "148:\tlearn: 28.4423742\ttotal: 312ms\tremaining: 1.78s\n",
      "149:\tlearn: 28.4379453\ttotal: 314ms\tremaining: 1.78s\n",
      "150:\tlearn: 28.4329334\ttotal: 316ms\tremaining: 1.78s\n",
      "151:\tlearn: 28.4245969\ttotal: 318ms\tremaining: 1.77s\n",
      "152:\tlearn: 28.4177205\ttotal: 320ms\tremaining: 1.77s\n",
      "153:\tlearn: 28.4138944\ttotal: 322ms\tremaining: 1.77s\n",
      "154:\tlearn: 28.4106674\ttotal: 324ms\tremaining: 1.76s\n",
      "155:\tlearn: 28.4049881\ttotal: 326ms\tremaining: 1.76s\n",
      "156:\tlearn: 28.4011683\ttotal: 327ms\tremaining: 1.76s\n",
      "157:\tlearn: 28.3975129\ttotal: 329ms\tremaining: 1.75s\n",
      "158:\tlearn: 28.3892424\ttotal: 331ms\tremaining: 1.75s\n",
      "159:\tlearn: 28.3837696\ttotal: 333ms\tremaining: 1.75s\n",
      "160:\tlearn: 28.3808156\ttotal: 335ms\tremaining: 1.74s\n",
      "161:\tlearn: 28.3781014\ttotal: 337ms\tremaining: 1.74s\n",
      "162:\tlearn: 28.3743551\ttotal: 338ms\tremaining: 1.74s\n",
      "163:\tlearn: 28.3727266\ttotal: 340ms\tremaining: 1.74s\n",
      "164:\tlearn: 28.3679701\ttotal: 342ms\tremaining: 1.73s\n",
      "165:\tlearn: 28.3654411\ttotal: 344ms\tremaining: 1.73s\n",
      "166:\tlearn: 28.3629430\ttotal: 346ms\tremaining: 1.72s\n",
      "167:\tlearn: 28.3604987\ttotal: 347ms\tremaining: 1.72s\n",
      "168:\tlearn: 28.3541029\ttotal: 349ms\tremaining: 1.72s\n",
      "169:\tlearn: 28.3495466\ttotal: 351ms\tremaining: 1.72s\n",
      "170:\tlearn: 28.3421655\ttotal: 353ms\tremaining: 1.71s\n",
      "171:\tlearn: 28.3395102\ttotal: 355ms\tremaining: 1.71s\n",
      "172:\tlearn: 28.3336430\ttotal: 357ms\tremaining: 1.71s\n",
      "173:\tlearn: 28.3299008\ttotal: 358ms\tremaining: 1.7s\n",
      "174:\tlearn: 28.3247068\ttotal: 360ms\tremaining: 1.7s\n",
      "175:\tlearn: 28.3197286\ttotal: 362ms\tremaining: 1.7s\n",
      "176:\tlearn: 28.3175824\ttotal: 364ms\tremaining: 1.69s\n",
      "177:\tlearn: 28.3155395\ttotal: 365ms\tremaining: 1.69s\n",
      "178:\tlearn: 28.3093180\ttotal: 367ms\tremaining: 1.68s\n",
      "179:\tlearn: 28.3010593\ttotal: 369ms\tremaining: 1.68s\n",
      "180:\tlearn: 28.2928945\ttotal: 371ms\tremaining: 1.68s\n",
      "181:\tlearn: 28.2909624\ttotal: 373ms\tremaining: 1.68s\n",
      "182:\tlearn: 28.2890840\ttotal: 374ms\tremaining: 1.67s\n",
      "183:\tlearn: 28.2856656\ttotal: 376ms\tremaining: 1.67s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184:\tlearn: 28.2814883\ttotal: 379ms\tremaining: 1.67s\n",
      "185:\tlearn: 28.2796514\ttotal: 382ms\tremaining: 1.67s\n",
      "186:\tlearn: 28.2766578\ttotal: 384ms\tremaining: 1.67s\n",
      "187:\tlearn: 28.2748825\ttotal: 386ms\tremaining: 1.67s\n",
      "188:\tlearn: 28.2723997\ttotal: 388ms\tremaining: 1.67s\n",
      "189:\tlearn: 28.2706178\ttotal: 391ms\tremaining: 1.67s\n",
      "190:\tlearn: 28.2645494\ttotal: 394ms\tremaining: 1.67s\n",
      "191:\tlearn: 28.2628223\ttotal: 396ms\tremaining: 1.67s\n",
      "192:\tlearn: 28.2609179\ttotal: 399ms\tremaining: 1.67s\n",
      "193:\tlearn: 28.2592659\ttotal: 401ms\tremaining: 1.67s\n",
      "194:\tlearn: 28.2577534\ttotal: 403ms\tremaining: 1.66s\n",
      "195:\tlearn: 28.2562528\ttotal: 405ms\tremaining: 1.66s\n",
      "196:\tlearn: 28.2547201\ttotal: 408ms\tremaining: 1.66s\n",
      "197:\tlearn: 28.2490906\ttotal: 410ms\tremaining: 1.66s\n",
      "198:\tlearn: 28.2475560\ttotal: 414ms\tremaining: 1.66s\n",
      "199:\tlearn: 28.2462003\ttotal: 416ms\tremaining: 1.66s\n",
      "200:\tlearn: 28.2447623\ttotal: 419ms\tremaining: 1.66s\n",
      "201:\tlearn: 28.2434735\ttotal: 420ms\tremaining: 1.66s\n",
      "202:\tlearn: 28.2421835\ttotal: 422ms\tremaining: 1.66s\n",
      "203:\tlearn: 28.2409619\ttotal: 424ms\tremaining: 1.65s\n",
      "204:\tlearn: 28.2392379\ttotal: 426ms\tremaining: 1.65s\n",
      "205:\tlearn: 28.2372536\ttotal: 428ms\tremaining: 1.65s\n",
      "206:\tlearn: 28.2285996\ttotal: 430ms\tremaining: 1.65s\n",
      "207:\tlearn: 28.2274659\ttotal: 432ms\tremaining: 1.64s\n",
      "208:\tlearn: 28.2262279\ttotal: 433ms\tremaining: 1.64s\n",
      "209:\tlearn: 28.2252589\ttotal: 435ms\tremaining: 1.64s\n",
      "210:\tlearn: 28.2240434\ttotal: 437ms\tremaining: 1.63s\n",
      "211:\tlearn: 28.2176358\ttotal: 439ms\tremaining: 1.63s\n",
      "212:\tlearn: 28.2169196\ttotal: 440ms\tremaining: 1.63s\n",
      "213:\tlearn: 28.2158708\ttotal: 442ms\tremaining: 1.62s\n",
      "214:\tlearn: 28.2097688\ttotal: 444ms\tremaining: 1.62s\n",
      "215:\tlearn: 28.2088395\ttotal: 446ms\tremaining: 1.62s\n",
      "216:\tlearn: 28.2034663\ttotal: 447ms\tremaining: 1.61s\n",
      "217:\tlearn: 28.2021974\ttotal: 449ms\tremaining: 1.61s\n",
      "218:\tlearn: 28.2012593\ttotal: 451ms\tremaining: 1.61s\n",
      "219:\tlearn: 28.2003251\ttotal: 453ms\tremaining: 1.6s\n",
      "220:\tlearn: 28.1969646\ttotal: 455ms\tremaining: 1.6s\n",
      "221:\tlearn: 28.1914407\ttotal: 457ms\tremaining: 1.6s\n",
      "222:\tlearn: 28.1908540\ttotal: 459ms\tremaining: 1.6s\n",
      "223:\tlearn: 28.1838988\ttotal: 461ms\tremaining: 1.6s\n",
      "224:\tlearn: 28.1835839\ttotal: 462ms\tremaining: 1.59s\n",
      "225:\tlearn: 28.1800473\ttotal: 464ms\tremaining: 1.59s\n",
      "226:\tlearn: 28.1791422\ttotal: 466ms\tremaining: 1.59s\n",
      "227:\tlearn: 28.1743343\ttotal: 468ms\tremaining: 1.58s\n",
      "228:\tlearn: 28.1688106\ttotal: 470ms\tremaining: 1.58s\n",
      "229:\tlearn: 28.1628453\ttotal: 472ms\tremaining: 1.58s\n",
      "230:\tlearn: 28.1620657\ttotal: 474ms\tremaining: 1.58s\n",
      "231:\tlearn: 28.1611575\ttotal: 475ms\tremaining: 1.57s\n",
      "232:\tlearn: 28.1604281\ttotal: 477ms\tremaining: 1.57s\n",
      "233:\tlearn: 28.1556197\ttotal: 479ms\tremaining: 1.57s\n",
      "234:\tlearn: 28.1549071\ttotal: 481ms\tremaining: 1.56s\n",
      "235:\tlearn: 28.1490351\ttotal: 483ms\tremaining: 1.56s\n",
      "236:\tlearn: 28.1485453\ttotal: 485ms\tremaining: 1.56s\n",
      "237:\tlearn: 28.1432481\ttotal: 487ms\tremaining: 1.56s\n",
      "238:\tlearn: 28.1428437\ttotal: 488ms\tremaining: 1.55s\n",
      "239:\tlearn: 28.1421480\ttotal: 490ms\tremaining: 1.55s\n",
      "240:\tlearn: 28.1414724\ttotal: 492ms\tremaining: 1.55s\n",
      "241:\tlearn: 28.1379012\ttotal: 494ms\tremaining: 1.55s\n",
      "242:\tlearn: 28.1372538\ttotal: 495ms\tremaining: 1.54s\n",
      "243:\tlearn: 28.1366266\ttotal: 497ms\tremaining: 1.54s\n",
      "244:\tlearn: 28.1360000\ttotal: 499ms\tremaining: 1.54s\n",
      "245:\tlearn: 28.1353870\ttotal: 501ms\tremaining: 1.54s\n",
      "246:\tlearn: 28.1297933\ttotal: 503ms\tremaining: 1.53s\n",
      "247:\tlearn: 28.1294235\ttotal: 505ms\tremaining: 1.53s\n",
      "248:\tlearn: 28.1273487\ttotal: 507ms\tremaining: 1.53s\n",
      "249:\tlearn: 28.1216220\ttotal: 510ms\tremaining: 1.53s\n",
      "250:\tlearn: 28.1202302\ttotal: 512ms\tremaining: 1.53s\n",
      "251:\tlearn: 28.1189838\ttotal: 514ms\tremaining: 1.52s\n",
      "252:\tlearn: 28.1121087\ttotal: 516ms\tremaining: 1.52s\n",
      "253:\tlearn: 28.1072763\ttotal: 518ms\tremaining: 1.52s\n",
      "254:\tlearn: 28.1022050\ttotal: 520ms\tremaining: 1.52s\n",
      "255:\tlearn: 28.1016632\ttotal: 522ms\tremaining: 1.52s\n",
      "256:\tlearn: 28.0981248\ttotal: 524ms\tremaining: 1.51s\n",
      "257:\tlearn: 28.0942862\ttotal: 526ms\tremaining: 1.51s\n",
      "258:\tlearn: 28.0937278\ttotal: 527ms\tremaining: 1.51s\n",
      "259:\tlearn: 28.0904242\ttotal: 529ms\tremaining: 1.51s\n",
      "260:\tlearn: 28.0871369\ttotal: 531ms\tremaining: 1.5s\n",
      "261:\tlearn: 28.0866249\ttotal: 533ms\tremaining: 1.5s\n",
      "262:\tlearn: 28.0818317\ttotal: 535ms\tremaining: 1.5s\n",
      "263:\tlearn: 28.0815083\ttotal: 536ms\tremaining: 1.5s\n",
      "264:\tlearn: 28.0810032\ttotal: 538ms\tremaining: 1.49s\n",
      "265:\tlearn: 28.0768506\ttotal: 540ms\tremaining: 1.49s\n",
      "266:\tlearn: 28.0734522\ttotal: 541ms\tremaining: 1.49s\n",
      "267:\tlearn: 28.0731862\ttotal: 543ms\tremaining: 1.48s\n",
      "268:\tlearn: 28.0692294\ttotal: 545ms\tremaining: 1.48s\n",
      "269:\tlearn: 28.0687159\ttotal: 547ms\tremaining: 1.48s\n",
      "270:\tlearn: 28.0656193\ttotal: 549ms\tremaining: 1.48s\n",
      "271:\tlearn: 28.0651756\ttotal: 550ms\tremaining: 1.47s\n",
      "272:\tlearn: 28.0632376\ttotal: 552ms\tremaining: 1.47s\n",
      "273:\tlearn: 28.0590796\ttotal: 554ms\tremaining: 1.47s\n",
      "274:\tlearn: 28.0547391\ttotal: 556ms\tremaining: 1.47s\n",
      "275:\tlearn: 28.0526024\ttotal: 558ms\tremaining: 1.46s\n",
      "276:\tlearn: 28.0486021\ttotal: 559ms\tremaining: 1.46s\n",
      "277:\tlearn: 28.0449590\ttotal: 561ms\tremaining: 1.46s\n",
      "278:\tlearn: 28.0437487\ttotal: 563ms\tremaining: 1.45s\n",
      "279:\tlearn: 28.0404045\ttotal: 565ms\tremaining: 1.45s\n",
      "280:\tlearn: 28.0400328\ttotal: 567ms\tremaining: 1.45s\n",
      "281:\tlearn: 28.0354799\ttotal: 568ms\tremaining: 1.45s\n",
      "282:\tlearn: 28.0320993\ttotal: 570ms\tremaining: 1.45s\n",
      "283:\tlearn: 28.0293194\ttotal: 572ms\tremaining: 1.44s\n",
      "284:\tlearn: 28.0251739\ttotal: 575ms\tremaining: 1.44s\n",
      "285:\tlearn: 28.0234942\ttotal: 578ms\tremaining: 1.44s\n",
      "286:\tlearn: 28.0206820\ttotal: 580ms\tremaining: 1.44s\n",
      "287:\tlearn: 28.0202641\ttotal: 582ms\tremaining: 1.44s\n",
      "288:\tlearn: 28.0188136\ttotal: 585ms\tremaining: 1.44s\n",
      "289:\tlearn: 28.0149155\ttotal: 588ms\tremaining: 1.44s\n",
      "290:\tlearn: 28.0137558\ttotal: 590ms\tremaining: 1.44s\n",
      "291:\tlearn: 28.0111648\ttotal: 593ms\tremaining: 1.44s\n",
      "292:\tlearn: 28.0039269\ttotal: 595ms\tremaining: 1.44s\n",
      "293:\tlearn: 28.0002842\ttotal: 597ms\tremaining: 1.43s\n",
      "294:\tlearn: 27.9978118\ttotal: 599ms\tremaining: 1.43s\n",
      "295:\tlearn: 27.9949773\ttotal: 601ms\tremaining: 1.43s\n",
      "296:\tlearn: 27.9946492\ttotal: 603ms\tremaining: 1.43s\n",
      "297:\tlearn: 27.9934974\ttotal: 605ms\tremaining: 1.42s\n",
      "298:\tlearn: 27.9909491\ttotal: 607ms\tremaining: 1.42s\n",
      "299:\tlearn: 27.9882410\ttotal: 609ms\tremaining: 1.42s\n",
      "300:\tlearn: 27.9852320\ttotal: 611ms\tremaining: 1.42s\n",
      "301:\tlearn: 27.9825768\ttotal: 613ms\tremaining: 1.42s\n",
      "302:\tlearn: 27.9801556\ttotal: 615ms\tremaining: 1.41s\n",
      "303:\tlearn: 27.9766214\ttotal: 617ms\tremaining: 1.41s\n",
      "304:\tlearn: 27.9739651\ttotal: 619ms\tremaining: 1.41s\n",
      "305:\tlearn: 27.9702644\ttotal: 621ms\tremaining: 1.41s\n",
      "306:\tlearn: 27.9692506\ttotal: 623ms\tremaining: 1.41s\n",
      "307:\tlearn: 27.9679801\ttotal: 625ms\tremaining: 1.4s\n",
      "308:\tlearn: 27.9644508\ttotal: 627ms\tremaining: 1.4s\n",
      "309:\tlearn: 27.9613182\ttotal: 628ms\tremaining: 1.4s\n",
      "310:\tlearn: 27.9570870\ttotal: 631ms\tremaining: 1.4s\n",
      "311:\tlearn: 27.9545429\ttotal: 633ms\tremaining: 1.4s\n",
      "312:\tlearn: 27.9541420\ttotal: 635ms\tremaining: 1.39s\n",
      "313:\tlearn: 27.9538371\ttotal: 636ms\tremaining: 1.39s\n",
      "314:\tlearn: 27.9499905\ttotal: 638ms\tremaining: 1.39s\n",
      "315:\tlearn: 27.9496954\ttotal: 640ms\tremaining: 1.39s\n",
      "316:\tlearn: 27.9482317\ttotal: 642ms\tremaining: 1.38s\n",
      "317:\tlearn: 27.9473741\ttotal: 644ms\tremaining: 1.38s\n",
      "318:\tlearn: 27.9464233\ttotal: 646ms\tremaining: 1.38s\n",
      "319:\tlearn: 27.9431553\ttotal: 648ms\tremaining: 1.38s\n",
      "320:\tlearn: 27.9422268\ttotal: 650ms\tremaining: 1.37s\n",
      "321:\tlearn: 27.9398572\ttotal: 651ms\tremaining: 1.37s\n",
      "322:\tlearn: 27.9375538\ttotal: 653ms\tremaining: 1.37s\n",
      "323:\tlearn: 27.9327530\ttotal: 655ms\tremaining: 1.37s\n",
      "324:\tlearn: 27.9291475\ttotal: 657ms\tremaining: 1.36s\n",
      "325:\tlearn: 27.9277226\ttotal: 659ms\tremaining: 1.36s\n",
      "326:\tlearn: 27.9264919\ttotal: 661ms\tremaining: 1.36s\n",
      "327:\tlearn: 27.9254183\ttotal: 663ms\tremaining: 1.36s\n",
      "328:\tlearn: 27.9245821\ttotal: 665ms\tremaining: 1.36s\n",
      "329:\tlearn: 27.9221823\ttotal: 667ms\tremaining: 1.35s\n",
      "330:\tlearn: 27.9188569\ttotal: 669ms\tremaining: 1.35s\n",
      "331:\tlearn: 27.9167567\ttotal: 671ms\tremaining: 1.35s\n",
      "332:\tlearn: 27.9156535\ttotal: 673ms\tremaining: 1.35s\n",
      "333:\tlearn: 27.9138021\ttotal: 674ms\tremaining: 1.34s\n",
      "334:\tlearn: 27.9110386\ttotal: 676ms\tremaining: 1.34s\n",
      "335:\tlearn: 27.9101785\ttotal: 679ms\tremaining: 1.34s\n",
      "336:\tlearn: 27.9068063\ttotal: 681ms\tremaining: 1.34s\n",
      "337:\tlearn: 27.9036820\ttotal: 683ms\tremaining: 1.34s\n",
      "338:\tlearn: 27.9014108\ttotal: 685ms\tremaining: 1.33s\n",
      "339:\tlearn: 27.8982349\ttotal: 686ms\tremaining: 1.33s\n",
      "340:\tlearn: 27.8961256\ttotal: 688ms\tremaining: 1.33s\n",
      "341:\tlearn: 27.8959444\ttotal: 690ms\tremaining: 1.33s\n",
      "342:\tlearn: 27.8931988\ttotal: 692ms\tremaining: 1.32s\n",
      "343:\tlearn: 27.8910708\ttotal: 694ms\tremaining: 1.32s\n",
      "344:\tlearn: 27.8907623\ttotal: 696ms\tremaining: 1.32s\n",
      "345:\tlearn: 27.8897879\ttotal: 698ms\tremaining: 1.32s\n",
      "346:\tlearn: 27.8878926\ttotal: 700ms\tremaining: 1.32s\n",
      "347:\tlearn: 27.8856159\ttotal: 702ms\tremaining: 1.31s\n",
      "348:\tlearn: 27.8846890\ttotal: 704ms\tremaining: 1.31s\n",
      "349:\tlearn: 27.8828404\ttotal: 705ms\tremaining: 1.31s\n",
      "350:\tlearn: 27.8810193\ttotal: 707ms\tremaining: 1.31s\n",
      "351:\tlearn: 27.8788276\ttotal: 709ms\tremaining: 1.3s\n",
      "352:\tlearn: 27.8786139\ttotal: 711ms\tremaining: 1.3s\n",
      "353:\tlearn: 27.8762200\ttotal: 713ms\tremaining: 1.3s\n",
      "354:\tlearn: 27.8734529\ttotal: 715ms\tremaining: 1.3s\n",
      "355:\tlearn: 27.8733001\ttotal: 716ms\tremaining: 1.3s\n",
      "356:\tlearn: 27.8688211\ttotal: 718ms\tremaining: 1.29s\n",
      "357:\tlearn: 27.8658204\ttotal: 720ms\tremaining: 1.29s\n",
      "358:\tlearn: 27.8637013\ttotal: 722ms\tremaining: 1.29s\n",
      "359:\tlearn: 27.8619691\ttotal: 724ms\tremaining: 1.29s\n",
      "360:\tlearn: 27.8601967\ttotal: 725ms\tremaining: 1.28s\n",
      "361:\tlearn: 27.8571976\ttotal: 727ms\tremaining: 1.28s\n",
      "362:\tlearn: 27.8555551\ttotal: 729ms\tremaining: 1.28s\n",
      "363:\tlearn: 27.8526495\ttotal: 731ms\tremaining: 1.28s\n",
      "364:\tlearn: 27.8496002\ttotal: 733ms\tremaining: 1.27s\n",
      "365:\tlearn: 27.8493196\ttotal: 734ms\tremaining: 1.27s\n",
      "366:\tlearn: 27.8490933\ttotal: 736ms\tremaining: 1.27s\n",
      "367:\tlearn: 27.8464553\ttotal: 738ms\tremaining: 1.27s\n",
      "368:\tlearn: 27.8447324\ttotal: 740ms\tremaining: 1.26s\n",
      "369:\tlearn: 27.8410985\ttotal: 741ms\tremaining: 1.26s\n",
      "370:\tlearn: 27.8384689\ttotal: 743ms\tremaining: 1.26s\n",
      "371:\tlearn: 27.8377329\ttotal: 745ms\tremaining: 1.26s\n",
      "372:\tlearn: 27.8348428\ttotal: 747ms\tremaining: 1.25s\n",
      "373:\tlearn: 27.8332157\ttotal: 749ms\tremaining: 1.25s\n",
      "374:\tlearn: 27.8305080\ttotal: 750ms\tremaining: 1.25s\n",
      "375:\tlearn: 27.8275741\ttotal: 752ms\tremaining: 1.25s\n",
      "376:\tlearn: 27.8269177\ttotal: 754ms\tremaining: 1.25s\n",
      "377:\tlearn: 27.8253891\ttotal: 756ms\tremaining: 1.24s\n",
      "378:\tlearn: 27.8233836\ttotal: 758ms\tremaining: 1.24s\n",
      "379:\tlearn: 27.8220835\ttotal: 760ms\tremaining: 1.24s\n",
      "380:\tlearn: 27.8195109\ttotal: 761ms\tremaining: 1.24s\n",
      "381:\tlearn: 27.8177440\ttotal: 763ms\tremaining: 1.23s\n",
      "382:\tlearn: 27.8148748\ttotal: 765ms\tremaining: 1.23s\n",
      "383:\tlearn: 27.8122162\ttotal: 767ms\tremaining: 1.23s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384:\tlearn: 27.8106817\ttotal: 770ms\tremaining: 1.23s\n",
      "385:\tlearn: 27.8079678\ttotal: 773ms\tremaining: 1.23s\n",
      "386:\tlearn: 27.8053711\ttotal: 775ms\tremaining: 1.23s\n",
      "387:\tlearn: 27.8023957\ttotal: 777ms\tremaining: 1.23s\n",
      "388:\tlearn: 27.8003731\ttotal: 779ms\tremaining: 1.22s\n",
      "389:\tlearn: 27.7999372\ttotal: 782ms\tremaining: 1.22s\n",
      "390:\tlearn: 27.7952048\ttotal: 784ms\tremaining: 1.22s\n",
      "391:\tlearn: 27.7934048\ttotal: 790ms\tremaining: 1.23s\n",
      "392:\tlearn: 27.7930923\ttotal: 794ms\tremaining: 1.23s\n",
      "393:\tlearn: 27.7917428\ttotal: 797ms\tremaining: 1.23s\n",
      "394:\tlearn: 27.7893028\ttotal: 800ms\tremaining: 1.23s\n",
      "395:\tlearn: 27.7889058\ttotal: 804ms\tremaining: 1.23s\n",
      "396:\tlearn: 27.7829531\ttotal: 807ms\tremaining: 1.23s\n",
      "397:\tlearn: 27.7814902\ttotal: 809ms\tremaining: 1.22s\n",
      "398:\tlearn: 27.7813063\ttotal: 812ms\tremaining: 1.22s\n",
      "399:\tlearn: 27.7787141\ttotal: 814ms\tremaining: 1.22s\n",
      "400:\tlearn: 27.7772745\ttotal: 817ms\tremaining: 1.22s\n",
      "401:\tlearn: 27.7764578\ttotal: 819ms\tremaining: 1.22s\n",
      "402:\tlearn: 27.7749077\ttotal: 821ms\tremaining: 1.22s\n",
      "403:\tlearn: 27.7695616\ttotal: 823ms\tremaining: 1.21s\n",
      "404:\tlearn: 27.7660754\ttotal: 825ms\tremaining: 1.21s\n",
      "405:\tlearn: 27.7635960\ttotal: 828ms\tremaining: 1.21s\n",
      "406:\tlearn: 27.7610584\ttotal: 831ms\tremaining: 1.21s\n",
      "407:\tlearn: 27.7609098\ttotal: 833ms\tremaining: 1.21s\n",
      "408:\tlearn: 27.7590244\ttotal: 835ms\tremaining: 1.21s\n",
      "409:\tlearn: 27.7568217\ttotal: 838ms\tremaining: 1.21s\n",
      "410:\tlearn: 27.7563319\ttotal: 841ms\tremaining: 1.21s\n",
      "411:\tlearn: 27.7548888\ttotal: 844ms\tremaining: 1.2s\n",
      "412:\tlearn: 27.7532130\ttotal: 847ms\tremaining: 1.2s\n",
      "413:\tlearn: 27.7529731\ttotal: 850ms\tremaining: 1.2s\n",
      "414:\tlearn: 27.7515673\ttotal: 853ms\tremaining: 1.2s\n",
      "415:\tlearn: 27.7488416\ttotal: 856ms\tremaining: 1.2s\n",
      "416:\tlearn: 27.7464656\ttotal: 858ms\tremaining: 1.2s\n",
      "417:\tlearn: 27.7450126\ttotal: 861ms\tremaining: 1.2s\n",
      "418:\tlearn: 27.7442863\ttotal: 864ms\tremaining: 1.2s\n",
      "419:\tlearn: 27.7429641\ttotal: 866ms\tremaining: 1.2s\n",
      "420:\tlearn: 27.7417450\ttotal: 869ms\tremaining: 1.19s\n",
      "421:\tlearn: 27.7403998\ttotal: 871ms\tremaining: 1.19s\n",
      "422:\tlearn: 27.7363253\ttotal: 873ms\tremaining: 1.19s\n",
      "423:\tlearn: 27.7332005\ttotal: 875ms\tremaining: 1.19s\n",
      "424:\tlearn: 27.7318201\ttotal: 877ms\tremaining: 1.19s\n",
      "425:\tlearn: 27.7316629\ttotal: 879ms\tremaining: 1.18s\n",
      "426:\tlearn: 27.7304322\ttotal: 881ms\tremaining: 1.18s\n",
      "427:\tlearn: 27.7288563\ttotal: 884ms\tremaining: 1.18s\n",
      "428:\tlearn: 27.7264381\ttotal: 886ms\tremaining: 1.18s\n",
      "429:\tlearn: 27.7254676\ttotal: 889ms\tremaining: 1.18s\n",
      "430:\tlearn: 27.7249304\ttotal: 892ms\tremaining: 1.18s\n",
      "431:\tlearn: 27.7224072\ttotal: 894ms\tremaining: 1.18s\n",
      "432:\tlearn: 27.7216714\ttotal: 896ms\tremaining: 1.17s\n",
      "433:\tlearn: 27.7165736\ttotal: 899ms\tremaining: 1.17s\n",
      "434:\tlearn: 27.7163169\ttotal: 901ms\tremaining: 1.17s\n",
      "435:\tlearn: 27.7161152\ttotal: 903ms\tremaining: 1.17s\n",
      "436:\tlearn: 27.7134211\ttotal: 906ms\tremaining: 1.17s\n",
      "437:\tlearn: 27.7111281\ttotal: 908ms\tremaining: 1.17s\n",
      "438:\tlearn: 27.7099083\ttotal: 910ms\tremaining: 1.16s\n",
      "439:\tlearn: 27.7056279\ttotal: 914ms\tremaining: 1.16s\n",
      "440:\tlearn: 27.7040654\ttotal: 916ms\tremaining: 1.16s\n",
      "441:\tlearn: 27.7028259\ttotal: 920ms\tremaining: 1.16s\n",
      "442:\tlearn: 27.7004857\ttotal: 922ms\tremaining: 1.16s\n",
      "443:\tlearn: 27.6984571\ttotal: 925ms\tremaining: 1.16s\n",
      "444:\tlearn: 27.6971642\ttotal: 928ms\tremaining: 1.16s\n",
      "445:\tlearn: 27.6922292\ttotal: 930ms\tremaining: 1.16s\n",
      "446:\tlearn: 27.6902854\ttotal: 933ms\tremaining: 1.15s\n",
      "447:\tlearn: 27.6856427\ttotal: 935ms\tremaining: 1.15s\n",
      "448:\tlearn: 27.6830405\ttotal: 937ms\tremaining: 1.15s\n",
      "449:\tlearn: 27.6817509\ttotal: 940ms\tremaining: 1.15s\n",
      "450:\tlearn: 27.6800557\ttotal: 942ms\tremaining: 1.15s\n",
      "451:\tlearn: 27.6794859\ttotal: 944ms\tremaining: 1.14s\n",
      "452:\tlearn: 27.6784651\ttotal: 945ms\tremaining: 1.14s\n",
      "453:\tlearn: 27.6773284\ttotal: 947ms\tremaining: 1.14s\n",
      "454:\tlearn: 27.6761708\ttotal: 950ms\tremaining: 1.14s\n",
      "455:\tlearn: 27.6742874\ttotal: 952ms\tremaining: 1.14s\n",
      "456:\tlearn: 27.6729450\ttotal: 954ms\tremaining: 1.13s\n",
      "457:\tlearn: 27.6715345\ttotal: 956ms\tremaining: 1.13s\n",
      "458:\tlearn: 27.6705119\ttotal: 958ms\tremaining: 1.13s\n",
      "459:\tlearn: 27.6700471\ttotal: 960ms\tremaining: 1.13s\n",
      "460:\tlearn: 27.6674928\ttotal: 962ms\tremaining: 1.12s\n",
      "461:\tlearn: 27.6664158\ttotal: 964ms\tremaining: 1.12s\n",
      "462:\tlearn: 27.6653766\ttotal: 967ms\tremaining: 1.12s\n",
      "463:\tlearn: 27.6642836\ttotal: 969ms\tremaining: 1.12s\n",
      "464:\tlearn: 27.6586261\ttotal: 971ms\tremaining: 1.12s\n",
      "465:\tlearn: 27.6549573\ttotal: 974ms\tremaining: 1.11s\n",
      "466:\tlearn: 27.6528102\ttotal: 977ms\tremaining: 1.11s\n",
      "467:\tlearn: 27.6509609\ttotal: 979ms\tremaining: 1.11s\n",
      "468:\tlearn: 27.6484186\ttotal: 981ms\tremaining: 1.11s\n",
      "469:\tlearn: 27.6482061\ttotal: 983ms\tremaining: 1.11s\n",
      "470:\tlearn: 27.6471250\ttotal: 985ms\tremaining: 1.11s\n",
      "471:\tlearn: 27.6462690\ttotal: 987ms\tremaining: 1.1s\n",
      "472:\tlearn: 27.6456082\ttotal: 989ms\tremaining: 1.1s\n",
      "473:\tlearn: 27.6447292\ttotal: 991ms\tremaining: 1.1s\n",
      "474:\tlearn: 27.6424408\ttotal: 993ms\tremaining: 1.1s\n",
      "475:\tlearn: 27.6408203\ttotal: 995ms\tremaining: 1.09s\n",
      "476:\tlearn: 27.6391728\ttotal: 997ms\tremaining: 1.09s\n",
      "477:\tlearn: 27.6382234\ttotal: 999ms\tremaining: 1.09s\n",
      "478:\tlearn: 27.6379824\ttotal: 1s\tremaining: 1.09s\n",
      "479:\tlearn: 27.6367079\ttotal: 1s\tremaining: 1.08s\n",
      "480:\tlearn: 27.6357851\ttotal: 1s\tremaining: 1.08s\n",
      "481:\tlearn: 27.6348571\ttotal: 1.01s\tremaining: 1.08s\n",
      "482:\tlearn: 27.6326181\ttotal: 1.01s\tremaining: 1.08s\n",
      "483:\tlearn: 27.6316892\ttotal: 1.01s\tremaining: 1.08s\n",
      "484:\tlearn: 27.6311044\ttotal: 1.01s\tremaining: 1.08s\n",
      "485:\tlearn: 27.6289740\ttotal: 1.02s\tremaining: 1.07s\n",
      "486:\tlearn: 27.6274644\ttotal: 1.02s\tremaining: 1.07s\n",
      "487:\tlearn: 27.6263470\ttotal: 1.02s\tremaining: 1.07s\n",
      "488:\tlearn: 27.6255915\ttotal: 1.02s\tremaining: 1.07s\n",
      "489:\tlearn: 27.6246933\ttotal: 1.03s\tremaining: 1.07s\n",
      "490:\tlearn: 27.6226964\ttotal: 1.03s\tremaining: 1.07s\n",
      "491:\tlearn: 27.6219493\ttotal: 1.03s\tremaining: 1.06s\n",
      "492:\tlearn: 27.6218673\ttotal: 1.03s\tremaining: 1.06s\n",
      "493:\tlearn: 27.6194286\ttotal: 1.04s\tremaining: 1.06s\n",
      "494:\tlearn: 27.6171672\ttotal: 1.04s\tremaining: 1.06s\n",
      "495:\tlearn: 27.6157264\ttotal: 1.04s\tremaining: 1.06s\n",
      "496:\tlearn: 27.6155598\ttotal: 1.04s\tremaining: 1.06s\n",
      "497:\tlearn: 27.6146757\ttotal: 1.05s\tremaining: 1.05s\n",
      "498:\tlearn: 27.6138349\ttotal: 1.05s\tremaining: 1.05s\n",
      "499:\tlearn: 27.6083751\ttotal: 1.05s\tremaining: 1.05s\n",
      "500:\tlearn: 27.6057127\ttotal: 1.05s\tremaining: 1.05s\n",
      "501:\tlearn: 27.6044590\ttotal: 1.06s\tremaining: 1.05s\n",
      "502:\tlearn: 27.6024397\ttotal: 1.06s\tremaining: 1.05s\n",
      "503:\tlearn: 27.6015209\ttotal: 1.06s\tremaining: 1.04s\n",
      "504:\tlearn: 27.6014069\ttotal: 1.06s\tremaining: 1.04s\n",
      "505:\tlearn: 27.6006107\ttotal: 1.07s\tremaining: 1.04s\n",
      "506:\tlearn: 27.5999466\ttotal: 1.07s\tremaining: 1.04s\n",
      "507:\tlearn: 27.5979331\ttotal: 1.07s\tremaining: 1.04s\n",
      "508:\tlearn: 27.5965545\ttotal: 1.08s\tremaining: 1.04s\n",
      "509:\tlearn: 27.5934463\ttotal: 1.08s\tremaining: 1.04s\n",
      "510:\tlearn: 27.5917194\ttotal: 1.08s\tremaining: 1.03s\n",
      "511:\tlearn: 27.5899168\ttotal: 1.08s\tremaining: 1.03s\n",
      "512:\tlearn: 27.5881152\ttotal: 1.09s\tremaining: 1.03s\n",
      "513:\tlearn: 27.5830248\ttotal: 1.09s\tremaining: 1.03s\n",
      "514:\tlearn: 27.5809883\ttotal: 1.09s\tremaining: 1.03s\n",
      "515:\tlearn: 27.5775645\ttotal: 1.09s\tremaining: 1.03s\n",
      "516:\tlearn: 27.5758215\ttotal: 1.1s\tremaining: 1.02s\n",
      "517:\tlearn: 27.5749551\ttotal: 1.1s\tremaining: 1.02s\n",
      "518:\tlearn: 27.5740987\ttotal: 1.1s\tremaining: 1.02s\n",
      "519:\tlearn: 27.5735572\ttotal: 1.11s\tremaining: 1.02s\n",
      "520:\tlearn: 27.5727320\ttotal: 1.11s\tremaining: 1.02s\n",
      "521:\tlearn: 27.5722122\ttotal: 1.11s\tremaining: 1.02s\n",
      "522:\tlearn: 27.5692001\ttotal: 1.11s\tremaining: 1.02s\n",
      "523:\tlearn: 27.5687927\ttotal: 1.12s\tremaining: 1.01s\n",
      "524:\tlearn: 27.5670453\ttotal: 1.12s\tremaining: 1.01s\n",
      "525:\tlearn: 27.5653749\ttotal: 1.12s\tremaining: 1.01s\n",
      "526:\tlearn: 27.5650106\ttotal: 1.13s\tremaining: 1.01s\n",
      "527:\tlearn: 27.5642802\ttotal: 1.13s\tremaining: 1.01s\n",
      "528:\tlearn: 27.5636650\ttotal: 1.13s\tremaining: 1.01s\n",
      "529:\tlearn: 27.5589512\ttotal: 1.14s\tremaining: 1.01s\n",
      "530:\tlearn: 27.5571498\ttotal: 1.14s\tremaining: 1s\n",
      "531:\tlearn: 27.5564694\ttotal: 1.14s\tremaining: 1s\n",
      "532:\tlearn: 27.5558259\ttotal: 1.14s\tremaining: 1s\n",
      "533:\tlearn: 27.5535614\ttotal: 1.15s\tremaining: 1000ms\n",
      "534:\tlearn: 27.5527957\ttotal: 1.15s\tremaining: 999ms\n",
      "535:\tlearn: 27.5512940\ttotal: 1.15s\tremaining: 997ms\n",
      "536:\tlearn: 27.5496280\ttotal: 1.15s\tremaining: 995ms\n",
      "537:\tlearn: 27.5476372\ttotal: 1.16s\tremaining: 994ms\n",
      "538:\tlearn: 27.5467328\ttotal: 1.16s\tremaining: 994ms\n",
      "539:\tlearn: 27.5462579\ttotal: 1.16s\tremaining: 992ms\n",
      "540:\tlearn: 27.5443167\ttotal: 1.17s\tremaining: 990ms\n",
      "541:\tlearn: 27.5442135\ttotal: 1.17s\tremaining: 988ms\n",
      "542:\tlearn: 27.5436042\ttotal: 1.17s\tremaining: 988ms\n",
      "543:\tlearn: 27.5412701\ttotal: 1.18s\tremaining: 987ms\n",
      "544:\tlearn: 27.5397587\ttotal: 1.18s\tremaining: 986ms\n",
      "545:\tlearn: 27.5390354\ttotal: 1.19s\tremaining: 985ms\n",
      "546:\tlearn: 27.5363296\ttotal: 1.19s\tremaining: 984ms\n",
      "547:\tlearn: 27.5348872\ttotal: 1.19s\tremaining: 983ms\n",
      "548:\tlearn: 27.5345650\ttotal: 1.19s\tremaining: 981ms\n",
      "549:\tlearn: 27.5337267\ttotal: 1.2s\tremaining: 980ms\n",
      "550:\tlearn: 27.5301859\ttotal: 1.2s\tremaining: 978ms\n",
      "551:\tlearn: 27.5291413\ttotal: 1.2s\tremaining: 975ms\n",
      "552:\tlearn: 27.5284161\ttotal: 1.2s\tremaining: 973ms\n",
      "553:\tlearn: 27.5263521\ttotal: 1.21s\tremaining: 971ms\n",
      "554:\tlearn: 27.5247593\ttotal: 1.21s\tremaining: 969ms\n",
      "555:\tlearn: 27.5220502\ttotal: 1.21s\tremaining: 968ms\n",
      "556:\tlearn: 27.5213178\ttotal: 1.21s\tremaining: 966ms\n",
      "557:\tlearn: 27.5196775\ttotal: 1.22s\tremaining: 965ms\n",
      "558:\tlearn: 27.5183378\ttotal: 1.22s\tremaining: 964ms\n",
      "559:\tlearn: 27.5177494\ttotal: 1.22s\tremaining: 962ms\n",
      "560:\tlearn: 27.5157677\ttotal: 1.23s\tremaining: 960ms\n",
      "561:\tlearn: 27.5143782\ttotal: 1.23s\tremaining: 959ms\n",
      "562:\tlearn: 27.5141174\ttotal: 1.23s\tremaining: 958ms\n",
      "563:\tlearn: 27.5125048\ttotal: 1.24s\tremaining: 956ms\n",
      "564:\tlearn: 27.5123454\ttotal: 1.24s\tremaining: 955ms\n",
      "565:\tlearn: 27.5089047\ttotal: 1.24s\tremaining: 953ms\n",
      "566:\tlearn: 27.5083580\ttotal: 1.25s\tremaining: 951ms\n",
      "567:\tlearn: 27.5063783\ttotal: 1.25s\tremaining: 950ms\n",
      "568:\tlearn: 27.5057765\ttotal: 1.25s\tremaining: 948ms\n",
      "569:\tlearn: 27.5056802\ttotal: 1.26s\tremaining: 948ms\n",
      "570:\tlearn: 27.5039216\ttotal: 1.26s\tremaining: 946ms\n",
      "571:\tlearn: 27.5033772\ttotal: 1.26s\tremaining: 944ms\n",
      "572:\tlearn: 27.5025756\ttotal: 1.26s\tremaining: 942ms\n",
      "573:\tlearn: 27.5020899\ttotal: 1.27s\tremaining: 940ms\n",
      "574:\tlearn: 27.4977294\ttotal: 1.27s\tremaining: 939ms\n",
      "575:\tlearn: 27.4973309\ttotal: 1.27s\tremaining: 937ms\n",
      "576:\tlearn: 27.4972350\ttotal: 1.27s\tremaining: 935ms\n",
      "577:\tlearn: 27.4954036\ttotal: 1.28s\tremaining: 934ms\n",
      "578:\tlearn: 27.4946472\ttotal: 1.28s\tremaining: 933ms\n",
      "579:\tlearn: 27.4945158\ttotal: 1.28s\tremaining: 931ms\n",
      "580:\tlearn: 27.4944269\ttotal: 1.29s\tremaining: 929ms\n",
      "581:\tlearn: 27.4936786\ttotal: 1.29s\tremaining: 927ms\n",
      "582:\tlearn: 27.4930116\ttotal: 1.29s\tremaining: 926ms\n",
      "583:\tlearn: 27.4925078\ttotal: 1.3s\tremaining: 925ms\n",
      "584:\tlearn: 27.4908145\ttotal: 1.3s\tremaining: 923ms\n",
      "585:\tlearn: 27.4902891\ttotal: 1.3s\tremaining: 921ms\n",
      "586:\tlearn: 27.4885957\ttotal: 1.31s\tremaining: 919ms\n",
      "587:\tlearn: 27.4880480\ttotal: 1.31s\tremaining: 918ms\n",
      "588:\tlearn: 27.4874308\ttotal: 1.31s\tremaining: 916ms\n",
      "589:\tlearn: 27.4859867\ttotal: 1.31s\tremaining: 914ms\n",
      "590:\tlearn: 27.4852160\ttotal: 1.32s\tremaining: 912ms\n",
      "591:\tlearn: 27.4847689\ttotal: 1.32s\tremaining: 910ms\n",
      "592:\tlearn: 27.4817117\ttotal: 1.32s\tremaining: 908ms\n",
      "593:\tlearn: 27.4811915\ttotal: 1.32s\tremaining: 906ms\n",
      "594:\tlearn: 27.4776336\ttotal: 1.33s\tremaining: 905ms\n",
      "595:\tlearn: 27.4775719\ttotal: 1.33s\tremaining: 903ms\n",
      "596:\tlearn: 27.4771786\ttotal: 1.33s\tremaining: 901ms\n",
      "597:\tlearn: 27.4742456\ttotal: 1.34s\tremaining: 899ms\n",
      "598:\tlearn: 27.4734145\ttotal: 1.34s\tremaining: 898ms\n",
      "599:\tlearn: 27.4728341\ttotal: 1.34s\tremaining: 896ms\n",
      "600:\tlearn: 27.4706370\ttotal: 1.35s\tremaining: 894ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "601:\tlearn: 27.4705283\ttotal: 1.35s\tremaining: 892ms\n",
      "602:\tlearn: 27.4700219\ttotal: 1.35s\tremaining: 890ms\n",
      "603:\tlearn: 27.4694919\ttotal: 1.35s\tremaining: 889ms\n",
      "604:\tlearn: 27.4690674\ttotal: 1.36s\tremaining: 887ms\n",
      "605:\tlearn: 27.4687108\ttotal: 1.36s\tremaining: 885ms\n",
      "606:\tlearn: 27.4682426\ttotal: 1.36s\tremaining: 883ms\n",
      "607:\tlearn: 27.4655879\ttotal: 1.37s\tremaining: 882ms\n",
      "608:\tlearn: 27.4637087\ttotal: 1.37s\tremaining: 880ms\n",
      "609:\tlearn: 27.4606678\ttotal: 1.37s\tremaining: 878ms\n",
      "610:\tlearn: 27.4593297\ttotal: 1.38s\tremaining: 877ms\n",
      "611:\tlearn: 27.4589616\ttotal: 1.38s\tremaining: 875ms\n",
      "612:\tlearn: 27.4560755\ttotal: 1.38s\tremaining: 873ms\n",
      "613:\tlearn: 27.4558781\ttotal: 1.39s\tremaining: 871ms\n",
      "614:\tlearn: 27.4544476\ttotal: 1.39s\tremaining: 869ms\n",
      "615:\tlearn: 27.4539191\ttotal: 1.39s\tremaining: 867ms\n",
      "616:\tlearn: 27.4535132\ttotal: 1.39s\tremaining: 866ms\n",
      "617:\tlearn: 27.4532251\ttotal: 1.4s\tremaining: 864ms\n",
      "618:\tlearn: 27.4496169\ttotal: 1.4s\tremaining: 862ms\n",
      "619:\tlearn: 27.4492838\ttotal: 1.4s\tremaining: 861ms\n",
      "620:\tlearn: 27.4487263\ttotal: 1.41s\tremaining: 859ms\n",
      "621:\tlearn: 27.4486307\ttotal: 1.41s\tremaining: 857ms\n",
      "622:\tlearn: 27.4481840\ttotal: 1.41s\tremaining: 855ms\n",
      "623:\tlearn: 27.4467291\ttotal: 1.42s\tremaining: 854ms\n",
      "624:\tlearn: 27.4465786\ttotal: 1.42s\tremaining: 852ms\n",
      "625:\tlearn: 27.4439438\ttotal: 1.42s\tremaining: 850ms\n",
      "626:\tlearn: 27.4438923\ttotal: 1.43s\tremaining: 848ms\n",
      "627:\tlearn: 27.4433520\ttotal: 1.43s\tremaining: 846ms\n",
      "628:\tlearn: 27.4425475\ttotal: 1.43s\tremaining: 845ms\n",
      "629:\tlearn: 27.4422042\ttotal: 1.43s\tremaining: 843ms\n",
      "630:\tlearn: 27.4396349\ttotal: 1.44s\tremaining: 841ms\n",
      "631:\tlearn: 27.4385616\ttotal: 1.44s\tremaining: 839ms\n",
      "632:\tlearn: 27.4383051\ttotal: 1.44s\tremaining: 837ms\n",
      "633:\tlearn: 27.4380217\ttotal: 1.45s\tremaining: 836ms\n",
      "634:\tlearn: 27.4367012\ttotal: 1.45s\tremaining: 833ms\n",
      "635:\tlearn: 27.4362115\ttotal: 1.45s\tremaining: 831ms\n",
      "636:\tlearn: 27.4357917\ttotal: 1.46s\tremaining: 829ms\n",
      "637:\tlearn: 27.4357468\ttotal: 1.46s\tremaining: 827ms\n",
      "638:\tlearn: 27.4353333\ttotal: 1.46s\tremaining: 825ms\n",
      "639:\tlearn: 27.4321527\ttotal: 1.46s\tremaining: 823ms\n",
      "640:\tlearn: 27.4303257\ttotal: 1.47s\tremaining: 821ms\n",
      "641:\tlearn: 27.4299413\ttotal: 1.47s\tremaining: 818ms\n",
      "642:\tlearn: 27.4254403\ttotal: 1.47s\tremaining: 816ms\n",
      "643:\tlearn: 27.4246858\ttotal: 1.47s\tremaining: 814ms\n",
      "644:\tlearn: 27.4243411\ttotal: 1.47s\tremaining: 812ms\n",
      "645:\tlearn: 27.4239957\ttotal: 1.48s\tremaining: 809ms\n",
      "646:\tlearn: 27.4224374\ttotal: 1.48s\tremaining: 807ms\n",
      "647:\tlearn: 27.4206586\ttotal: 1.48s\tremaining: 804ms\n",
      "648:\tlearn: 27.4189278\ttotal: 1.48s\tremaining: 802ms\n",
      "649:\tlearn: 27.4186970\ttotal: 1.48s\tremaining: 799ms\n",
      "650:\tlearn: 27.4182106\ttotal: 1.49s\tremaining: 797ms\n",
      "651:\tlearn: 27.4172328\ttotal: 1.49s\tremaining: 795ms\n",
      "652:\tlearn: 27.4143203\ttotal: 1.49s\tremaining: 793ms\n",
      "653:\tlearn: 27.4138716\ttotal: 1.49s\tremaining: 791ms\n",
      "654:\tlearn: 27.4108322\ttotal: 1.5s\tremaining: 788ms\n",
      "655:\tlearn: 27.4092651\ttotal: 1.5s\tremaining: 786ms\n",
      "656:\tlearn: 27.4073431\ttotal: 1.5s\tremaining: 784ms\n",
      "657:\tlearn: 27.4072753\ttotal: 1.5s\tremaining: 781ms\n",
      "658:\tlearn: 27.4046741\ttotal: 1.51s\tremaining: 779ms\n",
      "659:\tlearn: 27.4041829\ttotal: 1.51s\tremaining: 777ms\n",
      "660:\tlearn: 27.4037202\ttotal: 1.51s\tremaining: 775ms\n",
      "661:\tlearn: 27.4022555\ttotal: 1.51s\tremaining: 773ms\n",
      "662:\tlearn: 27.4010003\ttotal: 1.51s\tremaining: 770ms\n",
      "663:\tlearn: 27.4009235\ttotal: 1.52s\tremaining: 768ms\n",
      "664:\tlearn: 27.4005808\ttotal: 1.52s\tremaining: 766ms\n",
      "665:\tlearn: 27.3989035\ttotal: 1.52s\tremaining: 764ms\n",
      "666:\tlearn: 27.3968475\ttotal: 1.52s\tremaining: 762ms\n",
      "667:\tlearn: 27.3955132\ttotal: 1.53s\tremaining: 759ms\n",
      "668:\tlearn: 27.3952323\ttotal: 1.53s\tremaining: 757ms\n",
      "669:\tlearn: 27.3947020\ttotal: 1.53s\tremaining: 755ms\n",
      "670:\tlearn: 27.3920520\ttotal: 1.53s\tremaining: 753ms\n",
      "671:\tlearn: 27.3899453\ttotal: 1.54s\tremaining: 751ms\n",
      "672:\tlearn: 27.3877136\ttotal: 1.54s\tremaining: 749ms\n",
      "673:\tlearn: 27.3876268\ttotal: 1.54s\tremaining: 747ms\n",
      "674:\tlearn: 27.3865441\ttotal: 1.55s\tremaining: 745ms\n",
      "675:\tlearn: 27.3860684\ttotal: 1.55s\tremaining: 743ms\n",
      "676:\tlearn: 27.3857390\ttotal: 1.55s\tremaining: 741ms\n",
      "677:\tlearn: 27.3854304\ttotal: 1.56s\tremaining: 739ms\n",
      "678:\tlearn: 27.3846922\ttotal: 1.56s\tremaining: 737ms\n",
      "679:\tlearn: 27.3842797\ttotal: 1.56s\tremaining: 734ms\n",
      "680:\tlearn: 27.3841942\ttotal: 1.56s\tremaining: 732ms\n",
      "681:\tlearn: 27.3839513\ttotal: 1.56s\tremaining: 730ms\n",
      "682:\tlearn: 27.3838194\ttotal: 1.57s\tremaining: 728ms\n",
      "683:\tlearn: 27.3807236\ttotal: 1.57s\tremaining: 725ms\n",
      "684:\tlearn: 27.3800029\ttotal: 1.57s\tremaining: 723ms\n",
      "685:\tlearn: 27.3786477\ttotal: 1.57s\tremaining: 720ms\n",
      "686:\tlearn: 27.3780855\ttotal: 1.57s\tremaining: 718ms\n",
      "687:\tlearn: 27.3752718\ttotal: 1.58s\tremaining: 716ms\n",
      "688:\tlearn: 27.3744039\ttotal: 1.58s\tremaining: 713ms\n",
      "689:\tlearn: 27.3719054\ttotal: 1.58s\tremaining: 711ms\n",
      "690:\tlearn: 27.3718021\ttotal: 1.58s\tremaining: 709ms\n",
      "691:\tlearn: 27.3715524\ttotal: 1.59s\tremaining: 707ms\n",
      "692:\tlearn: 27.3707247\ttotal: 1.59s\tremaining: 704ms\n",
      "693:\tlearn: 27.3692603\ttotal: 1.59s\tremaining: 702ms\n",
      "694:\tlearn: 27.3666539\ttotal: 1.59s\tremaining: 700ms\n",
      "695:\tlearn: 27.3648158\ttotal: 1.6s\tremaining: 697ms\n",
      "696:\tlearn: 27.3644438\ttotal: 1.6s\tremaining: 696ms\n",
      "697:\tlearn: 27.3643255\ttotal: 1.6s\tremaining: 694ms\n",
      "698:\tlearn: 27.3632953\ttotal: 1.61s\tremaining: 692ms\n",
      "699:\tlearn: 27.3606945\ttotal: 1.61s\tremaining: 690ms\n",
      "700:\tlearn: 27.3587695\ttotal: 1.61s\tremaining: 687ms\n",
      "701:\tlearn: 27.3583913\ttotal: 1.61s\tremaining: 685ms\n",
      "702:\tlearn: 27.3580956\ttotal: 1.62s\tremaining: 683ms\n",
      "703:\tlearn: 27.3570349\ttotal: 1.62s\tremaining: 681ms\n",
      "704:\tlearn: 27.3567141\ttotal: 1.62s\tremaining: 679ms\n",
      "705:\tlearn: 27.3546897\ttotal: 1.63s\tremaining: 677ms\n",
      "706:\tlearn: 27.3540721\ttotal: 1.63s\tremaining: 675ms\n",
      "707:\tlearn: 27.3536813\ttotal: 1.63s\tremaining: 673ms\n",
      "708:\tlearn: 27.3511717\ttotal: 1.64s\tremaining: 672ms\n",
      "709:\tlearn: 27.3508748\ttotal: 1.64s\tremaining: 669ms\n",
      "710:\tlearn: 27.3507290\ttotal: 1.64s\tremaining: 667ms\n",
      "711:\tlearn: 27.3487440\ttotal: 1.64s\tremaining: 665ms\n",
      "712:\tlearn: 27.3487093\ttotal: 1.65s\tremaining: 663ms\n",
      "713:\tlearn: 27.3477452\ttotal: 1.65s\tremaining: 660ms\n",
      "714:\tlearn: 27.3473815\ttotal: 1.65s\tremaining: 658ms\n",
      "715:\tlearn: 27.3470162\ttotal: 1.65s\tremaining: 656ms\n",
      "716:\tlearn: 27.3457640\ttotal: 1.66s\tremaining: 654ms\n",
      "717:\tlearn: 27.3438167\ttotal: 1.66s\tremaining: 652ms\n",
      "718:\tlearn: 27.3437161\ttotal: 1.66s\tremaining: 650ms\n",
      "719:\tlearn: 27.3434298\ttotal: 1.66s\tremaining: 647ms\n",
      "720:\tlearn: 27.3431816\ttotal: 1.67s\tremaining: 645ms\n",
      "721:\tlearn: 27.3430144\ttotal: 1.67s\tremaining: 643ms\n",
      "722:\tlearn: 27.3425856\ttotal: 1.67s\tremaining: 641ms\n",
      "723:\tlearn: 27.3423540\ttotal: 1.68s\tremaining: 639ms\n",
      "724:\tlearn: 27.3422732\ttotal: 1.68s\tremaining: 636ms\n",
      "725:\tlearn: 27.3419256\ttotal: 1.68s\tremaining: 634ms\n",
      "726:\tlearn: 27.3405586\ttotal: 1.68s\tremaining: 632ms\n",
      "727:\tlearn: 27.3394528\ttotal: 1.69s\tremaining: 630ms\n",
      "728:\tlearn: 27.3383884\ttotal: 1.69s\tremaining: 628ms\n",
      "729:\tlearn: 27.3373370\ttotal: 1.69s\tremaining: 626ms\n",
      "730:\tlearn: 27.3372491\ttotal: 1.7s\tremaining: 624ms\n",
      "731:\tlearn: 27.3367282\ttotal: 1.7s\tremaining: 622ms\n",
      "732:\tlearn: 27.3366676\ttotal: 1.7s\tremaining: 620ms\n",
      "733:\tlearn: 27.3363704\ttotal: 1.7s\tremaining: 617ms\n",
      "734:\tlearn: 27.3332682\ttotal: 1.71s\tremaining: 615ms\n",
      "735:\tlearn: 27.3329992\ttotal: 1.71s\tremaining: 613ms\n",
      "736:\tlearn: 27.3317744\ttotal: 1.71s\tremaining: 611ms\n",
      "737:\tlearn: 27.3316106\ttotal: 1.72s\tremaining: 609ms\n",
      "738:\tlearn: 27.3304155\ttotal: 1.72s\tremaining: 607ms\n",
      "739:\tlearn: 27.3285193\ttotal: 1.72s\tremaining: 605ms\n",
      "740:\tlearn: 27.3276435\ttotal: 1.72s\tremaining: 603ms\n",
      "741:\tlearn: 27.3267103\ttotal: 1.73s\tremaining: 600ms\n",
      "742:\tlearn: 27.3254978\ttotal: 1.73s\tremaining: 598ms\n",
      "743:\tlearn: 27.3253878\ttotal: 1.73s\tremaining: 595ms\n",
      "744:\tlearn: 27.3250843\ttotal: 1.73s\tremaining: 593ms\n",
      "745:\tlearn: 27.3250519\ttotal: 1.74s\tremaining: 591ms\n",
      "746:\tlearn: 27.3248041\ttotal: 1.74s\tremaining: 589ms\n",
      "747:\tlearn: 27.3223430\ttotal: 1.74s\tremaining: 587ms\n",
      "748:\tlearn: 27.3220009\ttotal: 1.74s\tremaining: 585ms\n",
      "749:\tlearn: 27.3217736\ttotal: 1.75s\tremaining: 582ms\n",
      "750:\tlearn: 27.3215144\ttotal: 1.75s\tremaining: 580ms\n",
      "751:\tlearn: 27.3194919\ttotal: 1.75s\tremaining: 578ms\n",
      "752:\tlearn: 27.3176877\ttotal: 1.76s\tremaining: 576ms\n",
      "753:\tlearn: 27.3173581\ttotal: 1.76s\tremaining: 574ms\n",
      "754:\tlearn: 27.3169450\ttotal: 1.76s\tremaining: 572ms\n",
      "755:\tlearn: 27.3148168\ttotal: 1.76s\tremaining: 570ms\n",
      "756:\tlearn: 27.3131338\ttotal: 1.77s\tremaining: 567ms\n",
      "757:\tlearn: 27.3128084\ttotal: 1.77s\tremaining: 565ms\n",
      "758:\tlearn: 27.3104714\ttotal: 1.77s\tremaining: 563ms\n",
      "759:\tlearn: 27.3100942\ttotal: 1.77s\tremaining: 560ms\n",
      "760:\tlearn: 27.3094292\ttotal: 1.77s\tremaining: 558ms\n",
      "761:\tlearn: 27.3059100\ttotal: 1.78s\tremaining: 555ms\n",
      "762:\tlearn: 27.3058710\ttotal: 1.78s\tremaining: 553ms\n",
      "763:\tlearn: 27.3056876\ttotal: 1.78s\tremaining: 551ms\n",
      "764:\tlearn: 27.3037122\ttotal: 1.78s\tremaining: 548ms\n",
      "765:\tlearn: 27.3025652\ttotal: 1.79s\tremaining: 546ms\n",
      "766:\tlearn: 27.3022323\ttotal: 1.79s\tremaining: 544ms\n",
      "767:\tlearn: 27.3018897\ttotal: 1.79s\tremaining: 541ms\n",
      "768:\tlearn: 27.3016301\ttotal: 1.79s\tremaining: 539ms\n",
      "769:\tlearn: 27.3006894\ttotal: 1.79s\tremaining: 536ms\n",
      "770:\tlearn: 27.2996716\ttotal: 1.8s\tremaining: 534ms\n",
      "771:\tlearn: 27.2987180\ttotal: 1.8s\tremaining: 532ms\n",
      "772:\tlearn: 27.2978894\ttotal: 1.8s\tremaining: 529ms\n",
      "773:\tlearn: 27.2974291\ttotal: 1.8s\tremaining: 527ms\n",
      "774:\tlearn: 27.2972481\ttotal: 1.81s\tremaining: 524ms\n",
      "775:\tlearn: 27.2965085\ttotal: 1.81s\tremaining: 522ms\n",
      "776:\tlearn: 27.2947324\ttotal: 1.81s\tremaining: 519ms\n",
      "777:\tlearn: 27.2934409\ttotal: 1.81s\tremaining: 517ms\n",
      "778:\tlearn: 27.2930847\ttotal: 1.81s\tremaining: 515ms\n",
      "779:\tlearn: 27.2926271\ttotal: 1.82s\tremaining: 513ms\n",
      "780:\tlearn: 27.2900905\ttotal: 1.82s\tremaining: 510ms\n",
      "781:\tlearn: 27.2897322\ttotal: 1.82s\tremaining: 508ms\n",
      "782:\tlearn: 27.2892714\ttotal: 1.82s\tremaining: 505ms\n",
      "783:\tlearn: 27.2892373\ttotal: 1.82s\tremaining: 503ms\n",
      "784:\tlearn: 27.2886372\ttotal: 1.83s\tremaining: 500ms\n",
      "785:\tlearn: 27.2882446\ttotal: 1.83s\tremaining: 498ms\n",
      "786:\tlearn: 27.2873106\ttotal: 1.83s\tremaining: 496ms\n",
      "787:\tlearn: 27.2872734\ttotal: 1.83s\tremaining: 493ms\n",
      "788:\tlearn: 27.2863703\ttotal: 1.83s\tremaining: 491ms\n",
      "789:\tlearn: 27.2855184\ttotal: 1.84s\tremaining: 489ms\n",
      "790:\tlearn: 27.2837930\ttotal: 1.84s\tremaining: 486ms\n",
      "791:\tlearn: 27.2826590\ttotal: 1.84s\tremaining: 484ms\n",
      "792:\tlearn: 27.2822396\ttotal: 1.84s\tremaining: 481ms\n",
      "793:\tlearn: 27.2819749\ttotal: 1.85s\tremaining: 479ms\n",
      "794:\tlearn: 27.2815045\ttotal: 1.85s\tremaining: 477ms\n",
      "795:\tlearn: 27.2814241\ttotal: 1.85s\tremaining: 474ms\n",
      "796:\tlearn: 27.2811310\ttotal: 1.85s\tremaining: 472ms\n",
      "797:\tlearn: 27.2792956\ttotal: 1.85s\tremaining: 469ms\n",
      "798:\tlearn: 27.2790468\ttotal: 1.86s\tremaining: 467ms\n",
      "799:\tlearn: 27.2778933\ttotal: 1.86s\tremaining: 465ms\n",
      "800:\tlearn: 27.2748056\ttotal: 1.86s\tremaining: 463ms\n",
      "801:\tlearn: 27.2747763\ttotal: 1.86s\tremaining: 460ms\n",
      "802:\tlearn: 27.2747053\ttotal: 1.86s\tremaining: 458ms\n",
      "803:\tlearn: 27.2733101\ttotal: 1.87s\tremaining: 455ms\n",
      "804:\tlearn: 27.2732285\ttotal: 1.87s\tremaining: 453ms\n",
      "805:\tlearn: 27.2726573\ttotal: 1.87s\tremaining: 450ms\n",
      "806:\tlearn: 27.2695587\ttotal: 1.87s\tremaining: 448ms\n",
      "807:\tlearn: 27.2679108\ttotal: 1.88s\tremaining: 446ms\n",
      "808:\tlearn: 27.2669235\ttotal: 1.88s\tremaining: 444ms\n",
      "809:\tlearn: 27.2659077\ttotal: 1.88s\tremaining: 441ms\n",
      "810:\tlearn: 27.2658740\ttotal: 1.88s\tremaining: 439ms\n",
      "811:\tlearn: 27.2658246\ttotal: 1.88s\tremaining: 436ms\n",
      "812:\tlearn: 27.2647152\ttotal: 1.89s\tremaining: 434ms\n",
      "813:\tlearn: 27.2646874\ttotal: 1.89s\tremaining: 431ms\n",
      "814:\tlearn: 27.2645427\ttotal: 1.89s\tremaining: 429ms\n",
      "815:\tlearn: 27.2644820\ttotal: 1.89s\tremaining: 427ms\n",
      "816:\tlearn: 27.2644307\ttotal: 1.89s\tremaining: 424ms\n",
      "817:\tlearn: 27.2619381\ttotal: 1.9s\tremaining: 422ms\n",
      "818:\tlearn: 27.2614266\ttotal: 1.9s\tremaining: 420ms\n",
      "819:\tlearn: 27.2610091\ttotal: 1.9s\tremaining: 417ms\n",
      "820:\tlearn: 27.2604988\ttotal: 1.9s\tremaining: 415ms\n",
      "821:\tlearn: 27.2590626\ttotal: 1.91s\tremaining: 413ms\n",
      "822:\tlearn: 27.2582096\ttotal: 1.91s\tremaining: 410ms\n",
      "823:\tlearn: 27.2581768\ttotal: 1.91s\tremaining: 408ms\n",
      "824:\tlearn: 27.2574892\ttotal: 1.91s\tremaining: 406ms\n",
      "825:\tlearn: 27.2573269\ttotal: 1.91s\tremaining: 403ms\n",
      "826:\tlearn: 27.2556965\ttotal: 1.92s\tremaining: 401ms\n",
      "827:\tlearn: 27.2556487\ttotal: 1.92s\tremaining: 398ms\n",
      "828:\tlearn: 27.2534856\ttotal: 1.92s\tremaining: 396ms\n",
      "829:\tlearn: 27.2524080\ttotal: 1.92s\tremaining: 394ms\n",
      "830:\tlearn: 27.2505540\ttotal: 1.93s\tremaining: 392ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "831:\tlearn: 27.2485795\ttotal: 1.93s\tremaining: 389ms\n",
      "832:\tlearn: 27.2479692\ttotal: 1.93s\tremaining: 387ms\n",
      "833:\tlearn: 27.2461929\ttotal: 1.93s\tremaining: 385ms\n",
      "834:\tlearn: 27.2459925\ttotal: 1.94s\tremaining: 383ms\n",
      "835:\tlearn: 27.2459603\ttotal: 1.94s\tremaining: 380ms\n",
      "836:\tlearn: 27.2450992\ttotal: 1.94s\tremaining: 378ms\n",
      "837:\tlearn: 27.2441775\ttotal: 1.94s\tremaining: 376ms\n",
      "838:\tlearn: 27.2417084\ttotal: 1.95s\tremaining: 374ms\n",
      "839:\tlearn: 27.2413635\ttotal: 1.95s\tremaining: 372ms\n",
      "840:\tlearn: 27.2403828\ttotal: 1.95s\tremaining: 370ms\n",
      "841:\tlearn: 27.2403591\ttotal: 1.96s\tremaining: 367ms\n",
      "842:\tlearn: 27.2389737\ttotal: 1.96s\tremaining: 365ms\n",
      "843:\tlearn: 27.2388434\ttotal: 1.96s\tremaining: 363ms\n",
      "844:\tlearn: 27.2379803\ttotal: 1.96s\tremaining: 360ms\n",
      "845:\tlearn: 27.2357785\ttotal: 1.97s\tremaining: 358ms\n",
      "846:\tlearn: 27.2357622\ttotal: 1.97s\tremaining: 356ms\n",
      "847:\tlearn: 27.2354543\ttotal: 1.97s\tremaining: 353ms\n",
      "848:\tlearn: 27.2351841\ttotal: 1.97s\tremaining: 351ms\n",
      "849:\tlearn: 27.2339599\ttotal: 1.98s\tremaining: 349ms\n",
      "850:\tlearn: 27.2337533\ttotal: 1.98s\tremaining: 346ms\n",
      "851:\tlearn: 27.2334865\ttotal: 1.98s\tremaining: 344ms\n",
      "852:\tlearn: 27.2332003\ttotal: 1.98s\tremaining: 342ms\n",
      "853:\tlearn: 27.2314570\ttotal: 1.99s\tremaining: 340ms\n",
      "854:\tlearn: 27.2311184\ttotal: 1.99s\tremaining: 337ms\n",
      "855:\tlearn: 27.2310738\ttotal: 1.99s\tremaining: 335ms\n",
      "856:\tlearn: 27.2306770\ttotal: 1.99s\tremaining: 333ms\n",
      "857:\tlearn: 27.2302766\ttotal: 2s\tremaining: 330ms\n",
      "858:\tlearn: 27.2297864\ttotal: 2s\tremaining: 328ms\n",
      "859:\tlearn: 27.2295611\ttotal: 2s\tremaining: 326ms\n",
      "860:\tlearn: 27.2287267\ttotal: 2s\tremaining: 324ms\n",
      "861:\tlearn: 27.2283312\ttotal: 2.01s\tremaining: 321ms\n",
      "862:\tlearn: 27.2267781\ttotal: 2.01s\tremaining: 319ms\n",
      "863:\tlearn: 27.2250591\ttotal: 2.01s\tremaining: 317ms\n",
      "864:\tlearn: 27.2238685\ttotal: 2.02s\tremaining: 315ms\n",
      "865:\tlearn: 27.2229103\ttotal: 2.02s\tremaining: 312ms\n",
      "866:\tlearn: 27.2226462\ttotal: 2.02s\tremaining: 310ms\n",
      "867:\tlearn: 27.2209120\ttotal: 2.02s\tremaining: 308ms\n",
      "868:\tlearn: 27.2208858\ttotal: 2.02s\tremaining: 305ms\n",
      "869:\tlearn: 27.2205930\ttotal: 2.03s\tremaining: 303ms\n",
      "870:\tlearn: 27.2205452\ttotal: 2.03s\tremaining: 301ms\n",
      "871:\tlearn: 27.2189987\ttotal: 2.03s\tremaining: 299ms\n",
      "872:\tlearn: 27.2188046\ttotal: 2.04s\tremaining: 296ms\n",
      "873:\tlearn: 27.2179239\ttotal: 2.04s\tremaining: 294ms\n",
      "874:\tlearn: 27.2162623\ttotal: 2.04s\tremaining: 291ms\n",
      "875:\tlearn: 27.2151489\ttotal: 2.04s\tremaining: 289ms\n",
      "876:\tlearn: 27.2141805\ttotal: 2.04s\tremaining: 287ms\n",
      "877:\tlearn: 27.2137555\ttotal: 2.04s\tremaining: 284ms\n",
      "878:\tlearn: 27.2131142\ttotal: 2.05s\tremaining: 282ms\n",
      "879:\tlearn: 27.2118959\ttotal: 2.05s\tremaining: 279ms\n",
      "880:\tlearn: 27.2109324\ttotal: 2.05s\tremaining: 277ms\n",
      "881:\tlearn: 27.2102541\ttotal: 2.05s\tremaining: 275ms\n",
      "882:\tlearn: 27.2099063\ttotal: 2.05s\tremaining: 272ms\n",
      "883:\tlearn: 27.2098493\ttotal: 2.06s\tremaining: 270ms\n",
      "884:\tlearn: 27.2098177\ttotal: 2.06s\tremaining: 267ms\n",
      "885:\tlearn: 27.2096355\ttotal: 2.06s\tremaining: 265ms\n",
      "886:\tlearn: 27.2078658\ttotal: 2.06s\tremaining: 263ms\n",
      "887:\tlearn: 27.2072133\ttotal: 2.06s\tremaining: 260ms\n",
      "888:\tlearn: 27.2056207\ttotal: 2.06s\tremaining: 258ms\n",
      "889:\tlearn: 27.2043990\ttotal: 2.07s\tremaining: 256ms\n",
      "890:\tlearn: 27.2040014\ttotal: 2.07s\tremaining: 253ms\n",
      "891:\tlearn: 27.2037960\ttotal: 2.07s\tremaining: 251ms\n",
      "892:\tlearn: 27.2037474\ttotal: 2.07s\tremaining: 248ms\n",
      "893:\tlearn: 27.2033181\ttotal: 2.07s\tremaining: 246ms\n",
      "894:\tlearn: 27.2031047\ttotal: 2.08s\tremaining: 244ms\n",
      "895:\tlearn: 27.2029266\ttotal: 2.08s\tremaining: 241ms\n",
      "896:\tlearn: 27.2013185\ttotal: 2.08s\tremaining: 239ms\n",
      "897:\tlearn: 27.2009206\ttotal: 2.08s\tremaining: 236ms\n",
      "898:\tlearn: 27.1996067\ttotal: 2.08s\tremaining: 234ms\n",
      "899:\tlearn: 27.1994432\ttotal: 2.08s\tremaining: 232ms\n",
      "900:\tlearn: 27.1994162\ttotal: 2.09s\tremaining: 229ms\n",
      "901:\tlearn: 27.1992214\ttotal: 2.09s\tremaining: 227ms\n",
      "902:\tlearn: 27.1989553\ttotal: 2.09s\tremaining: 225ms\n",
      "903:\tlearn: 27.1987013\ttotal: 2.09s\tremaining: 222ms\n",
      "904:\tlearn: 27.1978686\ttotal: 2.09s\tremaining: 220ms\n",
      "905:\tlearn: 27.1970013\ttotal: 2.1s\tremaining: 217ms\n",
      "906:\tlearn: 27.1962850\ttotal: 2.1s\tremaining: 215ms\n",
      "907:\tlearn: 27.1954605\ttotal: 2.1s\tremaining: 213ms\n",
      "908:\tlearn: 27.1954011\ttotal: 2.1s\tremaining: 210ms\n",
      "909:\tlearn: 27.1952251\ttotal: 2.1s\tremaining: 208ms\n",
      "910:\tlearn: 27.1951972\ttotal: 2.1s\tremaining: 206ms\n",
      "911:\tlearn: 27.1951697\ttotal: 2.11s\tremaining: 203ms\n",
      "912:\tlearn: 27.1949898\ttotal: 2.11s\tremaining: 201ms\n",
      "913:\tlearn: 27.1949493\ttotal: 2.11s\tremaining: 198ms\n",
      "914:\tlearn: 27.1936616\ttotal: 2.11s\tremaining: 196ms\n",
      "915:\tlearn: 27.1935697\ttotal: 2.11s\tremaining: 194ms\n",
      "916:\tlearn: 27.1934877\ttotal: 2.11s\tremaining: 191ms\n",
      "917:\tlearn: 27.1933994\ttotal: 2.12s\tremaining: 189ms\n",
      "918:\tlearn: 27.1930309\ttotal: 2.12s\tremaining: 187ms\n",
      "919:\tlearn: 27.1921241\ttotal: 2.12s\tremaining: 184ms\n",
      "920:\tlearn: 27.1918920\ttotal: 2.12s\tremaining: 182ms\n",
      "921:\tlearn: 27.1911394\ttotal: 2.13s\tremaining: 180ms\n",
      "922:\tlearn: 27.1905194\ttotal: 2.13s\tremaining: 177ms\n",
      "923:\tlearn: 27.1889976\ttotal: 2.13s\tremaining: 175ms\n",
      "924:\tlearn: 27.1889525\ttotal: 2.13s\tremaining: 173ms\n",
      "925:\tlearn: 27.1887698\ttotal: 2.13s\tremaining: 171ms\n",
      "926:\tlearn: 27.1881258\ttotal: 2.14s\tremaining: 168ms\n",
      "927:\tlearn: 27.1865147\ttotal: 2.14s\tremaining: 166ms\n",
      "928:\tlearn: 27.1836094\ttotal: 2.14s\tremaining: 164ms\n",
      "929:\tlearn: 27.1816901\ttotal: 2.14s\tremaining: 161ms\n",
      "930:\tlearn: 27.1810170\ttotal: 2.15s\tremaining: 159ms\n",
      "931:\tlearn: 27.1807632\ttotal: 2.15s\tremaining: 157ms\n",
      "932:\tlearn: 27.1787211\ttotal: 2.15s\tremaining: 154ms\n",
      "933:\tlearn: 27.1759446\ttotal: 2.15s\tremaining: 152ms\n",
      "934:\tlearn: 27.1735529\ttotal: 2.15s\tremaining: 150ms\n",
      "935:\tlearn: 27.1731518\ttotal: 2.15s\tremaining: 147ms\n",
      "936:\tlearn: 27.1726518\ttotal: 2.16s\tremaining: 145ms\n",
      "937:\tlearn: 27.1726314\ttotal: 2.16s\tremaining: 143ms\n",
      "938:\tlearn: 27.1722574\ttotal: 2.16s\tremaining: 140ms\n",
      "939:\tlearn: 27.1719552\ttotal: 2.16s\tremaining: 138ms\n",
      "940:\tlearn: 27.1708072\ttotal: 2.16s\tremaining: 136ms\n",
      "941:\tlearn: 27.1700612\ttotal: 2.17s\tremaining: 133ms\n",
      "942:\tlearn: 27.1692652\ttotal: 2.17s\tremaining: 131ms\n",
      "943:\tlearn: 27.1692279\ttotal: 2.17s\tremaining: 129ms\n",
      "944:\tlearn: 27.1691040\ttotal: 2.17s\tremaining: 126ms\n",
      "945:\tlearn: 27.1686831\ttotal: 2.17s\tremaining: 124ms\n",
      "946:\tlearn: 27.1684082\ttotal: 2.17s\tremaining: 122ms\n",
      "947:\tlearn: 27.1679528\ttotal: 2.18s\tremaining: 119ms\n",
      "948:\tlearn: 27.1666513\ttotal: 2.18s\tremaining: 117ms\n",
      "949:\tlearn: 27.1663928\ttotal: 2.18s\tremaining: 115ms\n",
      "950:\tlearn: 27.1660663\ttotal: 2.18s\tremaining: 112ms\n",
      "951:\tlearn: 27.1658436\ttotal: 2.18s\tremaining: 110ms\n",
      "952:\tlearn: 27.1636693\ttotal: 2.19s\tremaining: 108ms\n",
      "953:\tlearn: 27.1634159\ttotal: 2.19s\tremaining: 105ms\n",
      "954:\tlearn: 27.1632368\ttotal: 2.19s\tremaining: 103ms\n",
      "955:\tlearn: 27.1610919\ttotal: 2.19s\tremaining: 101ms\n",
      "956:\tlearn: 27.1607960\ttotal: 2.19s\tremaining: 98.5ms\n",
      "957:\tlearn: 27.1597249\ttotal: 2.19s\tremaining: 96.2ms\n",
      "958:\tlearn: 27.1589317\ttotal: 2.2s\tremaining: 93.9ms\n",
      "959:\tlearn: 27.1576498\ttotal: 2.2s\tremaining: 91.6ms\n",
      "960:\tlearn: 27.1576246\ttotal: 2.2s\tremaining: 89.3ms\n",
      "961:\tlearn: 27.1570062\ttotal: 2.2s\tremaining: 87ms\n",
      "962:\tlearn: 27.1568319\ttotal: 2.2s\tremaining: 84.7ms\n",
      "963:\tlearn: 27.1558122\ttotal: 2.21s\tremaining: 82.4ms\n",
      "964:\tlearn: 27.1555322\ttotal: 2.21s\tremaining: 80.1ms\n",
      "965:\tlearn: 27.1544758\ttotal: 2.21s\tremaining: 77.8ms\n",
      "966:\tlearn: 27.1539813\ttotal: 2.21s\tremaining: 75.5ms\n",
      "967:\tlearn: 27.1538131\ttotal: 2.21s\tremaining: 73.1ms\n",
      "968:\tlearn: 27.1530075\ttotal: 2.21s\tremaining: 70.8ms\n",
      "969:\tlearn: 27.1508007\ttotal: 2.22s\tremaining: 68.5ms\n",
      "970:\tlearn: 27.1501222\ttotal: 2.22s\tremaining: 66.3ms\n",
      "971:\tlearn: 27.1495689\ttotal: 2.22s\tremaining: 64ms\n",
      "972:\tlearn: 27.1491843\ttotal: 2.22s\tremaining: 61.7ms\n",
      "973:\tlearn: 27.1479566\ttotal: 2.22s\tremaining: 59.4ms\n",
      "974:\tlearn: 27.1478945\ttotal: 2.23s\tremaining: 57.1ms\n",
      "975:\tlearn: 27.1472319\ttotal: 2.23s\tremaining: 54.8ms\n",
      "976:\tlearn: 27.1464811\ttotal: 2.23s\tremaining: 52.5ms\n",
      "977:\tlearn: 27.1459816\ttotal: 2.23s\tremaining: 50.2ms\n",
      "978:\tlearn: 27.1455997\ttotal: 2.23s\tremaining: 47.9ms\n",
      "979:\tlearn: 27.1453333\ttotal: 2.23s\tremaining: 45.6ms\n",
      "980:\tlearn: 27.1450242\ttotal: 2.24s\tremaining: 43.3ms\n",
      "981:\tlearn: 27.1444306\ttotal: 2.24s\tremaining: 41ms\n",
      "982:\tlearn: 27.1441242\ttotal: 2.24s\tremaining: 38.8ms\n",
      "983:\tlearn: 27.1440807\ttotal: 2.24s\tremaining: 36.5ms\n",
      "984:\tlearn: 27.1438308\ttotal: 2.24s\tremaining: 34.2ms\n",
      "985:\tlearn: 27.1432508\ttotal: 2.25s\tremaining: 31.9ms\n",
      "986:\tlearn: 27.1423921\ttotal: 2.25s\tremaining: 29.6ms\n",
      "987:\tlearn: 27.1416891\ttotal: 2.25s\tremaining: 27.3ms\n",
      "988:\tlearn: 27.1407896\ttotal: 2.25s\tremaining: 25ms\n",
      "989:\tlearn: 27.1402543\ttotal: 2.25s\tremaining: 22.8ms\n",
      "990:\tlearn: 27.1386429\ttotal: 2.25s\tremaining: 20.5ms\n",
      "991:\tlearn: 27.1378837\ttotal: 2.26s\tremaining: 18.2ms\n",
      "992:\tlearn: 27.1358953\ttotal: 2.26s\tremaining: 15.9ms\n",
      "993:\tlearn: 27.1348950\ttotal: 2.26s\tremaining: 13.6ms\n",
      "994:\tlearn: 27.1344460\ttotal: 2.26s\tremaining: 11.4ms\n",
      "995:\tlearn: 27.1338214\ttotal: 2.26s\tremaining: 9.09ms\n",
      "996:\tlearn: 27.1337697\ttotal: 2.27s\tremaining: 6.82ms\n",
      "997:\tlearn: 27.1336891\ttotal: 2.27s\tremaining: 4.54ms\n",
      "998:\tlearn: 27.1336713\ttotal: 2.27s\tremaining: 2.27ms\n",
      "999:\tlearn: 27.1334549\ttotal: 2.27s\tremaining: 0us\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE = 1550.6431144700593\n",
      "R_2 = -0.005312566072904756\n"
     ]
    }
   ],
   "source": [
    "cb = CatBoostRegressor(random_state=42)\n",
    "cb.fit(X_train_new, y_train)\n",
    "y_pred = cb.predict(X_test_new)\n",
    "print(f'MSE = {mse(y_test, y_pred)}')\n",
    "print(f'R_2 = {r2_score(y_test, y_pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
